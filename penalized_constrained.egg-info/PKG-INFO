Metadata-Version: 2.4
Name: penalized-constrained
Version: 0.1.0
Summary: Penalized regression with coefficient constraints for cost estimation
Author: Max Watstein
Author-email: Kevin Joy <kevin.joy@jlha.com>
Maintainer-email: Kevin Joy <kevin.joy@jlha.com>
License: MIT
Project-URL: Homepage, https://github.com/herrenassociates/penalized-constrained
Project-URL: Documentation, https://penalized-constrained.readthedocs.io
Project-URL: Bug Tracker, https://github.com/herrenassociates/penalized-constrained/issues
Project-URL: Source, https://github.com/herrenassociates/penalized-constrained
Keywords: regression,constrained optimization,regularization,lasso,ridge,elastic net,cost estimation,learning curve,multicollinearity
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Science/Research
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering
Classifier: Topic :: Scientific/Engineering :: Mathematics
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy>=1.20
Requires-Dist: scipy>=1.7
Requires-Dist: scikit-learn>=1.0
Requires-Dist: pandas>=1.3
Requires-Dist: openpyxl>=3.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0; extra == "dev"
Requires-Dist: black>=23.0; extra == "dev"
Requires-Dist: flake8>=6.0; extra == "dev"
Requires-Dist: mypy>=1.0; extra == "dev"
Provides-Extra: docs
Requires-Dist: sphinx>=6.0; extra == "docs"
Requires-Dist: sphinx-rtd-theme>=1.0; extra == "docs"
Requires-Dist: numpydoc>=1.5; extra == "docs"
Provides-Extra: viz
Requires-Dist: matplotlib>=3.4; extra == "viz"
Requires-Dist: seaborn>=0.12; extra == "viz"
Requires-Dist: plotly>=5.0; extra == "viz"
Provides-Extra: paper
Requires-Dist: jupyter>=1.0; extra == "paper"
Requires-Dist: nbformat>=5.0; extra == "paper"
Requires-Dist: ipykernel>=6.0; extra == "paper"
Requires-Dist: pingouin>=0.5; extra == "paper"
Requires-Dist: matplotlib>=3.4; extra == "paper"
Requires-Dist: seaborn>=0.12; extra == "paper"
Requires-Dist: pyarrow>=14.0; extra == "paper"
Requires-Dist: tabulate; extra == "paper"
Provides-Extra: simulation
Requires-Dist: joblib>=1.0; extra == "simulation"
Requires-Dist: tqdm>=4.0; extra == "simulation"
Provides-Extra: all
Requires-Dist: penalized-constrained[dev,docs,paper,simulation,viz]; extra == "all"
Dynamic: license-file

# Penalized-Constrained Regression

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![sklearn compatible](https://img.shields.io/badge/sklearn-compatible-brightgreen.svg)](https://scikit-learn.org/)

**Combining regularization and domain constraints for cost estimation with small, correlated datasets.**

## Overview

`penalized-constrained` provides sklearn-compatible estimators that combine:

- **Penalized Regularization** (L1/L2/ElasticNet) to handle multicollinearity
- **Coefficient Constraints** (bounds) to enforce domain knowledge
- **Customizable Prediction Function** defaults to standard linear prediction: X @ coef + intercept, but allows custom prediction function with signature: prediction_fn(X, params) -> y_pred
- **Customizable Loss Function** Sum of Squared Percentage Errors (SSPE), Sum Squared Error (SSE) and Mean Squared Error (MSE) or callable custom loss function with signature loss(y_true, y_pred)

This is particularly useful for:
- Learning curve analysis with rate effects
- Cost estimation with small samples (5-30 data points)
- Any regression where coefficients should be bounded based on prior knowledge

## Installation

install from source (pip installation comming soon):

```bash
git clone https://github.com/frankij11/penalized-constrained.git
cd penalized-constrained
pip install -e .
```

## Quick Start

```python
import numpy as np
import penalized_constrained as pcreg

# Generate sample data
np.random.seed(42)
X = np.random.randn(100, 2)
y = X @ np.array([-0.15, -0.07]) + 4.5 + 0.1 * np.random.randn(100)

# Fit with cross-validated hyperparameter selection
model = pcreg.PenalizedConstrainedCV(
    bounds=[(-1, 0), (-1, 0)],  # Both coefficients must be ≤ 0
    loss='sspe',                 # Sum of Squared Percentage Error
    cv=5
)
model.fit(X, y)

# Results
print(f"Best alpha: {model.alpha_:.4f}")
print(f"Best l1_ratio: {model.l1_ratio_:.2f}")
print(f"Coefficients: {model.coef_}")
print(f"Active constraints: {model.active_constraints_}")
```

## Learning Curve Example

```python
import penalized_constrained as pcreg

# Generate realistic learning curve data
data = pcreg.generate_correlated_learning_data(
    n_lots=20,
    T1=100,
    target_correlation=0.7,  # Correlation between predictors
    cv_error=0.1,            # 10% CV error
    random_state=42
)

X, y = data['X'], data['y']  # Log-transformed
true_params = data['params']

# Fit with named coefficients
model = pcreg.PenalizedConstrainedCV(
    coef_names=['LC', 'RC'],
    bounds={'LC': (-1, 0), 'RC': (-0.5, 0)},
    alphas=np.logspace(-2, 1, 10),
    l1_ratios=[0.0, 0.5, 1.0],
    loss='sspe'
)
model.fit(X, y)

# Compare to true values
print(f"True LC slope: {true_params['b']:.4f}")
print(f"Estimated LC: {model.named_coef_['LC']:.4f}")
print(f"True RC slope: {true_params['c']:.4f}")
print(f"Estimated RC: {model.named_coef_['RC']:.4f}")
```

## Custom Prediction Function

For non-linear models like `Y = T1 * X1^b * X2^c`:

```python
def lc_func(X, params):
    """Learning curve prediction: T1 * unit^b * qty^c"""
    T1, b, c = params
    return T1 * (X[:, 0] ** b) * (X[:, 1] ** c)

model = pcreg.PenalizedConstrainedRegression(
    prediction_fn=lc_func,
    coef_names=['T1', 'LC', 'RC'],
    bounds={'T1': (0, None), 'LC': (-1, 0), 'RC': (-1, 0)},
    fit_intercept=False,
    alpha=0.1
)
model.fit(X_original, y_original)  # Original space, not log-transformed
```

## Diagnostics

```python
from penalized_constrained import ModelDiagnostics

# Compute GDF-adjusted statistics
diag = ModelDiagnostics(model, X, y, gdf_method='hu')

# Basic summary
report = diag.summary()
report.print_summary()

# With bootstrap confidence intervals (recommended for small samples)
report = diag.summary(
    bootstrap=True,
    n_bootstrap=1000,
    ci_level=0.95,
    warm_start_coef=model.coef_  # Improves convergence for custom prediction_fn
)

# Export to various formats
report.to_html('model_report.html')   # Interactive HTML with plots
report.to_excel('model_report.xlsx')  # Multi-sheet Excel workbook
report.to_pdf('model_report.pdf')     # PDF report
report.to_dataframe()                 # Pandas DataFrame
```

### Bootstrap Confidence Intervals

For small samples where asymptotic standard errors may be unreliable:

```python
from penalized_constrained import bootstrap_confidence_intervals

# Compute bootstrap CIs
results = bootstrap_confidence_intervals(
    model, X, y,
    n_bootstrap=1000,
    ci_level=0.95,
    warm_start_coef=model.coef_,  # Use fitted coefs as initialization
    random_state=42
)

print(f"Coefficient means: {results['coef_mean']}")
print(f"Coefficient std: {results['coef_std']}")
print(f"95% CI lower: {results['coef_ci_lower']}")
print(f"95% CI upper: {results['coef_ci_upper']}")
print(f"Successful fits: {results['n_successful']}/{n_bootstrap}")
```

### Alpha Trace Analysis

Visualize coefficient stability across hyperparameter values:

```python
from penalized_constrained import compute_alpha_trace, plot_alpha_trace

# Compute trace across alpha grid
trace = compute_alpha_trace(
    model, X, y,
    alphas=np.logspace(-4, 0, 20),
    l1_ratios=[0.0, 0.5, 1.0]
)

# Plot coefficient paths
plot_alpha_trace(trace, coef_names=['LC', 'RC'])
```

## Key Features

### Loss Functions

| Loss | Formula | Use Case |
|------|---------|----------|
| `'sspe'` | Σ[(y-ŷ)/y]² | Cost estimation (default, MUPE-consistent) |
| `'sse'` | Σ(y-ŷ)² | Standard OLS |
| `'mse'` | mean(y-ŷ)² | Normalized MSE |
| callable | Custom | Any user-defined loss |

### Bounds Specification

```python
# Single tuple: same bounds for all coefficients
bounds = (-1, 0)

# List: individual bounds per coefficient
bounds = [(-1, 0), (-0.5, 0.1), (None, 10)]

# Dict: named bounds (requires coef_names)
bounds = {'LC': (-1, 0), 'RC': (-0.5, 0)}
```

### Penalty Exclusion

Exclude specific coefficients from regularization while still applying bounds:

```python
# T1 (theoretical first unit cost) should be constrained but not penalized
model = pcreg.PenalizedConstrainedCV(
    prediction_fn=lc_func,
    coef_names=['T1', 'LC', 'RC'],
    bounds={'T1': (0, None), 'LC': (-1, 0), 'RC': (-1, 0)},
    penalty_exclude=['T1'],  # Only LC and RC receive L1/L2 penalty
    alpha=0.1
)
```

### Safe Mode for Custom Functions

When using custom `prediction_fn`, invalid parameters can produce `inf` or `nan`. With `safe_mode=True` (default), the optimizer:
- Detects invalid predictions
- Returns a gradient-informative penalty pointing back toward valid parameters
- Tracks last valid parameters for recovery

### Generalized Degrees of Freedom

Two methods available for constraint-adjusted statistics:

- **Hu's method**: `gdf_method='hu'` - All specified constraints count
- **Gaines' method**: `gdf_method='gaines'` - Only binding constraints count

## API Reference

### PenalizedConstrainedRegression

Main estimator class.

```python
PenalizedConstrainedRegression(
    alpha=0.0,            # Penalty strength
    l1_ratio=0.0,         # L1/L2 mix: 0=Ridge, 1=Lasso
    bounds=None,          # Coefficient bounds
    coef_names=None,      # Names for coefficients
    penalty_exclude=None, # Coefficients to exclude from penalty
    fit_intercept=True,   # Fit intercept
    loss='sspe',          # Loss function
    prediction_fn=None,   # Custom prediction function
    scale=False,          # Standardize X internally
    x0='ols',             # Initial params: 'ols', 'zeros', or array
    method='SLSQP',       # Optimizer
    max_iter=1000,        # Max iterations
    tol=1e-6,             # Tolerance
    safe_mode=True,       # Handle inf/nan in custom prediction_fn
    verbose=0             # Verbosity
)
```

### PenalizedConstrainedCV

Cross-validated version with automatic hyperparameter tuning.

```python
PenalizedConstrainedCV(
    alphas=None,         # Grid of alpha values (default: logspace(-4, 0, 10))
    l1_ratios=None,      # Grid of l1_ratio values (default: [0.0, 0.5, 1.0])
    bounds=None,         # Coefficient bounds
    selection='cv',      # Selection method (see below)
    cv=5,                # Cross-validation folds
    scoring='neg_mean_squared_error',
    n_jobs=-1,           # Parallel jobs
    ...                  # Same as base class
)
```

**Selection Methods** (`selection` parameter):

| Method | Description | Best For |
|--------|-------------|----------|
| `'cv'` | K-fold cross-validation (default) | Moderate sample sizes (n > 30) |
| `'loocv'` | Leave-one-out cross-validation | Small samples, no randomness |
| `'aic'` | Akaike Information Criterion | Balancing fit and complexity |
| `'aicc'` | Corrected AIC | Small samples (n/df < 40) - **recommended** |
| `'bic'` | Bayesian Information Criterion | Conservative model selection |
| `'gcv'` | Generalized Cross-Validation | Approximates LOOCV efficiently |

## References

- Joy, K. & Watstein, M. (2026). "Small Data, Big Problems: Can Constraints and Penalties Save Regression?" ICEAA Professional Development & Training Workshop.
- James, G.M., Paulson, C., & Rusmevichientong, P. (2020). "Penalized and Constrained Optimization." JASA.
- Gaines, B.R., Kim, J., & Zhou, H. (2018). "Algorithms for Fitting the Constrained Lasso." JCGS.
- Hu, S. (2010). "Generalized Degrees of Freedom for Constrained CERs." Tecolote Research.

## License

MIT License - see LICENSE file for details.

## Contributing

Contributions welcome! Please open an issue or submit a pull request.
