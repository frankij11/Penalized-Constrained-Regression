{
  "hash": "a1cabec55956a893ca3c4efb0bc9bef3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Penalized-Constrained Regression for Learning Curve Estimation\"\nsubtitle: \"ICEAA 2026 Professional Development & Training Workshop\"\nauthor:\n  - name: Kevin Joy\n    affiliations:\n      - Herren Associates\n  - name: Max Watstein\n    affiliations:\n      - Herren Associates\ndate: last-modified\nabstract: |\n  Small datasets with multicollinearity pose serious challenges to the stability\n  of coefficients generated by Ordinary Least Squares (OLS) regression. A motivating\n  example in cost estimating is Cost Improvement Curve with Rate Effect analysis,\n  where datasets are typically small, the lot midpoint (Learning) is correlated to\n  lot size (Rate) as production ramps up, and slopes are expected to be negative.\n  This paper presents Penalized-Constrained Regression (PCReg), a method that\n  combines elastic net regularization with domain-knowledge constraints for\n  improved learning curve estimation. Through a comprehensive Monte Carlo\n  simulation study with 6,075 scenario-replications, we demonstrate that PCReg\n  outperforms OLS in 58% of scenarios, with particular advantages when data quality\n  is high, sample sizes are small, or when OLS produces coefficients with incorrect signs.\nkeywords: [learning curves, constrained optimization, regularization, cost estimation, multicollinearity]\nbibliography: references.bib\n---\n\n# Executive Summary {.unnumbered}\n\n::: {.callout-tip title=\"Key Findings\"}\n**Problem**: Small datasets with correlated predictors cause OLS to produce unstable, often nonsensical coefficient estimates (wrong signs, implausible magnitudes).\n\n**Solution**: Penalized-Constrained Regression (PCReg) combines elastic net regularization with bound constraints based on domain knowledge.\n\n**Results**: In our Monte Carlo study (243 scenarios x 25 replications = 6,075 total):\n\n- **Overall**: PCReg wins 58% of scenarios vs OLS\n- **When OLS has wrong signs**: PCReg wins 81% of scenarios\n- **High-quality data** (CV error = 0.01): PCReg wins 67-75%\n- **Small samples** (n = 5): PCReg wins 64%\n\n**Recommendation**: Use PCReg when sample size is small (n ≤ 10) or data quality is high (CV error ≤ 0.10). Use OLS when sample size is large (n ≥ 30) and noise is high (CV error = 0.20).\n:::\n\n# Introduction {#sec-introduction}\n\n## The Problem: Small, Correlated Datasets in Cost Estimation\n\nDeveloping hundreds of Cost Estimating Relationships (CERs) for small datasets ranging from 5-30 data points, a recurring pattern emerges: strong fit statistics (R², CV) but nonsensical coefficients---wrong signs, implausible magnitudes, and poor p-values for critical predictors. As Department of Defense analysts know, this story may feel too close to home.\n\nMulticollinear datasets are a frequent presence in cost analysis, causing models to misbehave. The consequences of multicollinearity in small samples are well-documented [@flynn2016multicollinearity]:\n\n- **\"Bouncing β's\"**---unstable coefficient estimates that swing wildly with small changes in the data\n- **Wrong coefficient signs**---estimates flip positive/negative contrary to domain knowledge\n- **Unreliable hypothesis testing**---high F-statistic but statistically insignificant individual t-statistics\n- **Hidden extrapolation**---predictions fall outside the convex hull of training data despite appearing within variable ranges\n- **Inflated variance**---coefficient variance increases by factor $1/(1-R^2)$ where $R^2$ is correlation between predictors\n\n### Motivating Example: Cost Improvement Curve with Rate Effect\n\nLearning curves are fundamental to cost estimation in manufacturing and aerospace industries. The classic power-law model describes how unit costs decrease with cumulative production:\n\n$$Y = T_1 \\cdot X^b$$\n\nwhere $Y$ is the unit cost, $T_1$ is the theoretical first unit cost, $X$ is the cumulative unit number, and $b$ is the learning curve slope (typically negative, indicating cost reduction with experience).\n\nWhen multiple factors affect costs (e.g., production rate effects), the model extends to:\n\n$$Y = T_1 \\cdot X_1^b \\cdot X_2^c \\cdot \\varepsilon$$ {#eq-learning-rate}\n\nwhere $X_1$ represents lot midpoint (Learning variable) and $X_2$ represents lot quantity (Rate variable). In this specification, lot midpoint is inherently correlated with lot size as production ramps up from Prototypes to Low-Rate Initial Production (LRIP) to Full-Rate Production (FRP). Correlations of $\\rho = (-0.3, 0.88)$ have been found in Selected Acquisition Reports comparing lot size to cumulative quantities. Domain knowledge establishes that learning and rate slopes should be $\\leq 100\\%$ (i.e., costs should not increase with cumulative production).\n\n## Diagnosing Multicollinearity\n\nStandard diagnostic measures for detecting multicollinearity include:\n\n| Diagnostic | Threshold | Interpretation |\n|------------|-----------|----------------|\n| Simple correlation $r_{X_i,X_j}$ | > 0.8 | Potential multicollinearity |\n| VIF (Variance Inflation Factor) | > 10 | Likely harmful (CEBoK threshold) |\n| Condition number $\\kappa$ | > 30 | Collinearity harmful |\n| R² among predictors | > 0.90 | Harmful if $R^2$ for $X_i \\vert \\text{other } X$'s > 0.90 |\n| F vs t mismatch | High F, low t | Classic multicollinearity symptom |\n\n: Multicollinearity diagnostic thresholds {#tbl-diagnostics}\n\nThe Variance Inflation Factor is defined as $\\text{VIF} = 1/(1-R^2_{X_i|\\text{other}X})$, and the condition number $\\kappa = \\lambda_{\\max}/\\lambda_{\\min}$ from eigenvalue decomposition of $X'X$.\n\n## Traditional Remedies and Their Limitations\n\nTraditional approaches each come with trade-offs [@flynn2016multicollinearity]:\n\n| Remedy | Description | Limitation |\n|--------|-------------|------------|\n| Collect more data | Increases sample size, reduces variance | Often infeasible in defense cost analysis |\n| Drop variables | Remove collinear predictors via confluence analysis | May lose domain-required variables |\n| Centering/Scaling | Reduces structural multicollinearity | Only helps polynomial/interaction terms |\n| Ridge Regression | L2 penalty shrinks coefficients | No sparsity; no domain constraints |\n| Lasso Regression | L1 penalty enables variable selection | Arbitrary selection among correlated vars; no constraints |\n| Elastic Net | Combined L1+L2 penalties | Still no explicit domain constraints |\n| PCA / PLS | Transforms to uncorrelated components | Loses coefficient interpretability |\n\n: Traditional multicollinearity remedies {#tbl-remedies}\n\n## Theoretical Foundation\n\n### Why Some Regularization is Always Optimal\n\n::: {.callout-note title=\"Theobald-Farebrother Theorem (1974, 1976)\"}\nFor any OLS problem, there exists a ridge parameter $\\lambda^* > 0$ such that the ridge estimator has strictly lower Mean Squared Error (MSE) than OLS. This result holds for the population MSE (true prediction risk), not merely training error [@theobald1974; @farebrother1976].\n:::\n\n**Why This Matters**: OLS minimizes training error but may overfit---especially with correlated predictors. Ridge introduces bias but reduces variance. The theorem guarantees that for some $\\lambda > 0$, the variance reduction exceeds the bias increase, yielding lower total error.\n\n**The Practical Challenge**: The optimal $\\lambda^*$ depends on unknown population parameters ($\\beta$, $\\sigma^2$). Cross-validation provides an empirical estimate. When CV selects $\\lambda \\approx 0$, OLS was already near-optimal. The framework adapts automatically.\n\n### Constrained Methods Superior to Unconstrained\n\nThe Penalized and Constrained (PAC) optimization method developed by @james2020pac found:\n\n> \"The results suggest that PAC and relaxed PAC are surprisingly robust to random violations in the constraints. While both methods deteriorated slightly as [constraint error] increased, they were still both superior to their unconstrained counterparts for all values of [error] and all settings.\"\n\n## Research Contribution\n\nThis paper provides a practical guide and framework that combines penalized regularization (Lasso, Ridge, Elastic Net) with constrained optimization in the context of cost estimation. Key contributions include:\n\n1. **Investigation of small-sample applicability**: Most research on penalized methods uses large datasets---does it apply to cost estimation's typical 5-30 point samples?\n\n2. **Python package implementation**: The `penalized_constrained` package combines Elastic Net penalties with lower and upper bound constraints on coefficients\n\n3. **Proper diagnostic adjustments**: Via Generalized Degrees of Freedom (GDF) for constraint-driven regression [@hu2010gdf]\n\n4. **Cross-validation framework**: For likelihood-free hyperparameter selection\n\n5. **sklearn-compatible implementation**: For practical application in existing workflows\n\n6. **Comprehensive benchmarks**: Simulation data comparing proposed method against OLS, Ridge, Lasso, and constrained-only approaches across varying sample sizes, correlations, and error variances\n\n7. **Practical guidance**: On when and how to implement these algorithms\n\n**Future Work**: Validation on actual program data using publicly available Selected Acquisition Reports (SARs), which are not subject to CUI restrictions.\n\n## Paper Organization\n\nThe remainder of this paper is organized as follows: @sec-methodology presents the mathematical formulation and algorithm details, @sec-simulation-design describes our Monte Carlo study design, @sec-results presents the empirical findings, @sec-doe-analysis provides rigorous statistical analysis using DOE methodology, and @sec-discussion offers practical recommendations and discusses limitations.\n\n\n# Methodology {#sec-methodology}\n\n## Problem Formulation\n\nGiven training data $(X, y)$ where $X \\in \\mathbb{R}^{n \\times p}$ and $y \\in \\mathbb{R}^n$, Penalized-Constrained Regression solves:\n\n$$\\min_{\\beta} \\mathcal{L}(\\beta) + \\alpha \\cdot \\text{l1\\_ratio} \\cdot \\|\\beta\\|_1 + \\frac{1}{2} \\cdot \\alpha \\cdot (1 - \\text{l1\\_ratio}) \\cdot \\|\\beta\\|_2^2$$\n\nsubject to:\n\n$$l_j \\leq \\beta_j \\leq u_j \\quad \\forall j \\in \\{1, \\ldots, p\\}$$\n\nwhere:\n\n- $\\mathcal{L}(\\beta)$ is the loss function (e.g., SSPE, MSE, or user-defined; default is SSPE)\n- $\\alpha \\geq 0$ is the overall penalty strength; $\\alpha = 0$ recovers constrained-only optimization\n- $\\text{l1\\_ratio} \\in [0,1]$ is the mix between L1 and L2; l1\\_ratio = 1 is Lasso, l1\\_ratio = 0 is Ridge\n- $l_j, u_j$ are the lower and upper bounds for coefficient $j$\n\n### Why sklearn Elastic Net Parameterization?\n\nWe adopt sklearn's $(\\alpha, \\text{l1\\_ratio})$ parameterization rather than the standard statistical $(\\lambda_1, \\lambda_2)$ formulation for several practical reasons:\n\n1. **Consistency**: sklearn is the dominant machine learning library in Python, enabling direct comparison with sklearn's ElasticNetCV results\n2. **Intuitive interpretation**: l1\\_ratio provides clear meaning---0 = pure Ridge, 1 = pure Lasso, 0.5 = balanced\n3. **Reproducibility**: Practitioners can directly use sklearn's cross-validation infrastructure for hyperparameter tuning before applying constraints\n\nThe conversion is straightforward: $\\lambda_1 = \\alpha \\cdot \\text{l1\\_ratio}$ and $\\lambda_2 = \\alpha \\cdot (1 - \\text{l1\\_ratio})$.\n\n## Loss Functions\n\nThe framework supports multiple loss functions $\\mathcal{L}(\\beta)$, each with distinct properties:\n\n| Loss | Formula | Properties | Use Case |\n|------|---------|------------|----------|\n| SSE | $\\sum(y_i - \\hat{y}_i)^2$ | Convex; closed-form for linear | Standard OLS setting |\n| SSPE | $\\sum[(y_i - \\hat{y}_i)/y_i]^2$ | Unit space; MUPE-consistent | Default for CERs |\n| LAD | $\\sum\\|y_i - \\hat{y}_i\\|$ | Robust to outliers | Heavy-tailed errors |\n\n: Loss function options {#tbl-loss-functions}\n\n### Why Unit Space (SSPE)?\n\nThe default loss function is Sum of Squared Percentage Errors:\n\n$$\\text{SSPE} = \\sum_{i=1}^{n} \\left( \\frac{y_i - f(X_i, \\beta)}{y_i} \\right)^2$$\n\nThis produces directly interpretable percentage errors while penalizing large errors proportionally to the magnitude of actual costs. SSPE is comparable to the MUPE formulation widely used in cost estimation [@hu2007mupe].\n\n::: {.callout-tip title=\"Future Work: MUPE and pcLAD\"}\n**MUPE** (Minimum Unbiased Percentage Error) employs Iteratively Reweighted Least Squares (IRLS), producing the Best Linear Unbiased Estimator for multiplicative error models [@hu2007mupe].\n\n**pcLAD** (Penalized-Constrained Least Absolute Deviation) offers superior performance with heavy-tailed errors or outliers: \"pcLAD enjoys the Oracle property even with Cauchy-distributed errors\" [@wu2022pclad].\n:::\n\n## Convexity and Global Optimality\n\nGlobal optimality is guaranteed when:\n\n1. The loss function $\\mathcal{L}(\\beta)$ is convex (e.g., SSE with linear model)\n2. All constraints define a convex feasible region (linear inequalities/equalities)\n3. Regularization terms (L1 and L2 penalties) are convex\n\n::: {.callout-warning title=\"Non-Convex Cases\"}\nFor nonlinear models (e.g., power forms $Y = aX^bZ^c$) with SSPE objective, the problem is non-convex. Local minima are possible. ZMPE is documented to be sensitive to starting points [@hu2007mupe]. **Recommendation**: Test multiple starting points; COBYLA optimizer recommended for ZMPE-type problems [@schiavoni2021assessing].\n:::\n\n## Note on BLUE\n\n::: {.callout-note title=\"Best Linear Unbiased Estimator (BLUE)\"}\nBy the Gauss-Markov theorem, OLS is BLUE under classical assumptions. Introducing penalties and/or constraints means the resulting estimator is **no longer BLUE**. This is an intentional tradeoff: we accept bias in exchange for reduced variance (bias-variance tradeoff), improved interpretability (domain-consistent coefficients), and enhanced predictive accuracy (regularization benefits).\n\nPer the Theobald-Farebrother theorem, this tradeoff yields lower MSE for some $\\lambda > 0$. Cross-validation identifies when that $\\lambda$ is near zero (OLS suffices) versus when substantial regularization helps.\n:::\n\n## Algorithm Overview\n\nThe high-level algorithm proceeds as follows:\n\n1. **Input**: Data $(X, y)$, functional form $f(X, \\beta)$, penalty parameters $(\\alpha, \\text{l1\\_ratio})$, bounds/constraints, loss function, optimizer choice\n\n2. **Scale** (optional): Standardize $X$ (mean=0, std=1) if `scale=True`\n\n3. **Initialize**: Starting coefficients from OLS (default when possible), trimmed to satisfy bounds; alternatively zeros or user-specified\n\n4. **Optimize**: Solve constrained penalized minimization via selected optimizer\n\n5. **Unscale**: Transform coefficients back to original units: $\\beta_{\\text{original}} = \\beta_{\\text{scaled}} / \\sigma$\n\n6. **Output**: Coefficient estimates $\\hat{\\beta}$, fit statistics (GDF-adjusted), `active_constraints_` flag\n\n### Optimization Methods\n\nThe implementation uses general constrained solvers from `scipy.optimize.minimize`:\n\n- **SLSQP** (Sequential Least-Squares Quadratic Programming): Current default. Handles bounds and linear constraints efficiently.\n- **COBYLA** (Constrained Optimization BY Linear Approximation): Derivative-free; recommended for ZMPE-type problems [@schiavoni2021assessing].\n- **trust-constr**: Interior point method; suitable for larger-scale problems with many constraints.\n\nSee @sec-appendix-algorithm for detailed algorithm information including initialization strategy and scaling considerations.\n\n## Degrees of Freedom for Constrained Models\n\n::: {.callout-important title=\"Critical Issue\"}\nWhen constraints are imposed on regression coefficients, the effective degrees of freedom (DF) must be adjusted. Without this adjustment, fit statistics (SEE, SPE, Adjusted R²) are incorrect and misleading [@hu2010gdf].\n:::\n\n### Hu's GDF Formula\n\n$$\\text{GDF} = n - p - (\\text{\\# Constraints}) + (\\text{\\# Redundancies})$$\n\nwhere $p$ is the number of estimated parameters and $n$ is the sample size. One restriction is equivalent to a loss of one DF.\n\n### Gaines et al. Constrained Lasso DF Formula\n\n$$\\text{df} = |\\text{Active predictors}| - (\\text{\\# equality constraints}) - (\\text{\\# binding inequality constraints})$$\n\nThe key difference: Gaines' formulation [@gaines2018constrained] only counts binding inequality constraints, while Hu's formulation counts all specified constraints. See @sec-appendix-gdf for detailed comparison.\n\n### GDF-Adjusted Fit Statistics\n\n$$\\text{SEE} = \\sqrt{\\frac{\\sum(y_i - \\hat{y}_i)^2}{\\text{GDF}}} \\qquad \\text{SPE} = \\sqrt{\\frac{\\sum((y_i - \\hat{y}_i)/\\hat{y}_i)^2}{\\text{GDF}}}$$\n\n## Model Diagnostics and Validation\n\n### Cross-Validation as Primary Model Selection\n\nSince penalized-constrained estimators do not follow standard likelihood theory, cross-validation serves as the primary arbiter of model quality [@hastie2009elements]:\n\n- Directly estimates out-of-sample prediction error without distributional assumptions\n- Enables comparison across fundamentally different model classes (OLS vs. penalized vs. constrained)\n- Provides a principled basis for hyperparameter selection ($\\alpha$, l1\\_ratio)\n\nFor cost estimation with small samples, Leave-One-Out Cross-Validation (LOOCV) is recommended to maximize training data usage. K-fold CV (k=5 or k=10) provides a computationally cheaper alternative.\n\n### Coefficient Uncertainty Estimation\n\nFour approaches are available for likelihood-free coefficient uncertainty:\n\n1. **Hessian-based covariance**: Uses the inverse Hessian of the objective function at the solution. Available directly from most optimizers. Assumes local quadratic approximation is valid.\n\n2. **Jacobian-based**: Estimates covariance from the Jacobian of residuals, following `scipy.optimize.curve_fit` methodology. Appropriate for nonlinear least squares problems.\n\n3. **Bootstrap**: Resamples data and re-estimates coefficients to build empirical confidence intervals [@efron1993bootstrap]. Does not require distributional assumptions. *Caveat*: For penalized models, bootstrap CIs may be artificially narrow because penalties constrain coefficient variability across resamples.\n\n4. **Profile likelihood**: Varies each coefficient individually while re-optimizing others to trace out confidence regions. Most computationally expensive but most robust for non-quadratic objective surfaces.\n\nThis implementation supports Bootstrap and Hessian-based covariance methods.\n\n## Hyperparameter Selection\n\nThe regularization strength $\\alpha$ is selected via:\n\n1. **Cross-validation**: Minimize out-of-fold SSPE (primary method)\n2. **AICc**: Corrected Akaike Information Criterion\n3. **GCV**: Generalized Cross-Validation\n\nFor this simulation study, we use 5-fold cross-validation with an alpha grid spanning $10^{-5}$ to $1.0$.\n\n\n# Simulation Study Design {#sec-simulation-design}\n\n::: {#setup-simulation .cell .hidden execution_count=1}\n``` {.python .cell-code .hidden}\nimport sys\nfrom pathlib import Path\n\n# Find project root by looking for pyproject.toml\ndef find_project_root():\n    current = Path.cwd()\n    for parent in [current] + list(current.parents):\n        if (parent / \"pyproject.toml\").exists():\n            return parent\n    return current.parent.parent  # Fallback\n\nproject_root = find_project_root()\nsys.path.insert(0, str(project_root))\nsys.path.insert(0, str(project_root / \"scripts\"))\n```\n:::\n\n\n:::{#ac5886fe .cell .markdown}\n## Data Generating Process\n\nWe simulate learning curve data using the multiplicative power-law model:\n\n$$Y = T_1 \\cdot X_1^b \\cdot X_2^c \\cdot \\exp(\\epsilon)$$\n\nwhere:\n\n- $T_1 = 100$ (first unit cost)---**Note**: $T_1$ is estimated in all examples, not fixed\n- $X_1$ = lot midpoint (cumulative units)\n- $X_2$ = lot quantity (rate variable)\n- $\\epsilon \\sim N(0, \\sigma^2)$ with $\\sigma^2 = \\log(1 + \\text{cv\\_error}^2)$\n\nThe correlation between $\\log(X_1)$ and $\\log(X_2)$ is controlled to study multicollinearity effects. Correlated predictor datasets are generated using a custom function that creates realistic production ramp-up schedules where lot midpoint and quantity naturally correlate at the specified $\\rho$ levels.\n\n## Experimental Design\n\nWe employ a full factorial design with 5 factors:\n\n| Factor | Levels | Values |\n|--------|--------|--------|\n| Sample size (n_lots) | 3 | 5, 10, 30 |\n| Predictor correlation | 3 | 0.0, 0.5, 0.9 |\n| CV error (noise) | 3 | 0.01, 0.10, 0.20 |\n| Learning rate (b) | 3 | log(0.85), log(0.90), log(0.95) / log(2) |\n| Rate effect (c) | 3 | log(0.80), log(0.85), log(0.90) / log(2) |\n\nThis yields $3^5 = 243$ unique scenarios. Each scenario is replicated 25 times with different random seeds, producing **6,075 total scenario-replications**.\n\n## Models Compared\n\nWe evaluate 18 regression approaches:\n\n### Traditional Methods (Log-Space)\n- **OLS**: Ordinary Least Squares on log-transformed data (base case, unconstrained, unpenalized)\n- **OLS_LearnOnly**: OLS with only the learning variable (confluence analysis remedy---drop rate variable)\n- **Ridge**: Ridge regression with CV-selected $\\lambda$, no constraints\n- **Lasso**: Lasso regression with CV-selected $\\lambda$, no constraints\n- **BayesianRidge**: sklearn implementation using spherical Gaussian prior $p(w|\\lambda) = N(w|0, \\lambda^{-1}I)$ centered at zero. Iteratively maximizes marginal log-likelihood to optimize regularization parameters [@mackay1992bayesian; @tipping2001sparse]. *Note*: Uses uninformative priors without domain-specific constraints, serving as comparison to automatic regularization without domain knowledge.\n- **PLS / PCA_Linear**: Both produce identical results for two predictors\n\n### PCReg Variants (Unit-Space)\n- **PCReg_ConstrainOnly**: Constraints only, no penalty ($\\alpha = 0$)\n- **PCReg_CV**: Constraints + CV-tuned elastic net with loose bounds ($\\beta \\in [-1, 0]$)\n- **PCReg_AICc**: Constraints + AICc-selected penalty\n- **PCReg_GCV**: Constraints + GCV-selected penalty\n- **PCReg_CV_Tight**: With tight bounds near true values. This serves as an approximate upper bound on performance---analogous to an \"oracle\" benchmark that knows the true parameter region.\n- **PCReg_CV_Wrong**: With deliberately incorrect constraints. Following @james2020pac methodology, \"wrong\" constraints are generated by adding random noise to the true bounds: $\\text{constraint}_{\\text{wrong}} = \\text{constraint}_{\\text{true}} + a \\cdot U(-1,1)$, where $a$ controls the magnitude of constraint error. This tests robustness to constraint misspecification.\n\n### PCReg MSE Loss Variants\n- **PCReg_MSE**: MSE loss with constraints\n- **PCReg_MSE_CV**: MSE loss with CV-tuned penalty\n\n### PCReg Log-Space Variants\n- **PCReg_LogMSE**: Log-transformed MSE loss\n- **PCReg_LogMSE_CV**: Log-transformed with CV penalty\n\n## Evaluation Metrics\n\nAll models are evaluated on 5 held-out test lots using:\n\n1. **Test SSPE**: Sum of squared percentage errors (primary metric)\n2. **Test MAPE**: Mean absolute percentage error\n3. **Test MSE**: Mean squared error\n4. **Coefficient bias**: $|\\hat{\\beta} - \\beta_{\\text{true}}|$ for $b$ (learning) and $c$ (rate)\n5. **Coefficient variance**: Stability across replications\n6. **Sign correctness (Domain consistency rate)**: Whether $\\hat{b} \\leq 0$ and $\\hat{c} \\leq 0$---percentage of replications with correct coefficient signs\n\n## Computational Details\n\n- **Parallelization**: joblib with all available CPU cores\n- **Resume capability**: Results saved in batches to parquet files\n- **Model hashing**: Automatic re-run if model definitions change\n- **Total model fits**: $243 \\times 25 \\times 18 = 109,350$\n\n\n# Results {#sec-results}\n:::\n\n::: {#setup-results .cell .hidden execution_count=2}\n``` {.python .cell-code .hidden}\nimport sys\nfrom pathlib import Path\n\n# Find project root by looking for pyproject.toml\ndef find_project_root():\n    current = Path.cwd()\n    for parent in [current] + list(current.parents):\n        if (parent / \"pyproject.toml\").exists():\n            return parent\n    return current.parent.parent  # Fallback\n\nproject_root = find_project_root()\nsys.path.insert(0, str(project_root))\nsys.path.insert(0, str(project_root / \"scripts\"))\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom scripts.ICEAA.analysis import (\n    load_simulation_results,\n    create_overall_boxplot,\n    create_boxplot_by_factor,\n    create_win_rate_heatmap,\n    create_sign_correctness_plot,\n)\nfrom scripts.ICEAA.analysis.visualization import create_wrong_sign_heatmap\n\n# Load data\ndf = load_simulation_results()\nprint(f\"Loaded {len(df):,} observations across {df['model_name'].nunique()} models\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoaded 85,050 observations across 14 models\n```\n:::\n:::\n\n\n:::{#e4d0d88d .cell .markdown}\nThis section presents the results of our Monte Carlo simulation study comparing Penalized-Constrained Regression (PCReg) against traditional methods.\n\n## Overall Model Performance\n:::\n\n::: {.cell fig-height='6' fig-width='12' execution_count=3}\n``` {.python .cell-code .hidden}\nfig = create_overall_boxplot(df)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Overall model performance comparison across all scenarios. Models are ordered by mean Test SSPE (lower is better). Green boxes indicate PCReg variants; blue boxes indicate standard methods. The winner is highlighted with a red border.](paper_files/figure-docx/fig-overall-output-1.png){#fig-overall}\n:::\n:::\n\n\n:::{#ede07d63 .cell .markdown}\n@fig-overall shows the distribution of Test SSPE across all 6,075 scenario-replications. Key observations:\n:::\n\n::: {#tbl-overall-stats .cell tbl-cap='Summary statistics by model, ranked by mean Test SSPE' execution_count=4}\n``` {.python .cell-code .hidden}\nstats = df.groupby('model_name')['test_sspe'].agg(['mean', 'std', 'median', 'count'])\nstats = stats.sort_values('mean').round(4)\nstats.columns = ['Mean SSPE', 'Std', 'Median', 'Count']\nstats\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Mean SSPE</th>\n      <th>Std</th>\n      <th>Median</th>\n      <th>Count</th>\n    </tr>\n    <tr>\n      <th>model_name</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>PCReg_CV_Tight</th>\n      <td>0.0465</td>\n      <td>0.0998</td>\n      <td>0.0040</td>\n      <td>6075</td>\n    </tr>\n    <tr>\n      <th>PCReg_ConstrainOnly</th>\n      <td>0.0798</td>\n      <td>0.2637</td>\n      <td>0.0052</td>\n      <td>6075</td>\n    </tr>\n    <tr>\n      <th>PCReg_LogMSE</th>\n      <td>0.0862</td>\n      <td>0.2888</td>\n      <td>0.0053</td>\n      <td>6075</td>\n    </tr>\n    <tr>\n      <th>PCReg_GCV</th>\n      <td>0.0937</td>\n      <td>0.3209</td>\n      <td>0.0052</td>\n      <td>6075</td>\n    </tr>\n    <tr>\n      <th>PCReg_MSE</th>\n      <td>0.1062</td>\n      <td>0.3328</td>\n      <td>0.0076</td>\n      <td>6075</td>\n    </tr>\n    <tr>\n      <th>PCReg_MSE_CV</th>\n      <td>0.1079</td>\n      <td>0.3389</td>\n      <td>0.0079</td>\n      <td>6075</td>\n    </tr>\n    <tr>\n      <th>PCReg_CV</th>\n      <td>0.1083</td>\n      <td>0.4609</td>\n      <td>0.0050</td>\n      <td>6075</td>\n    </tr>\n    <tr>\n      <th>PCReg_CV_Wrong</th>\n      <td>0.1090</td>\n      <td>0.4648</td>\n      <td>0.0049</td>\n      <td>6075</td>\n    </tr>\n    <tr>\n      <th>OLS</th>\n      <td>0.1146</td>\n      <td>0.7557</td>\n      <td>0.0055</td>\n      <td>6075</td>\n    </tr>\n    <tr>\n      <th>BayesianRidgeModel</th>\n      <td>0.1409</td>\n      <td>0.7857</td>\n      <td>0.0057</td>\n      <td>6075</td>\n    </tr>\n    <tr>\n      <th>PCReg_LogMSE_CV</th>\n      <td>0.1525</td>\n      <td>0.6453</td>\n      <td>0.0066</td>\n      <td>6075</td>\n    </tr>\n    <tr>\n      <th>Ridge</th>\n      <td>0.1580</td>\n      <td>1.0644</td>\n      <td>0.0058</td>\n      <td>6075</td>\n    </tr>\n    <tr>\n      <th>Lasso</th>\n      <td>0.2014</td>\n      <td>1.0043</td>\n      <td>0.0072</td>\n      <td>6075</td>\n    </tr>\n    <tr>\n      <th>OLS_LearnOnly</th>\n      <td>0.8658</td>\n      <td>2.5266</td>\n      <td>0.1410</td>\n      <td>6075</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::{#b09c2deb .cell .markdown}\n## Sign Correctness Analysis\n\nA critical advantage of PCReg is enforcing domain-appropriate coefficient signs. Learning curve slopes should be negative ($b \\leq 0$, $c \\leq 0$).\n:::\n\n::: {.cell fig-height='5' fig-width='10' execution_count=5}\n``` {.python .cell-code .hidden}\nfig = create_sign_correctness_plot(df)\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n```\nC:\\Users\\KevinJoy\\OneDrive - Herren Associates\\Constrained-Penalized Regression Paper - General\\penalized_constrained\\scripts\\ICEAA\\analysis\\visualization.py:288: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(sign_df[\"model\"], rotation=45, ha=\"right\")\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Percentage of scenarios where both coefficients have correct signs (negative). PCReg methods achieve 100% sign correctness by design.](paper_files/figure-docx/fig-sign-correctness-output-2.png){#fig-sign-correctness}\n:::\n:::\n\n\n::: {.cell fig-height='5' fig-width='8' execution_count=6}\n``` {.python .cell-code .hidden}\nfig = create_wrong_sign_heatmap(df, 'n_lots', 'target_correlation', 'OLS')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![OLS wrong sign rate by sample size and predictor correlation. Wrong signs are most common with small samples and high correlation.](paper_files/figure-docx/fig-wrong-sign-heatmap-output-1.png){#fig-wrong-sign-heatmap}\n:::\n:::\n\n\n:::{#8d9edcdd .cell .markdown}\n## Head-to-Head Comparison: PCReg vs OLS\n:::\n\n::: {#pcreg-vs-ols .cell .hidden execution_count=7}\n``` {.python .cell-code .hidden}\nfrom scripts.ICEAA.analysis.load_results import get_model_comparison\n\ncomparison = get_model_comparison(df, 'OLS', 'PCReg_ConstrainOnly', 'test_sspe')\npcreg_wins = comparison['b_wins'].sum()\ntotal = len(comparison)\nwin_rate = pcreg_wins / total\n\n# When OLS has wrong sign\nif 'any_wrong_sign' in comparison.columns:\n    wrong_sign = comparison[comparison['any_wrong_sign']]\n    correct_sign = comparison[~comparison['any_wrong_sign']]\n    wrong_sign_win_rate = wrong_sign['b_wins'].mean() if len(wrong_sign) > 0 else 0\n    correct_sign_win_rate = correct_sign['b_wins'].mean() if len(correct_sign) > 0 else 0\n```\n:::\n\n\n:::{#beeb98da .cell .markdown}\nComparing PCReg (constraints only, $\\alpha = 0$) against OLS:\n\n- **Overall win rate**: PCReg wins in 58\\.2% of scenarios\n- **When OLS has wrong sign**: PCReg wins in 81\\.2% of scenarios\n- **When OLS has correct sign**: PCReg wins in 56\\.6% of scenarios\n\n## Performance by Design Factors\n\n### Effect of Sample Size\n:::\n\n::: {.cell fig-height='6' fig-width='15' execution_count=8}\n``` {.python .cell-code .hidden}\nfig = create_boxplot_by_factor(df, 'n_lots')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Model performance by sample size. PCReg shows strongest advantage with small samples (n=5).](paper_files/figure-docx/fig-by-n-lots-output-1.png){#fig-by-n-lots}\n:::\n:::\n\n\n:::{#56007bed .cell .markdown}\n### Effect of CV Error (Data Quality)\n:::\n\n::: {.cell fig-height='6' fig-width='15' execution_count=9}\n``` {.python .cell-code .hidden}\nfig = create_boxplot_by_factor(df, 'cv_error')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Model performance by CV error level. PCReg excels when data quality is high (CV error = 0.01).](paper_files/figure-docx/fig-by-cv-error-output-1.png){#fig-by-cv-error}\n:::\n:::\n\n\n:::{#744e8ba0 .cell .markdown}\n### Effect of Predictor Correlation\n:::\n\n::: {.cell fig-height='6' fig-width='15' execution_count=10}\n``` {.python .cell-code .hidden}\nfig = create_boxplot_by_factor(df, 'target_correlation')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Model performance by predictor correlation. High correlation (0.9) leads to more variable OLS estimates.](paper_files/figure-docx/fig-by-correlation-output-1.png){#fig-by-correlation}\n:::\n:::\n\n\n:::{#2530181e .cell .markdown}\n## Interaction Effects\n:::\n\n::: {.cell fig-height='6' fig-width='8' execution_count=11}\n``` {.python .cell-code .hidden}\nfig = create_win_rate_heatmap(df, 'n_lots', 'cv_error', 'PCReg_ConstrainOnly', 'OLS')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![PCReg improvement over OLS by sample size and CV error. Positive values (green) indicate PCReg is better.](paper_files/figure-docx/fig-heatmap-n-cv-output-1.png){#fig-heatmap-n-cv}\n:::\n:::\n\n\n::: {.cell fig-height='6' fig-width='8' execution_count=12}\n``` {.python .cell-code .hidden}\nfig = create_win_rate_heatmap(df, 'n_lots', 'target_correlation', 'PCReg_ConstrainOnly', 'OLS')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![PCReg improvement over OLS by sample size and predictor correlation.](paper_files/figure-docx/fig-heatmap-n-corr-output-1.png){#fig-heatmap-n-corr}\n:::\n:::\n\n\n:::{#f8786924 .cell .markdown}\n## Value of Penalization\n\nDoes adding elastic net penalty ($\\alpha > 0$) improve upon constraints alone ($\\alpha = 0$)?\n:::\n\n::: {#alpha-comparison .cell execution_count=13}\n``` {.python .cell-code .hidden}\n# Compare PCReg_CV (alpha tuned via CV) vs PCReg_ConstrainOnly (alpha=0)\ncv_comparison = get_model_comparison(df, 'PCReg_ConstrainOnly', 'PCReg_CV', 'test_sspe')\ncv_wins = cv_comparison['b_wins'].mean()\n\nprint(f\"PCReg_CV (with penalty) beats PCReg_ConstrainOnly: {cv_wins:.1%} of scenarios\")\nprint(f\"Conclusion: Constraints alone often sufficient; penalty adds modest benefit\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPCReg_CV (with penalty) beats PCReg_ConstrainOnly: 42.1% of scenarios\nConclusion: Constraints alone often sufficient; penalty adds modest benefit\n```\n:::\n:::\n\n\n:::{#2758e2ac .cell .markdown}\nThis finding suggests that for learning curve estimation, the primary value of PCReg comes from the constraints rather than the penalty term.\n\n\n# Statistical Analysis {#sec-doe-analysis}\n:::\n\n::: {#setup-doe .cell .hidden execution_count=14}\n``` {.python .cell-code .hidden}\nimport sys\nfrom pathlib import Path\n\n# Find project root by looking for pyproject.toml\ndef find_project_root():\n    current = Path.cwd()\n    for parent in [current] + list(current.parents):\n        if (parent / \"pyproject.toml\").exists():\n            return parent\n    return current.parent.parent  # Fallback\n\nproject_root = find_project_root()\nsys.path.insert(0, str(project_root))\nsys.path.insert(0, str(project_root / \"scripts\"))\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\nfrom scripts.ICEAA.analysis import load_simulation_results\n\ndf = load_simulation_results()\n\n# Try importing pingouin for statistical tests\ntry:\n    import pingouin as pg\n    HAS_PINGOUIN = True\nexcept ImportError:\n    HAS_PINGOUIN = False\n    print(\"Note: Install pingouin for full statistical analysis: pip install pingouin\")\n```\n:::\n\n\n:::{#796f60c4 .cell .markdown}\nThis section provides rigorous statistical analysis using Design of Experiments (DOE) methodology.\n\n## Repeated Measures ANOVA\n\nWe use repeated measures ANOVA to test whether model choice significantly affects performance, accounting for the fact that all models are evaluated on the same data scenarios.\n:::\n\n::: {#rm-anova .cell execution_count=15}\n``` {.python .cell-code .hidden}\n# Create scenario ID for repeated measures structure\nscenario_cols = ['n_lots', 'target_correlation', 'cv_error', 'learning_rate', 'rate_effect', 'replication']\ndf['scenario_id'] = df.groupby(scenario_cols).ngroup()\n\n# Focus on key models for statistical comparison\nDOE_MODELS = ['OLS', 'PCReg_ConstrainOnly', 'PCReg_CV', 'PCReg_CV_Tight']\ndf_doe = df[df['model_name'].isin(DOE_MODELS)].copy()\n\n# Repeated measures ANOVA\nrm_aov = pg.rm_anova(\n    data=df_doe,\n    dv='test_sspe',\n    within='model_name',\n    subject='scenario_id',\n    correction=True\n)\n\nprint(\"Repeated Measures ANOVA Results:\")\nprint(\"=\"*60)\ndisplay(rm_aov)\n\n# Extract key statistics\neta_sq_col = 'np2' if 'np2' in rm_aov.columns else 'ng2'\neta_sq = rm_aov[eta_sq_col].values[0]\nf_val = rm_aov['F'].values[0]\np_col = 'p-GG-corr' if 'p-GG-corr' in rm_aov.columns else 'p-unc'\np_val = rm_aov[p_col].values[0]\n\nprint(f\"\\nInterpretation:\")\nprint(f\"  F-statistic: {f_val:.2f}\")\nprint(f\"  p-value: {p_val:.4e}\")\nprint(f\"  Effect size (η²): {eta_sq:.4f}\")\nprint(f\"  Model choice explains {eta_sq*100:.1f}% of variance in test SSPE\")\n```\n:::\n\n\n:::{#7e6ac7d4 .cell .markdown}\n## Pairwise Comparisons\n:::\n\n::: {#pairwise .cell execution_count=16}\n``` {.python .cell-code .hidden}\npairwise = pg.pairwise_tests(\n    data=df_doe,\n    dv='test_sspe',\n    within='model_name',\n    subject='scenario_id',\n    padjust='holm',\n    effsize='hedges'\n)\n\nprint(\"Pairwise Comparisons (Holm-Bonferroni corrected):\")\nprint(\"=\"*60)\n\n# Focus on key comparisons\nkey_pairs = [\n    ('PCReg_CV', 'OLS'),\n    ('PCReg_ConstrainOnly', 'OLS'),\n    ('PCReg_CV_Tight', 'OLS'),\n    ('PCReg_CV_Tight', 'PCReg_CV'),\n]\n\nresults = []\nfor a, b in key_pairs:\n    row = pairwise[(pairwise['A'] == a) & (pairwise['B'] == b)]\n    if len(row) == 0:\n        row = pairwise[(pairwise['A'] == b) & (pairwise['B'] == a)]\n\n    if len(row) > 0:\n        row = row.iloc[0]\n        g = row['hedges']\n        p = row['p-corr']\n\n        sig = '***' if p < 0.001 else ('**' if p < 0.01 else ('*' if p < 0.05 else 'ns'))\n        g_abs = abs(g)\n        g_size = 'negligible' if g_abs < 0.2 else ('small' if g_abs < 0.5 else ('medium' if g_abs < 0.8 else 'large'))\n        better = a if g < 0 else b\n\n        results.append({\n            'Comparison': f'{a} vs {b}',\n            'Hedges g': f'{g:.4f}',\n            'p-value': f'{p:.4f}',\n            'Significance': sig,\n            'Effect Size': g_size,\n            'Better Model': better\n        })\n\nresults_df = pd.DataFrame(results)\ndisplay(results_df)\n```\n:::\n\n\n:::{#37b35f13 .cell .markdown}\n## Win Rate Analysis\n:::\n\n::: {#win-rates .cell execution_count=17}\n``` {.python .cell-code .hidden}\n# Prepare wide format for win rate calculation\nscenario_cols = ['n_lots', 'target_correlation', 'cv_error', 'learning_rate', 'rate_effect', 'replication']\ndf_wide = df.pivot_table(\n    index=scenario_cols,\n    columns='model_name',\n    values='test_sspe'\n).reset_index()\n\n# Overall win rate: PCReg_ConstrainOnly vs OLS\nif 'PCReg_ConstrainOnly' in df_wide.columns and 'OLS' in df_wide.columns:\n    df_wide['pcreg_wins'] = df_wide['PCReg_ConstrainOnly'] < df_wide['OLS']\n\n    overall_wins = df_wide['pcreg_wins'].sum()\n    overall_total = len(df_wide)\n    overall_rate = overall_wins / overall_total\n\n    # Binomial test\n    binom_result = stats.binomtest(overall_wins, overall_total, p=0.5, alternative='greater')\n\n    print(\"PCReg_ConstrainOnly vs OLS Win Rate Analysis\")\n    print(\"=\"*60)\n    print(f\"  Overall Win Rate: {overall_rate:.1%} ({overall_wins}/{overall_total})\")\n    print(f\"  Binomial Test p-value: {binom_result.pvalue:.4e}\")\n\n    if binom_result.pvalue < 0.05:\n        print(\"  ✓ PCReg significantly outperforms OLS (p < 0.05)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPCReg_ConstrainOnly vs OLS Win Rate Analysis\n============================================================\n  Overall Win Rate: 58.2% (3536/6075)\n  Binomial Test p-value: 7.4510e-38\n  ✓ PCReg significantly outperforms OLS (p < 0.05)\n```\n:::\n:::\n\n\n:::{#ac50b1c1 .cell .markdown}\n## Win Rates by Design Factor\n:::\n\n::: {.cell execution_count=18}\n``` {.python .cell-code .hidden}\nFACTORS = ['n_lots', 'cv_error', 'target_correlation']\n\nprint(\"Win Rates by Design Factor\")\nprint(\"=\"*60)\n\nwin_rates = []\nfor factor in FACTORS:\n    print(f\"\\n{factor}:\")\n    for level in sorted(df_wide[factor].unique()):\n        mask = df_wide[factor] == level\n        level_wins = df_wide.loc[mask, 'pcreg_wins'].sum()\n        level_total = mask.sum()\n        level_rate = level_wins / level_total\n\n        binom = stats.binomtest(level_wins, level_total, p=0.5, alternative='greater')\n        sig = '*' if binom.pvalue < 0.05 else ''\n\n        print(f\"  {level}: {level_rate:.1%} ({level_wins}/{level_total}) {sig}\")\n\n        win_rates.append({\n            'Factor': factor,\n            'Level': level,\n            'Win Rate': f'{level_rate:.1%}',\n            'p-value': f'{binom.pvalue:.4f}',\n            'Significant': 'Yes' if binom.pvalue < 0.05 else 'No'\n        })\n\n# Display as table\nprint(\"\\n\")\nwin_rates_df = pd.DataFrame(win_rates)\ndisplay(win_rates_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWin Rates by Design Factor\n============================================================\n\nn_lots:\n  5: 65.5% (1327/2025) *\n  10: 59.3% (1200/2025) *\n  30: 49.8% (1009/2025) \n\ncv_error:\n  0.01: 71.8% (1453/2025) *\n  0.1: 56.5% (1145/2025) *\n  0.2: 46.3% (938/2025) \n\ntarget_correlation:\n  0.0: 62.3% (1261/2025) *\n  0.5: 55.4% (1122/2025) *\n  0.9: 56.9% (1153/2025) *\n\n\n```\n:::\n\n::: {#win-rates-by-factor .cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Factor</th>\n      <th>Level</th>\n      <th>Win Rate</th>\n      <th>p-value</th>\n      <th>Significant</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>n_lots</td>\n      <td>5.00</td>\n      <td>65.5%</td>\n      <td>0.0000</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>n_lots</td>\n      <td>10.00</td>\n      <td>59.3%</td>\n      <td>0.0000</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>n_lots</td>\n      <td>30.00</td>\n      <td>49.8%</td>\n      <td>0.5705</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cv_error</td>\n      <td>0.01</td>\n      <td>71.8%</td>\n      <td>0.0000</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>cv_error</td>\n      <td>0.10</td>\n      <td>56.5%</td>\n      <td>0.0000</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>cv_error</td>\n      <td>0.20</td>\n      <td>46.3%</td>\n      <td>0.9996</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>target_correlation</td>\n      <td>0.00</td>\n      <td>62.3%</td>\n      <td>0.0000</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>target_correlation</td>\n      <td>0.50</td>\n      <td>55.4%</td>\n      <td>0.0000</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>target_correlation</td>\n      <td>0.90</td>\n      <td>56.9%</td>\n      <td>0.0000</td>\n      <td>Yes</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::{#e158ffc1 .cell .markdown}\n## Key Statistical Findings\n\nBased on the statistical analysis:\n\n1. **Model choice matters**: The repeated measures ANOVA confirms significant differences between models (p < 0.001)\n\n2. **PCReg outperforms OLS**: The overall win rate exceeds 50% and is statistically significant\n\n3. **Effect sizes are meaningful**: Hedges' g values indicate practically important differences\n\n4. **Context matters**: Win rates vary substantially by design factor levels, suggesting PCReg is particularly valuable in specific conditions\n\n\n# Discussion and Recommendations {#sec-discussion}\n:::\n\n::: {#setup-discussion .cell .hidden execution_count=19}\n``` {.python .cell-code .hidden}\nimport sys\nfrom pathlib import Path\n\n# Find project root by looking for pyproject.toml\ndef find_project_root():\n    current = Path.cwd()\n    for parent in [current] + list(current.parents):\n        if (parent / \"pyproject.toml\").exists():\n            return parent\n    return current.parent.parent  # Fallback\n\nproject_root = find_project_root()\nsys.path.insert(0, str(project_root))\nsys.path.insert(0, str(project_root / \"scripts\"))\n\nimport pandas as pd\nfrom scripts.ICEAA.analysis import load_simulation_results\n\ndf = load_simulation_results()\n```\n:::\n\n\n:::{#5ea133de .cell .markdown}\n## Summary of Findings\n\nOur comprehensive simulation study comparing Penalized-Constrained Regression (PCReg) against traditional methods yields several key insights:\n\n### PCReg Advantages\n\n1. **Guaranteed sign correctness**: PCReg always produces economically sensible coefficients (negative learning slopes) while OLS can produce wrong signs in up to 14% of small-sample scenarios\n\n2. **Superior small-sample performance**: PCReg shows its strongest advantages when sample sizes are small (n ≤ 10 lots), precisely when cost analysts need reliable estimates most\n\n3. **Robustness to multicollinearity**: High predictor correlation degrades OLS performance but has less impact on PCReg\n\n4. **High data quality benefits**: When measurement error is low (CV error = 0.01), PCReg wins 67-75% of scenarios\n\n### When OLS May Be Adequate\n\n1. **Large samples**: With n ≥ 30 lots and high CV error, OLS and PCReg perform similarly\n\n2. **Low correlation**: When predictors are uncorrelated, OLS estimates are more stable\n\n## Practical Recommendations\n\n### Decision Framework\n:::\n\n::: {#tbl-recommendations .cell tbl-cap='Practical recommendations for method selection' execution_count=20}\n``` {.python .cell-code .hidden}\nrecommendations = pd.DataFrame({\n    'Condition': [\n        'CV error = 0.01 (high quality data)',\n        'CV error = 0.10, n ≤ 10',\n        'CV error = 0.10, n = 30',\n        'CV error = 0.20, n = 5',\n        'CV error = 0.20, n = 10',\n        'CV error = 0.20, n = 30',\n        'OLS produces wrong sign'\n    ],\n    'PCReg Win Rate': ['67-75%', '57-64%', '~48%', '~58%', '~47%', '~34%', '~81%'],\n    'Recommendation': [\n        'Use PCReg',\n        'Use PCReg',\n        'Either method',\n        'Use PCReg',\n        'Either method',\n        'Consider OLS',\n        'Use PCReg'\n    ]\n})\nrecommendations\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Condition</th>\n      <th>PCReg Win Rate</th>\n      <th>Recommendation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CV error = 0.01 (high quality data)</td>\n      <td>67-75%</td>\n      <td>Use PCReg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CV error = 0.10, n ≤ 10</td>\n      <td>57-64%</td>\n      <td>Use PCReg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CV error = 0.10, n = 30</td>\n      <td>~48%</td>\n      <td>Either method</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CV error = 0.20, n = 5</td>\n      <td>~58%</td>\n      <td>Use PCReg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CV error = 0.20, n = 10</td>\n      <td>~47%</td>\n      <td>Either method</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>CV error = 0.20, n = 30</td>\n      <td>~34%</td>\n      <td>Consider OLS</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>OLS produces wrong sign</td>\n      <td>~81%</td>\n      <td>Use PCReg</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::{#3f9e7e49 .cell .markdown}\n### Practical Guidance for Cost Estimators\n\n::: {.callout-tip title=\"Implementation Checklist\"}\n1. **Document everything**: Constraints, penalties, active bounds at solution. This is NOT OLS---transparency is essential.\n\n2. **Start with loose constraints** based on domain knowledge (e.g., learning slope ≤ 100%)\n\n3. **Constraints need not be perfect**: Even approximately correct bounds improve estimation [@james2020pac]\n\n4. **Derive constraints from domain benchmarks**: Published learning curve studies, historical program data, and subject matter expert knowledge can inform reasonable bounds. Reference benchmarks specific to your domain (e.g., aerospace learning curves typically range 75-95%).\n\n5. **Use cross-validation** for ($\\alpha$, l1\\_ratio) selection---do not impose arbitrary penalty values\n\n6. **Report GDF-adjusted statistics** for transparency [@hu2010gdf]\n\n7. **Try multiple starting points**: For nonlinear models, testing multiple starting points can avoid local optima\n\n8. **Don't worry (too much) about global optimum**: Even if not confirmed to be globally optimal, it can still be the best reasonable model\n\n9. **Regularization is recommended** (even if minimal): Per Theobald-Farebrother, some L2 regularization is always optimal, even when correlation is not high [@theobald1974]\n:::\n\n### Implementation Guidance\n\n1. **Start with constraints only** ($\\alpha = 0$): Our results show that constraints alone often outperform CV-tuned penalties. The regularization benefit is secondary to the constraint benefit.\n\n2. **Use loose bounds**: Rather than trying to specify tight coefficient bounds, use conservative ranges:\n   - Learning slope ($b$): $-0.5 \\leq b \\leq 0$\n   - Rate effect ($c$): $-0.5 \\leq c \\leq 0$\n   - First unit cost ($T_1$): $0 < T_1 < \\infty$\n\n3. **Consider observable indicators**:\n   - Estimate CV error from residual variance\n   - Estimate predictor correlation from data\n   - Use sample size directly\n\n4. **Validate with out-of-sample testing**: When possible, hold out recent lots for validation\n\n## Limitations and Cautions\n\n::: {.callout-warning title=\"Important Cautions\"}\n1. **Abuse potential**: Constraints could be used to force desired results. Transparency in documentation is essential---always disclose what constraints were applied and why.\n\n2. **Not BLUE**: Always disclose that the method intentionally introduces bias in exchange for reduced variance. This is a feature, not a bug, but stakeholders should understand the tradeoff.\n\n3. **Local optima**: For nonlinear models, test multiple starting points. The solution may not be globally optimal.\n\n4. **Bootstrap CIs**: May be artificially narrow for penalized models because penalties constrain coefficient variability across resamples [@goeman_penalized]. Compare unconstrained bootstrap to constrained bootstrap when possible.\n\n5. **Speed**: Optimization routines take longer to converge than closed-form OLS (trivial concern for small datasets and few runs).\n\n6. **Heteroscedasticity**: This implementation does not include weighted approaches. SSPE partially addresses heteroscedasticity through unit-space operation, but formal weighted least squares integration is future work.\n:::\n\n### Additional Limitations\n\n1. **Simulation vs. reality**: Our data generating process, while realistic, cannot capture all complexities of real manufacturing data\n\n2. **Bound specification**: Results assume practitioners can specify reasonable coefficient bounds\n\n3. **Model form**: We assume the multiplicative power-law model is correct; model misspecification was not studied\n\n4. **Single outcome metric**: We focused on SSPE; other metrics might yield different conclusions\n\n## Future Research\n\n1. **Real data validation**: Apply PCReg to historical cost datasets using publicly available Selected Acquisition Reports (SARs), which are not subject to CUI restrictions\n\n2. **pcLAD implementation**: Robust estimation with outliers using penalized-constrained LAD [@wu2022pclad]\n\n3. **Additional algorithms**: Coordinate descent, projected gradient methods, and the PAC algorithm from @james2020pac\n\n4. **Alternative optimizers**: Systematic comparison of SLSQP, COBYLA, trust-constr, and cvxpy performance\n\n5. **Weighted approaches**: Explicit heteroscedasticity modeling\n\n6. **PenalizedConstrainedMUPE**: Proposed method using MUPE/IRLS loss function with penalties and constraints\n\n7. **Alternative model forms**: Extend to other functional forms (e.g., S-curves, plateau models)\n\n8. **Bayesian approaches**: Compare with Bayesian methods that incorporate prior knowledge through priors rather than constraints\n\n## Conclusion\n\nPenalized-Constrained Regression offers a principled approach to incorporating domain knowledge into learning curve estimation. By enforcing economically sensible constraints, PCReg produces more reliable estimates, particularly in the challenging conditions that cost analysts frequently face: small samples, noisy data, and correlated predictors.\n\nThe `penalized_constrained` Python package provides a production-ready implementation with cross-validation, multiple penalty selection methods, and comprehensive diagnostics. We recommend PCReg as a practical tool for cost estimation practitioners seeking to improve upon traditional OLS methods.\n\n\n# References {.unnumbered}\n\n::: {#refs}\n:::\n\n# Appendices {.unnumbered}\n\n# Appendix A: Research Paper Summaries {#sec-appendix-research .unnumbered}\n\nThis appendix summarizes key research papers supporting the methodology presented in this paper. Papers are ordered by direct applicability to the research objectives: addressing multicollinearity in small datasets, imposing prior knowledge through constraints, and assessing model fit using likelihood-free diagnostics.\n\n## A.1 Penalized and Constrained LAD Estimation {.unnumbered}\n\n**Authors**: Wu, Liang, Yang (2022) | **Source**: Statistical Papers | **Relevance**: Most directly applicable\n\nThis paper proposes L1 penalized Least Absolute Deviation (LAD) estimation with linear constraints (pcLAD). Unlike constrained Lasso which uses squared error loss, pcLAD uses absolute deviation loss, making it robust to heavy-tailed errors and outliers. The method supports both equality constraints ($C\\beta = b$) and inequality constraints ($C\\beta \\leq b$).\n\n**Key Contributions**:\n\n- Proves Oracle property for the constrained estimator in fixed dimensions\n- For high-dimensional settings ($p >> n$), derives error bounds showing estimation error is $O(\\sqrt{\\max(m, k-m)\\log(p)/n})$, where $m$ is constraints and $k$ is non-zero coefficients\n- Demonstrates that adding constraints can sharpen estimation bounds\n\n**Application to Cost Estimation**: Non-negative lasso variant is relevant for cost estimation where coefficients must be positive. Monotonic order estimation supports learning curve constraints. Robustness to heavy-tailed errors addresses outliers common in cost data.\n\n## A.2 Algorithms for Fitting the Constrained Lasso {.unnumbered}\n\n**Authors**: Gaines, Kim, Zhou (2018) | **Source**: J. Comp. & Graph. Statistics | **Relevance**: Highly applicable\n\nProvides three computational approaches for constrained Lasso: Quadratic Programming (QP), ADMM, and a novel solution path algorithm. The constrained Lasso augments standard Lasso with linear equality and inequality constraints.\n\n**Key Contributions**:\n\n- Demonstrates generalized Lasso can be transformed into constrained Lasso\n- Derives degrees of freedom formula: $\\text{df} = |\\text{Active predictors}| - (\\text{\\# equality constraints}) - (\\text{\\# binding inequality constraints})$\n- Enables proper model selection criteria (AIC, BIC) for constrained models\n\n**Application to Cost Estimation**: Non-negativity constraints (positive Lasso) support cost estimation requirements. Monotonic ordering constraints (ordered Lasso) support learning curve slope constraints. Solution path algorithm enables efficient cross-validation.\n\n## A.3 PAC: Penalized and Constrained Optimization {.unnumbered}\n\n**Authors**: James, Paulson, Rusmevichientong (2020) | **Source**: JASA | **Relevance**: Highly applicable\n\nPAC extends constrained optimization beyond squared error loss to general loss functions $g(\\beta)$, including generalized linear models. The formulation minimizes $g(\\beta) + \\lambda\\|\\beta\\|_1$ subject to $C\\beta \\leq b$.\n\n**Key Contributions**:\n\n- Shows generalized Lasso is a special case of constrained problem\n- Develops efficient path algorithm reducing constrained optimization to sequence of standard Lasso problems\n- Demonstrates that even when constraints are approximately (not exactly) satisfied, PAC outperforms unconstrained methods\n\n**Application to Cost Estimation**: Monotone curve fitting directly supports learning curve estimation. Robustness to constraint violations is realistic for cost estimation where prior knowledge is informative but not perfect.\n\n## A.4 Multicollinearity in CER Development {.unnumbered}\n\n**Authors**: Flynn & James (2016) | **Source**: ICEAA Workshop | **Relevance**: Directly applicable to problem statement\n\nProvides comprehensive treatment of multicollinearity diagnosis and remediation specifically for defense cost analysis. Demonstrates classic symptoms: wrong coefficient signs, bouncing β's, mismatch between t and F statistics, inflated variance.\n\n**Key Contributions**:\n\n- Shows coefficient variance increases by factor $1/(1-R^2)$ where $R^2$ is correlation between predictors\n- Demonstrates Frisch's confluence analysis for variable selection\n- Provides diagnostic tests including condition numbers and eigenvalue analysis\n- Acknowledges learning curve correlation between lot midpoint and lot size---the exact motivating example\n\n**Application to Cost Estimation**: Directly addresses the problem statement. Demonstrates with spacecraft payload data how multicollinearity causes coefficient instability in small samples.\n\n## A.5 Generalized Degrees of Freedom for Constrained CERs {.unnumbered}\n\n**Author**: Hu (Tecolote Research) | **Source**: PRT-191 | **Relevance**: Important for model assessment\n\nAddresses how degrees of freedom should be adjusted when constraints are included in CER development. Compares MUPE and ZMPE methods, showing that ZMPE's fit statistics can be misleading without proper DF adjustment.\n\n**Key Contributions**:\n\n- Defines $\\text{GDF} = n - p - m$, where $m$ is number of constraints\n- Shows unadjusted DF leads to underestimated standard errors\n- Demonstrates ZMPE CERs are less stable than MUPE, especially for small samples\n\n**Application to Cost Estimation**: Critical for model assessment. When combining penalties with constraints, degrees of freedom must account for both.\n\n## A.6 Why ZMPE When You Can MUPE {.unnumbered}\n\n**Authors**: Hu & Smith (2007) | **Source**: SCEA-ISPA | **Relevance**: Background on constrained CER methods\n\nCompares MUPE (IRLS) and ZMPE for multiplicative error models. MUPE is unconstrained and produces BLUE estimates. ZMPE uses zero-percentage-bias constraint but has unclear statistical properties.\n\n**Key Points**:\n\n- MUPE produces consistent, unbiased estimates with known statistical properties\n- ZMPE is sensitive to starting points, less stable for small samples\n- Statistical interpretation of ZMPE is unclear (mean, median, or mode?)\n\n**Application to Cost Estimation**: Demonstrates cost estimation community already uses constraints, often without proper statistical foundation. PCReg provides that foundation.\n\n## A.7 Linear Regression Regularization Methods {.unnumbered}\n\n**Author**: Roye (2022) | **Source**: ICEAA Workshop | **Relevance**: Good foundation for cost estimation audience\n\nIntroduces Ridge, Lasso, and Elastic Net regularization to cost estimation community. Explains bias-variance tradeoff with intuitive visual explanations.\n\n**Application to Cost Estimation**: Accessible introduction for ICEAA audience unfamiliar with regularization. Emphasizes cross-validation for tuning parameter selection.\n\n## A.8 Assessing Regression Methods {.unnumbered}\n\n**Authors**: Schiavoni et al. (2021) | **Source**: ICEAA Workshop | **Relevance**: Comparison of cost estimation methods\n\nCompares convergence rates and performance of different regression methods (Log Error, PING, GRMLN, MUPE, ZMPE) across various sample sizes and variance conditions.\n\n**Key Finding**: Recommends COBYLA optimizer for ZMPE-type problems.\n\n**Application to Cost Estimation**: Provides performance benchmarks for existing methods. PCReg can be positioned against these approaches in challenging \"small sample, high variance\" scenarios.\n\n## Synthesis: Recommendations for Implementation {.unnumbered}\n\n**Core Methodological Foundation**: Build on the pcLAD framework from Wu et al. (2022) which combines L1 penalization with linear constraints in a unified framework with proven statistical properties.\n\n**Algorithmic Implementation**: Use solution path algorithm from Gaines et al. (2018) for efficient computation across tuning parameter values. PAC algorithm extends to non-squared-error loss functions.\n\n**Cost Estimation Context**: Multicollinearity papers provide problem motivation familiar to ICEAA audience. MUPE/ZMPE papers show constrained estimation is already practiced but without rigorous foundation.\n\n**Model Assessment**: Use GDF concept from Hu's paper to properly account for constraints. Apply cross-validation for tuning parameter selection.\n\n\n# Appendix B: Algorithm Details {#sec-appendix-algorithm .unnumbered}\n\nThis appendix provides detailed algorithm information for Penalized-Constrained Regression implementation.\n\n## B.1 High-Level Algorithm {.unnumbered}\n\n```\nInput:  Data (X, y), functional form f(X, β), penalty parameters (α, l1_ratio),\n        bounds/constraints, error function, optimizer choice\n\n1. Scale (optional):\n   - Standardize X (mean=0, std=1) if scale=True\n   - Store scaling parameters for unscaling\n\n2. Initialize:\n   - Compute OLS coefficients when possible (X'X invertible)\n   - Trim starting values to satisfy bounds\n   - Alternative: zeros or user-specified starting point\n\n3. Optimize:\n   - Solve constrained penalized minimization via selected optimizer\n   - Monitor convergence and constraint satisfaction\n\n4. Unscale:\n   - Transform coefficients back to original units\n   - β_original = β_scaled / σ\n\nOutput: Coefficient estimates β̂, fit statistics (GDF-adjusted),\n        active_constraints_ flag\n```\n\n## B.2 Initialization Strategy {.unnumbered}\n\nThe default initialization uses OLS coefficients when the problem is well-conditioned ($X'X$ invertible). These starting values are then trimmed to satisfy bounds---any coefficient outside the specified bounds is clamped to the nearest boundary.\n\n**Benefits**:\n\n- Provides a warm start that respects domain constraints from the first iteration\n- Improves convergence speed\n- Reduces risk of local minima\n\n**Alternative initializations**:\n\n- Zeros: Simple but may require more iterations\n- User-specified: When domain expertise suggests better starting point\n- Random: For testing multiple starting points in non-convex problems\n\n## B.3 Why Scaling Matters {.unnumbered}\n\nScaling ensures that penalty terms (L1 and L2) are applied fairly across features. Without scaling:\n\n- Features with larger magnitudes may dominate the optimization\n- Leads to biased shrinkage and poor variable selection\n- Degrades conditioning of the optimization problem\n\n**For linear models**: Standardization (zero mean, unit variance) is typically sufficient.\n\n**For nonlinear models** like $Y = AX^b$:\n\n- Log-transformations (common in power-law modeling)\n- Min-max scaling or domain-specific normalization\n- Careful unscaling of both $X$ and $\\beta$ to preserve interpretability\n\n::: {.callout-warning}\nIn nonlinear models such as $Y = AX^b$, scaling the predictor $X$ alters the curvature and interpretation of the exponent $b$. When applying penalized regression with L1 and L2 penalties to such models, scaling affects both the error function and the regularization terms. Scaling must be applied with caution, and unscaling procedures should be explicitly defined.\n:::\n\n## B.4 Optimization Methods {.unnumbered}\n\n### SLSQP (Sequential Least-Squares Quadratic Programming)\n\n**Current default**. Handles bounds and linear constraints efficiently.\n\n- Uses sequential quadratic programming approach\n- Requires gradient computation (finite differences if not provided)\n- Efficient for small to medium problems\n\n### COBYLA (Constrained Optimization BY Linear Approximation)\n\n**Derivative-free**; recommended for ZMPE-type problems [@schiavoni2021assessing].\n\n- Does not require gradient computation\n- Handles nonlinear constraints\n- More robust for non-smooth objectives\n- Slower convergence than gradient-based methods\n\n### trust-constr\n\nInterior point method; suitable for larger-scale problems with many constraints.\n\n- Modern implementation with trust-region approach\n- Handles equality and inequality constraints\n- Good numerical stability\n- Requires more computational resources\n\n## B.5 Note on L1 Penalty Implementation {.unnumbered}\n\n::: {.callout-note title=\"Technical Note\"}\nThe L1 penalty $\\|\\beta\\|_1$ is convex but not differentiable at zero. This non-smoothness introduces challenges for gradient-based optimization algorithms. Standard approaches include:\n\n1. **Coordinate descent**: Most popular approach; for constrained problems, use projected or constrained coordinate descent\n2. **Subgradient methods**: Handle non-differentiability directly\n3. **Proximal algorithms**: Efficient for structured sparsity problems\n4. **CVXPY**: Convex optimization framework that handles non-smooth objectives\n\nThis implementation uses general-purpose scipy optimizers. Despite theoretical limitations, they yield reasonable and stable solutions when problem size is moderate and constraints are well-posed.\n\nFuture work will evaluate coordinate descent and PAC algorithm implementations.\n:::\n\n## B.6 Convergence Criteria {.unnumbered}\n\nThe optimizer terminates when any of these conditions are met:\n\n1. **Function tolerance**: Change in objective function $< \\text{ftol}$ (default: $10^{-9}$)\n2. **Parameter tolerance**: Change in parameters $< \\text{xtol}$ (default: $10^{-9}$)\n3. **Maximum iterations**: Exceeds `maxiter` (default: 1000)\n4. **Constraint satisfaction**: All constraints satisfied within tolerance\n\n## B.7 Post-Optimization Checks {.unnumbered}\n\nAfter optimization completes:\n\n1. **Constraint satisfaction**: Verify all bounds are satisfied within numerical tolerance\n2. **Active constraints**: Identify which inequality constraints are binding (within tolerance of bound)\n3. **Gradient check**: Verify KKT conditions are approximately satisfied\n4. **Hessian conditioning**: Check for numerical issues in uncertainty estimation\n\nThe `active_constraints_` attribute indicates which constraints are binding at the solution, informing degrees of freedom calculation.\n\n\n# Appendix C: Generalized Degrees of Freedom {#sec-appendix-gdf .unnumbered}\n\nThis appendix provides detailed information on degrees of freedom adjustment for constrained regression models.\n\n## C.1 The Critical Issue {.unnumbered}\n\n::: {.callout-important}\n\"ZMPE users do not adjust the degrees of freedom (DF) to account for constraints included in the regression process. As a result, fit statistics for the ZMPE equations, e.g., the standard percent error (SPE) and generalized R² (GRSQ), can be incorrect and misleading.\" --- @hu2010gdf\n:::\n\nWhen constraints are imposed on regression coefficients, the effective degrees of freedom must be adjusted. Without this adjustment:\n\n- Standard errors are underestimated\n- Confidence intervals are too narrow\n- Statistical tests are invalid\n- S-curves in cost uncertainty analysis are artificially tightened\n\n## C.2 Hu's GDF Formula {.unnumbered}\n\n@hu2010gdf defines Generalized Degrees of Freedom as:\n\n$$\\text{GDF} = n - p - (\\text{\\# Constraints}) + (\\text{\\# Redundancies})$$\n\nwhere:\n\n- $n$ = sample size\n- $p$ = number of estimated parameters (coefficients)\n- Constraints = number of restrictions imposed\n- Redundancies = constraints that can be derived from others\n\n**Interpretation**: One restriction is equivalent to a loss of one DF.\n\n### Redundancies Definition\n\nIf two constraints are specified but one can be derived from the other, count only a loss of one DF rather than two. Additionally:\n\n- If a parameter is known (e.g., startup cost is given), this amounts to a **gain** of one DF\n- For ZMPE CERs (except simple factor CERs), DF should be subtracted by one because the solution uses the constraint alone\n\n### Example\n\nConsider estimating $Y = T_1 \\cdot X_1^b \\cdot X_2^c$ with:\n\n- $n = 10$ observations\n- $p = 3$ parameters ($T_1$, $b$, $c$)\n- Constraints: $b \\leq 0$, $c \\leq 0$ (2 inequality constraints)\n\nIf both constraints are binding:\n$$\\text{GDF} = 10 - 3 - 2 + 0 = 5$$\n\n## C.3 Gaines et al. Formula {.unnumbered}\n\n@gaines2018constrained derive degrees of freedom for constrained Lasso as:\n\n$$\\text{df} = |\\text{Active predictors}| - (\\text{\\# equality constraints}) - (\\text{\\# binding inequality constraints})$$\n\n**Key difference from Hu's formula**: Only counts binding inequality constraints.\n\n### Implications\n\n- **Loose bounds** ($\\beta \\leq 0$) may not bind---no DF loss if inactive at the solution\n- **Tight bounds** ($0.85 < \\beta < 0.95$) are more likely to bind\n\n## C.4 Comparing the Two Formulations {.unnumbered}\n\n| Aspect | Hu's Formula | Gaines' Formula |\n|--------|--------------|-----------------|\n| Counts all constraints | Yes | No |\n| Counts binding only | No | Yes |\n| Best for | ZMPE/MUPE CERs | Penalized regression |\n| Conservative | Yes | No |\n\n### Open Question\n\nWhen applying Penalized-Constrained regression changes the signs of coefficients even if the constraints don't explicitly bind at the solution, which formulation is appropriate?\n\n- **Hu's formulation** would decrease DF for all specified constraints\n- **Gaines' formulation** would not count non-binding constraints\n\nFuture empirical work will evaluate which approach produces more accurate uncertainty quantification through simulation.\n\n## C.5 GDF-Adjusted Fit Statistics {.unnumbered}\n\n### Standard Error of Estimate (SEE)\n\n$$\\text{SEE} = \\sqrt{\\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\text{GDF}}}$$\n\n### Standard Percent Error (SPE)\n\n$$\\text{SPE} = \\sqrt{\\frac{\\sum_{i=1}^{n}\\left(\\frac{y_i - \\hat{y}_i}{\\hat{y}_i}\\right)^2}{\\text{GDF}}}$$\n\n### Adjusted R²\n\n$$R^2_{\\text{adj}} = 1 - \\frac{(1 - R^2)(n - 1)}{\\text{GDF}}$$\n\n## C.6 Impact on Uncertainty Analysis {.unnumbered}\n\n::: {.callout-warning}\n\"Using ZMPE CERs in cost uncertainty analysis may unduly tighten the S-curve because their SPEs underestimate the CER error distribution.\" --- @hu2010gdf\n:::\n\n**Practical implications**:\n\n1. Cost estimates may appear more precise than they actually are\n2. Risk analysis may understate uncertainty\n3. Decision-makers may have false confidence in point estimates\n\n**Recommendation**: Always report which DF adjustment method was used and the resulting fit statistics alongside unadjusted statistics for comparison.\n\n## C.7 Implementation in `penalized_constrained` {.unnumbered}\n\nThe package provides both adjusted and unadjusted statistics:\n\n- `gdf_`: Generalized degrees of freedom (Hu's formula by default)\n- `see_adjusted_`: GDF-adjusted standard error of estimate\n- `spe_adjusted_`: GDF-adjusted standard percent error\n- `active_constraints_`: Boolean array indicating which constraints are binding\n\nUsers can override the DF calculation by specifying which constraints should count toward the adjustment.\n\n\n# Appendix D: Key Quotes for Reference {#sec-appendix-quotes .unnumbered}\n\nThis appendix compiles key quotes from the literature that support the methodology and findings presented in this paper. These quotes may be useful for presentations, discussions, and future research.\n\n## D.1 On Multicollinearity Severity {.unnumbered}\n\n> \"The statistical literature regards multicollinearity as one of the most vexing and intractable problems in all of regression analysis.\"\n> --- @flynn2016multicollinearity\n\n## D.2 On Coefficient Instability {.unnumbered}\n\n> \"Generate unstable models where small changes in the data produce big changes in parameter estimates [bouncing β's].\"\n> --- @flynn2016multicollinearity\n\n## D.3 On GDF Importance {.unnumbered}\n\n> \"ZMPE users do not adjust the degrees of freedom (DF) to account for constraints included in the regression process. As a result, fit statistics for the ZMPE equations, e.g., the standard percent error (SPE) and generalized R² (GRSQ), can be incorrect and misleading.\"\n> --- @hu2010gdf\n\n> \"Using ZMPE CERs in cost uncertainty analysis may unduly tighten the S-curve because their SPEs underestimate the CER error distribution.\"\n> --- @hu2010gdf\n\n## D.4 On Robustness to Imperfect Constraints {.unnumbered}\n\n> \"The results suggest that PAC and relaxed PAC are surprisingly robust to random violations in the constraints. While both methods deteriorated slightly as [constraint error] increased, they were still both superior to their unconstrained counterparts for all values of [error] and all settings.\"\n> --- @james2020pac\n\n## D.5 On Constrained Lasso Flexibility {.unnumbered}\n\n> \"The constrained lasso is a very flexible framework for imposing additional knowledge and structure onto the lasso coefficient estimates.\"\n> --- @gaines2018constrained\n\n## D.6 On pcLAD with Heavy-Tailed Errors {.unnumbered}\n\n> \"pcLAD enjoys the Oracle property even with Cauchy-distributed errors... particularly effective for monotone curve fitting and non-negative constraints.\"\n> --- @wu2022pclad\n\n## D.7 On Ridge Regression Optimality {.unnumbered}\n\nThe Theobald-Farebrother theorem establishes:\n\n> For any OLS problem, there exists a ridge parameter $\\lambda^* > 0$ such that the ridge estimator has strictly lower Mean Squared Error (MSE) than OLS.\n> --- @theobald1974; @farebrother1976\n\n## D.8 On Bayesian Regularization {.unnumbered}\n\n> \"Bayesian interpolation using a spherical Gaussian prior $p(w|\\lambda) = N(w|0, \\lambda^{-1}I)$ iteratively maximizes marginal log-likelihood to optimize regularization parameters.\"\n> --- @mackay1992bayesian\n\n## D.9 Key Definitions {.unnumbered}\n\n### Oracle Property\n\nAn estimator has the **Oracle property** if it:\n\n1. Correctly identifies which coefficients are truly zero (variable selection consistency)\n2. Estimates non-zero coefficients as efficiently as if an \"oracle\" revealed the true model in advance\n\nThis is the gold standard for high-dimensional estimators---the Lasso achieves it under certain conditions.\n\n### Cauchy-Distributed Errors\n\nThe **Cauchy distribution** has extremely heavy tails---so heavy that its mean and variance are mathematically undefined (infinite). Outliers occur far more frequently than with normal distributions.\n\n- Squared-error methods (OLS, Ridge, Lasso) perform poorly with Cauchy errors because outliers dominate the objective\n- LAD methods minimize absolute errors, so outliers have linear rather than quadratic influence---making them robust to such extremes\n\n### BLUE (Best Linear Unbiased Estimator)\n\nBy the **Gauss-Markov theorem**, OLS is the Best Linear Unbiased Estimator under classical assumptions:\n\n- Linear in the parameters\n- Unbiased: $E[\\hat{\\beta}] = \\beta$\n- Minimum variance among all linear unbiased estimators\n\nIntroducing penalties and/or constraints means the resulting estimator is **no longer BLUE**. This is an intentional tradeoff accepting bias for reduced variance.\n\n## D.10 Summary Table {.unnumbered}\n\n| Topic | Key Finding | Source |\n|-------|-------------|--------|\n| Multicollinearity | \"Most vexing problem in regression\" | Flynn & James (2016) |\n| Constraint robustness | PAC outperforms unconstrained even with wrong constraints | James et al. (2020) |\n| GDF adjustment | Unadjusted fit stats are misleading | Hu (2010) |\n| Ridge optimality | Always exists λ* > 0 with lower MSE than OLS | Theobald-Farebrother |\n| pcLAD robustness | Oracle property even with Cauchy errors | Wu et al. (2022) |\n:::\n\n",
    "supporting": [
      "paper_files\\figure-docx"
    ],
    "filters": []
  }
}