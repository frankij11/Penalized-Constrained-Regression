{
  "hash": "4bbd04ab55420f04d4ece508718a80fa",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Simulation Study Design\n---\n\n\n:::{#87146229 .cell .markdown}\n\n:::\n\n::: {#setup-simulation .cell .hidden execution_count=1}\n``` {}\n#| label: setup-simulation\n#| include: false\nimport sys\nfrom pathlib import Path\n\n# Find project root by looking for pyproject.toml\ndef find_project_root():\n    current = Path.cwd()\n    for parent in [current] + list(current.parents):\n        if (parent / \"pyproject.toml\").exists():\n            return parent\n    return current.parent.parent  # Fallback\n\nproject_root = find_project_root()\nsys.path.insert(0, str(project_root))\nsys.path.insert(0, str(project_root / \"scripts\"))\n```\n:::\n\n\n:::{#f4145ec7 .cell .markdown}\n## Data Generating Process\n\nWe simulate learning curve data using the multiplicative power-law model:\n\n$$Y = T_1 \\cdot X_1^b \\cdot X_2^c \\cdot \\exp(\\epsilon)$$\n\nwhere:\n\n-   $T_1 = 100$ (first unit cost)---**Note**: $T_1$ is estimated in all examples, not fixed\n-   $X_1$ = lot midpoint (cumulative units)\n-   $X_2$ = lot quantity (rate variable)\n-   $\\epsilon \\sim N(0, \\sigma^2)$ with $\\sigma^2 = \\log(1 + \\text{cv\\_error}^2)$\n\nThe correlation between $\\log(X_1)$ and $\\log(X_2)$ is controlled to study multicollinearity effects. Correlated predictor datasets are generated using a custom function that creates realistic production ramp-up schedules where lot midpoint and quantity naturally correlate at the specified $\\rho$ levels.\n\n## Experimental Design\n\nWe employ a full factorial design with 5 factors:\n\n| Factor                | Levels | Values                                   |\n|-----------------------|--------|------------------------------------------|\n| Sample size (n_lots)  | 3      | 5, 10, 30                                |\n| Predictor correlation | 3      | 0.0, 0.5, 0.9                            |\n| CV error (noise)      | 3      | 0.01, 0.10, 0.20                         |\n| Learning rate (b)     | 3      | log(0.85), log(0.90), log(0.95) / log(2) |\n| Rate effect (c)       | 3      | log(0.80), log(0.85), log(0.90) / log(2) |\n\nThis yields $3^5 = 243$ unique scenarios. Each scenario is replicated 25 times with different random seeds, producing **6,075 total scenario-replications**.\n\n## Models Compared\n\nWe evaluate 18 regression approaches:\n\n### Traditional Methods (Log-Space)\n\n-   **OLS**: Ordinary Least Squares on log-transformed data (base case, unconstrained, unpenalized)\n-   **OLS_LearnOnly**: OLS with only the learning variable (confluence analysis remedy---drop rate variable)\n-   **Ridge**: Ridge regression with CV-selected $\\lambda$, no constraints\n-   **Lasso**: Lasso regression with CV-selected $\\lambda$, no constraints\n-   **BayesianRidge**: sklearn implementation using spherical Gaussian prior $p(w|\\lambda) = N(w|0, \\lambda^{-1}I)$ centered at zero. Iteratively maximizes marginal log-likelihood to optimize regularization parameters [@mackay1992bayesian; @tipping2001sparse]. *Note*: Uses uninformative priors without domain-specific constraints, serving as comparison to automatic regularization without domain knowledge.\n-   **PLS / PCA_Linear**: Both produce identical results for two predictors\n\n### PCReg Variants (Unit-Space)\n\n-   **PCReg_ConstrainOnly**: Constraints only, no penalty ($\\alpha = 0$)\n-   **PCReg_CV**: Constraints + CV-tuned elastic net with loose bounds ($\\beta \\in [-1, 0]$)\n-   **PCReg_AICc**: Constraints + AICc-selected penalty\n-   **PCReg_GCV**: Constraints + GCV-selected penalty\n-   **PCReg_CV_Tight**: With tight bounds near true values. This serves as an approximate upper bound on performance---analogous to an \"oracle\" benchmark that knows the true parameter region.\n-   **PCReg_CV_Wrong**: With deliberately incorrect constraints. Following @james2020pac methodology, \"wrong\" constraints are generated by adding random noise to the true bounds: $\\text{constraint}_{\\text{wrong}} = \\text{constraint}_{\\text{true}} + a \\cdot U(-1,1)$, where $a$ controls the magnitude of constraint error. This tests robustness to constraint misspecification.\n\n### PCReg MSE Loss Variants\n\n-   **PCReg_MSE**: MSE loss with constraints\n-   **PCReg_MSE_CV**: MSE loss with CV-tuned penalty\n\n### PCReg Log-Space Variants\n\n-   **PCReg_LogMSE**: Log-transformed MSE loss\n-   **PCReg_LogMSE_CV**: Log-transformed with CV penalty\n\n## Out-of-Sample Test Data\n\nAll models are evaluated on 5 held-out test lots that simulate steady-state production conditions:\n\n-   **Starting point**: Test lots begin immediately after training data ends (unit number = last training unit + 1)\n-   **Fixed quantity**: All test lots use the same quantity, set to 2Ã— the maximum training lot quantity\n-   **No noise**: True costs are used (cv_error = 0) to measure pure prediction accuracy without confounding error\n-   **Rationale**: This setup mimics real-world forecasting where models trained on ramp-up data must predict stable production\n\nThis design tests whether models can extrapolate beyond training data to predict costs at higher cumulative units with larger, consistent lot sizes---a common requirement in production cost estimation.\n\n## Evaluation Metrics\n\nModels are evaluated on the test lots using:\n\n1.  **Test SSPE**: Sum of squared percentage errors (primary metric)\n2.  **Test MAPE**: Mean absolute percentage error\n3.  **Test MSE**: Mean squared error\n4.  **Coefficient bias**: $|\\hat{\\beta} - \\beta_{\\text{true}}|$ for $b$ (learning) and $c$ (rate)\n5.  **Coefficient variance**: Stability across replications\n6.  **Sign correctness (Domain consistency rate)**: Whether $\\hat{b} \\leq 0$ and $\\hat{c} \\leq 0$---percentage of replications with correct coefficient signs\n\n## Computational Details\n\n-   **Parallelization**: joblib with all available CPU cores\n-   **Resume capability**: Results saved in batches to parquet files\n-   **Model hashing**: Automatic re-run if model definitions change\n-   **Total model fits**: $243 \\times 25 \\times 18 = 109,350$\n:::\n\n",
    "supporting": [
      "03-simulation-design_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}