{
  "hash": "387feec03bf3284923947af8c0611bbd",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Discussion and Recommendations\n---\n\n\n:::{#7e39644c .cell .markdown}\n\n:::\n\n::: {#setup-discussion .cell .hidden execution_count=1}\n``` {}\n#| label: setup-discussion\n#| include: false\nimport sys\nfrom pathlib import Path\n\n# Find project root by looking for pyproject.toml\ndef find_project_root():\n    current = Path.cwd()\n    for parent in [current] + list(current.parents):\n        if (parent / \"pyproject.toml\").exists():\n            return parent\n    return current.parent.parent  # Fallback\n\nproject_root = find_project_root()\nsys.path.insert(0, str(project_root))\nsys.path.insert(0, str(project_root / \"scripts\"))\n\nimport pandas as pd\nfrom scripts.ICEAA.analysis import load_simulation_results\n\ndf = load_simulation_results()\n```\n:::\n\n\n:::{#72c0241a .cell .markdown}\n## Summary of Findings\n\nOur comprehensive simulation study comparing Penalized-Constrained Regression (PCReg) against traditional methods yields several key insights:\n\n### PCReg Advantages\n\n1. **Guaranteed sign correctness**: PCReg always produces economically sensible coefficients (negative learning slopes) while OLS can produce wrong signs in up to 14% of small-sample scenarios\n\n2. **Superior small-sample performance**: PCReg shows its strongest advantages when sample sizes are small (n ≤ 10 lots), precisely when cost analysts need reliable estimates most\n\n3. **Robustness to multicollinearity**: High predictor correlation degrades OLS performance but has less impact on PCReg\n\n4. **High data quality benefits**: When measurement error is low (CV error = 0.01), PCReg wins 67-75% of scenarios\n\n### When OLS May Be Adequate\n\n1. **Large samples**: With n ≥ 30 lots and high CV error, OLS and PCReg perform similarly\n\n2. **Low correlation**: When predictors are uncorrelated, OLS estimates are more stable\n\n## Practical Recommendations\n\n### Decision Framework\n:::\n\n::: {#tbl-recommendations .cell tbl-cap='Practical recommendations for method selection' execution_count=2}\n``` {}\n#| label: tbl-recommendations\n#| tbl-cap: Practical recommendations for method selection\nrecommendations = pd.DataFrame({\n    'Condition': [\n        'CV error = 0.01 (high quality data)',\n        'CV error = 0.10, n ≤ 10',\n        'CV error = 0.10, n = 30',\n        'CV error = 0.20, n = 5',\n        'CV error = 0.20, n = 10',\n        'CV error = 0.20, n = 30',\n        'OLS produces wrong sign'\n    ],\n    'PCReg Win Rate': ['67-75%', '57-64%', '~48%', '~58%', '~47%', '~34%', '~81%'],\n    'Recommendation': [\n        'Use PCReg',\n        'Use PCReg',\n        'Either method',\n        'Use PCReg',\n        'Either method',\n        'Consider OLS',\n        'Use PCReg'\n    ]\n})\nrecommendations\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Condition</th>\n      <th>PCReg Win Rate</th>\n      <th>Recommendation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CV error = 0.01 (high quality data)</td>\n      <td>67-75%</td>\n      <td>Use PCReg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CV error = 0.10, n ≤ 10</td>\n      <td>57-64%</td>\n      <td>Use PCReg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CV error = 0.10, n = 30</td>\n      <td>~48%</td>\n      <td>Either method</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CV error = 0.20, n = 5</td>\n      <td>~58%</td>\n      <td>Use PCReg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CV error = 0.20, n = 10</td>\n      <td>~47%</td>\n      <td>Either method</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>CV error = 0.20, n = 30</td>\n      <td>~34%</td>\n      <td>Consider OLS</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>OLS produces wrong sign</td>\n      <td>~81%</td>\n      <td>Use PCReg</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::{#b0f5dbae .cell .markdown}\n### Practical Guidance for Cost Estimators\n\n::: {.callout-tip title=\"Implementation Checklist\"}\n1. **Document everything**: Constraints, penalties, active bounds at solution. This is NOT OLS---transparency is essential.\n\n2. **Start with loose constraints** based on domain knowledge (e.g., learning slope ≤ 100%)\n\n3. **Constraints need not be perfect**: Even approximately correct bounds improve estimation [@james2020pac]\n\n4. **Derive constraints from domain benchmarks**: Published learning curve studies, historical program data, and subject matter expert knowledge can inform reasonable bounds. Reference benchmarks specific to your domain (e.g., aerospace learning curves typically range 75-95%).\n\n5. **Use cross-validation** for ($\\alpha$, l1\\_ratio) selection---do not impose arbitrary penalty values\n\n6. **Report GDF-adjusted statistics** for transparency [@hu2010gdf]\n\n7. **Try multiple starting points**: For nonlinear models, testing multiple starting points can avoid local optima\n\n8. **Don't worry (too much) about global optimum**: Even if not confirmed to be globally optimal, it can still be the best reasonable model\n\n9. **Regularization is recommended** (even if minimal): Per Theobald-Farebrother, some L2 regularization is always optimal, even when correlation is not high [@theobald1974]\n:::\n\n### Implementation Guidance\n\n1. **Start with constraints only** ($\\alpha = 0$): Our results show that constraints alone often outperform CV-tuned penalties. The regularization benefit is secondary to the constraint benefit.\n\n2. **Use loose bounds**: Rather than trying to specify tight coefficient bounds, use conservative ranges:\n   - Learning slope ($b$): $-0.5 \\leq b \\leq 0$\n   - Rate effect ($c$): $-0.5 \\leq c \\leq 0$\n   - First unit cost ($T_1$): $0 < T_1 < \\infty$\n\n3. **Consider observable indicators**:\n   - Estimate CV error from residual variance\n   - Estimate predictor correlation from data\n   - Use sample size directly\n\n4. **Validate with out-of-sample testing**: When possible, hold out recent lots for validation\n\n## Limitations and Cautions\n\n::: {.callout-warning title=\"Important Cautions\"}\n1. **Abuse potential**: Constraints could be used to force desired results. Transparency in documentation is essential---always disclose what constraints were applied and why.\n\n2. **Not BLUE**: Always disclose that the method intentionally introduces bias in exchange for reduced variance. This is a feature, not a bug, but stakeholders should understand the tradeoff.\n\n3. **Local optima**: For nonlinear models, test multiple starting points. The solution may not be globally optimal.\n\n4. **Bootstrap CIs**: May be artificially narrow for penalized models because penalties constrain coefficient variability across resamples [@goeman_penalized]. Compare unconstrained bootstrap to constrained bootstrap when possible.\n\n5. **Speed**: Optimization routines take longer to converge than closed-form OLS (trivial concern for small datasets and few runs).\n\n6. **Heteroscedasticity**: This implementation does not include weighted approaches. SSPE partially addresses heteroscedasticity through unit-space operation, but formal weighted least squares integration is future work.\n:::\n\n### Additional Limitations\n\n1. **Simulation vs. reality**: Our data generating process, while realistic, cannot capture all complexities of real manufacturing data\n\n2. **Bound specification**: Results assume practitioners can specify reasonable coefficient bounds\n\n3. **Model form**: We assume the multiplicative power-law model is correct; model misspecification was not studied\n\n4. **Single outcome metric**: We focused on SSPE; other metrics might yield different conclusions\n\n## Future Research\n\n1. **Real data validation**: Apply PCReg to historical cost datasets using publicly available Selected Acquisition Reports (SARs), which are not subject to CUI restrictions\n\n2. **pcLAD implementation**: Robust estimation with outliers using penalized-constrained LAD [@wu2022pclad]\n\n3. **Additional algorithms**: Coordinate descent, projected gradient methods, and the PAC algorithm from @james2020pac\n\n4. **Alternative optimizers**: Systematic comparison of SLSQP, COBYLA, trust-constr, and cvxpy performance\n\n5. **Weighted approaches**: Explicit heteroscedasticity modeling\n\n6. **PenalizedConstrainedMUPE**: Proposed method using MUPE/IRLS loss function with penalties and constraints\n\n7. **Alternative model forms**: Extend to other functional forms (e.g., S-curves, plateau models)\n\n8. **Bayesian approaches**: Compare with Bayesian methods that incorporate prior knowledge through priors rather than constraints\n\n## Conclusion\n\nPenalized-Constrained Regression offers a principled approach to incorporating domain knowledge into learning curve estimation. By enforcing economically sensible constraints, PCReg produces more reliable estimates, particularly in the challenging conditions that cost analysts frequently face: small samples, noisy data, and correlated predictors.\n\nThe `penalized_constrained` Python package provides a production-ready implementation with cross-validation, multiple penalty selection methods, and comprehensive diagnostics. We recommend PCReg as a practical tool for cost estimation practitioners seeking to improve upon traditional OLS methods.\n:::\n\n",
    "supporting": [
      "06-discussion_files\\figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}