{
  "hash": "409d2e3ce74b520caa29be65a33ce4e5",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Results\n---\n\n\n:::{#7f12c9fc .cell .markdown}\n\n:::\n\n::: {#setup-results .cell .hidden execution_count=1}\n``` {}\n#| label: setup-results\n#| include: false\nimport sys\nfrom pathlib import Path\n\n# Find project root by looking for pyproject.toml\ndef find_project_root():\n    current = Path.cwd()\n    for parent in [current] + list(current.parents):\n        if (parent / \"pyproject.toml\").exists():\n            return parent\n    return current.parent.parent  # Fallback\n\nproject_root = find_project_root()\nsys.path.insert(0, str(project_root))\nsys.path.insert(0, str(project_root / \"scripts\"))\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom scripts.ICEAA.analysis import (\n    load_simulation_results,\n    create_sign_correctness_plot,\n    create_model_ranking_chart,\n)\nfrom scripts.ICEAA.analysis.visualization import create_wrong_sign_heatmap\nfrom scripts.ICEAA.analysis.load_results import get_model_comparison\n\n# Load data\ndf = load_simulation_results()\nprint(f\"Loaded {len(df):,} observations across {df['model_name'].nunique()} models\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoaded 85,050 observations across 14 models\n```\n:::\n:::\n\n\n:::{#422cd134 .cell .markdown}\nThis section presents the results of our Monte Carlo simulation study comparing Penalized-Constrained Regression (PCReg) against standard approaches. We focus on three key methods:\n\n1. **OLS**: Standard unconstrained regression in log-space (baseline)\n2. **OLS_LearnOnly**: OLS with only the learning variable (confluence remedy - drop rate variable)\n3. **PCReg_CV**: Constrained regression in unit-space with CV-tuned penalty\n\n## Model Ranking\n\nSince PCReg variants are highly correlated with each other, we compare OLS, OLS_LearnOnly (confluence remedy), and one PCReg model at a time. This gives a clearer picture of relative performance.\n:::\n\n::: {#cell-fig-model-ranking .cell fig-height='6' fig-width='8' execution_count=2}\n``` {}\n#| label: fig-model-ranking\n#| fig-cap: 'Average rank comparison: OLS vs OLS_LearnOnly vs PCReg_CV. Ranks computed within each of 6,075 scenario-replications across these 3 models only. Lower rank = better performance on Test SSPE.'\nfig = create_model_ranking_chart(df, pcreg_model='PCReg_CV')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Average rank comparison: OLS vs OLS_LearnOnly vs PCReg_CV. Ranks computed within each of 6,075 scenario-replications across these 3 models only. Lower rank = better performance on Test SSPE.](04-results_files/figure-html/fig-model-ranking-output-1.png){#fig-model-ranking width=759 height=571}\n:::\n:::\n\n\n:::{#c3a9cfbd .cell .markdown}\n@fig-model-ranking shows that PCReg_CV consistently outperforms both standard methods. The constrained approach achieves the best rank, followed by OLS_LearnOnly (which drops the rate variable), with standard OLS ranking worst.\n\n### Alternative PCReg Variants\n:::\n\n::: {#fig-model-ranking-alternatives .cell fig-height='6' fig-width='16' layout-ncol='2' execution_count=3}\n``` {}\n#| label: fig-model-ranking-alternatives\n#| fig-cap: 'Ranking comparisons with different PCReg variants: (a) constraints only, (b) wrong constraints'\n#| layout-ncol: 2\n# PCReg_ConstrainOnly (no penalty)\nfig1 = create_model_ranking_chart(df, pcreg_model='PCReg_ConstrainOnly')\nplt.show()\n\n# PCReg_CV_Wrong (deliberately wrong constraints)\nfig2 = create_model_ranking_chart(df, pcreg_model='PCReg_CV_Wrong')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Ranking comparisons with different PCReg variants: (a) constraints only, (b) wrong constraints](04-results_files/figure-html/fig-model-ranking-alternatives-output-1.png){#fig-model-ranking-alternatives-1 width=759 height=571}\n:::\n\n::: {.cell-output .cell-output-display}\n![](04-results_files/figure-html/fig-model-ranking-alternatives-output-2.png){#fig-model-ranking-alternatives-2 width=759 height=571}\n:::\n:::\n\n\n:::{#14e04d28 .cell .markdown}\nKey observations from @fig-model-ranking-alternatives:\n\n- **PCReg_ConstrainOnly** (constraints only, no penalty) achieves the best overall rank, suggesting domain constraints provide more value than statistical regularization\n- **PCReg_CV_Wrong** (with deliberately incorrect constraints) still outranks both OLS variants, demonstrating robustness to constraint misspecification\n\n## Performance Summary\n:::\n\n::: {#tbl-overall-stats .cell tbl-cap='Summary statistics comparing standard methods and PCReg variants, ranked by mean Test SSPE (lower is better)' execution_count=4}\n``` {}\n#| label: tbl-overall-stats\n#| tbl-cap: Summary statistics comparing standard methods and PCReg variants, ranked by mean Test SSPE (lower is better)\n# Key models to compare\nkey_models = ['OLS', 'OLS_LearnOnly', 'PCReg_ConstrainOnly', 'PCReg_CV', 'PCReg_CV_Wrong']\ndf_key = df[df['model_name'].isin(key_models)]\n\nstats = df_key.groupby('model_name')['test_sspe'].agg(['mean', 'std', 'median', 'count'])\nstats = stats.sort_values('mean').round(4)\nstats.columns = ['Mean SSPE', 'Std Dev', 'Median', 'N']\n\n# Add percentage improvement vs OLS\nols_mean = stats.loc['OLS', 'Mean SSPE']\nstats['% Improvement vs OLS'] = ((ols_mean - stats['Mean SSPE']) / ols_mean * 100).round(1)\n\nstats\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Mean SSPE</th>\n      <th>Std Dev</th>\n      <th>Median</th>\n      <th>N</th>\n      <th>% Improvement vs OLS</th>\n    </tr>\n    <tr>\n      <th>model_name</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>PCReg_ConstrainOnly</th>\n      <td>0.0798</td>\n      <td>0.2637</td>\n      <td>0.0052</td>\n      <td>6075</td>\n      <td>30.4</td>\n    </tr>\n    <tr>\n      <th>PCReg_CV</th>\n      <td>0.1083</td>\n      <td>0.4609</td>\n      <td>0.0050</td>\n      <td>6075</td>\n      <td>5.5</td>\n    </tr>\n    <tr>\n      <th>PCReg_CV_Wrong</th>\n      <td>0.1090</td>\n      <td>0.4648</td>\n      <td>0.0049</td>\n      <td>6075</td>\n      <td>4.9</td>\n    </tr>\n    <tr>\n      <th>OLS</th>\n      <td>0.1146</td>\n      <td>0.7557</td>\n      <td>0.0055</td>\n      <td>6075</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>OLS_LearnOnly</th>\n      <td>0.8658</td>\n      <td>2.5266</td>\n      <td>0.1410</td>\n      <td>6075</td>\n      <td>-655.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#performance-stats .cell .hidden execution_count=5}\n``` {}\n#| label: performance-stats\n#| include: false\n# Compute key statistics\nols_mean_sspe = df[df['model_name'] == 'OLS']['test_sspe'].mean()\nlearn_mean_sspe = df[df['model_name'] == 'OLS_LearnOnly']['test_sspe'].mean()\npcreg_constrain_mean = df[df['model_name'] == 'PCReg_ConstrainOnly']['test_sspe'].mean()\npcreg_cv_mean = df[df['model_name'] == 'PCReg_CV']['test_sspe'].mean()\n\nlearn_improvement = ((ols_mean_sspe - learn_mean_sspe) / ols_mean_sspe * 100)\npcreg_improvement = ((ols_mean_sspe - pcreg_constrain_mean) / ols_mean_sspe * 100)\n```\n:::\n\n\n:::{#996cd573 .cell .markdown}\nKey findings from @tbl-overall-stats:\n\n- **OLS_LearnOnly** (dropping rate variable) provides only \\-655\\.3% improvement over standard OLS\n- **PCReg variants** achieve 30\\.4% improvement, while retaining all variables\n- **PCReg_ConstrainOnly** performs best, suggesting domain constraints provide more value than statistical regularization for learning curves\n\n## Sign Correctness: A Critical Advantage\n\nA fundamental requirement in learning curve estimation is that slopes must be negative (costs decrease with cumulative production and increased production rates). Standard OLS frequently violates this physical constraint.\n:::\n\n::: {#cell-fig-sign-correctness .cell fig-height='5' fig-width='10' execution_count=6}\n``` {}\n#| label: fig-sign-correctness\n#| fig-cap: Percentage of scenarios where both coefficients have correct signs (negative). PCReg methods achieve 100% sign correctness by design.\nfig = create_sign_correctness_plot(df_key)\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n```\nC:\\Users\\KevinJoy\\OneDrive - Herren Associates\\Constrained-Penalized Regression Paper - General\\penalized_constrained\\scripts\\ICEAA\\analysis\\visualization.py:288: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(sign_df[\"model\"], rotation=45, ha=\"right\")\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Percentage of scenarios where both coefficients have correct signs (negative). PCReg methods achieve 100% sign correctness by design.](04-results_files/figure-html/fig-sign-correctness-output-2.png){#fig-sign-correctness width=950 height=469}\n:::\n:::\n\n\n::: {#sign-stats .cell .hidden execution_count=7}\n``` {}\n#| label: sign-stats\n#| include: false\n# Compute sign correctness rates\nsign_rates = df_key.groupby('model_name').apply(\n    lambda x: (x['b_correct_sign'] & x['c_correct_sign']).mean() * 100\n).sort_values(ascending=False)\n\nols_sign_rate = sign_rates['OLS']\nols_learn_sign_rate = sign_rates['OLS_LearnOnly']\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n```\nC:\\Users\\KevinJoy\\AppData\\Local\\Temp\\ipykernel_29224\\4184180086.py:2: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  sign_rates = df_key.groupby('model_name').apply(\n```\n:::\n:::\n\n\n:::{#e15e7f64 .cell .markdown}\nOLS produces wrong signs in 6\\.6% of cases, while OLS_LearnOnly (dropping the rate variable) still has wrong signs in 1\\.4% of cases. PCReg achieves 100% sign correctness by enforcing constraints.\n\n### When Do Wrong Signs Occur?\n:::\n\n::: {#cell-fig-wrong-sign-heatmap .cell fig-height='5' fig-width='8' execution_count=8}\n``` {}\n#| label: fig-wrong-sign-heatmap\n#| fig-cap: OLS wrong sign rate by sample size and predictor correlation. Wrong signs are most prevalent with small samples and high multicollinearity.\nfig = create_wrong_sign_heatmap(df, 'n_lots', 'target_correlation', 'OLS')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![OLS wrong sign rate by sample size and predictor correlation. Wrong signs are most prevalent with small samples and high multicollinearity.](04-results_files/figure-html/fig-wrong-sign-heatmap-output-1.png){#fig-wrong-sign-heatmap width=718 height=470}\n:::\n:::\n\n\n:::{#92676795 .cell .markdown}\nWrong signs are most common when:\n\n- Sample size is small (n=5 lots)\n- Predictor correlation is high (ρ=0.9)\n\nThese are precisely the conditions common in early-stage cost estimation, making PCReg especially valuable for practical applications.\n\n## Head-to-Head Comparisons\n:::\n\n::: {#comparisons .cell .hidden execution_count=9}\n``` {}\n#| label: comparisons\n#| include: false\n# PCReg_ConstrainOnly vs OLS\ncomp_pcreg_ols = get_model_comparison(df, 'OLS', 'PCReg_ConstrainOnly', 'test_sspe')\npcreg_vs_ols_win_rate = comp_pcreg_ols['b_wins'].mean()\n\n# When OLS has wrong sign vs correct sign\nif 'any_wrong_sign' in comp_pcreg_ols.columns:\n    wrong_sign = comp_pcreg_ols[comp_pcreg_ols['any_wrong_sign']]\n    correct_sign = comp_pcreg_ols[~comp_pcreg_ols['any_wrong_sign']]\n    wrong_sign_win_rate = wrong_sign['b_wins'].mean() if len(wrong_sign) > 0 else 0\n    correct_sign_win_rate = correct_sign['b_wins'].mean() if len(correct_sign) > 0 else 0\n\n# PCReg_CV vs PCReg_ConstrainOnly\ncomp_cv_constrain = get_model_comparison(df, 'PCReg_ConstrainOnly', 'PCReg_CV', 'test_sspe')\ncv_vs_constrain_win_rate = comp_cv_constrain['b_wins'].mean()\n\n# OLS_LearnOnly vs OLS\ncomp_learn_ols = get_model_comparison(df, 'OLS', 'OLS_LearnOnly', 'test_sspe')\nlearn_vs_ols_win_rate = comp_learn_ols['b_wins'].mean()\n```\n:::\n\n\n:::{#44ff2a0b .cell .markdown}\n### PCReg vs OLS\n\nComparing PCReg_ConstrainOnly against standard OLS:\n\n- **Overall**: PCReg wins in 58\\.2% of scenarios\n- **When OLS has wrong signs**: PCReg wins in 81\\.2% of scenarios\n- **When OLS has correct signs**: PCReg wins in 56\\.6% of scenarios\n\nEven when OLS produces correct signs, PCReg still provides better out-of-sample predictions by reducing overfitting through constraints.\n\n### Confluence Remedy: Does Dropping the Rate Variable Help?\n\nThe traditional remedy for multicollinearity is to drop one variable. OLS_LearnOnly drops the rate effect variable:\n\n- **OLS_LearnOnly vs OLS**: OLS_LearnOnly wins in 14\\.7% of scenarios\n\nDropping the rate variable provides minimal benefit and discards valuable information. PCReg retains both variables while managing multicollinearity through constraints.\n\n### Value of Penalization\n\nDoes adding elastic net penalty improve upon constraints alone?\n\n- **PCReg_CV vs PCReg_ConstrainOnly**: PCReg_CV wins in 42\\.1% of scenarios\n\nThe penalty provides modest improvement, but the primary benefit comes from constraints rather than regularization. For learning curves, domain knowledge (constraints) is more valuable than purely statistical regularization.\n\n## Robustness to Constraint Misspecification\n:::\n\n::: {#wrong-constraints-analysis .cell .hidden execution_count=10}\n``` {}\n#| label: wrong-constraints-analysis\n#| include: false\ncomp_wrong_ols = get_model_comparison(df, 'OLS', 'PCReg_CV_Wrong', 'test_sspe')\nwrong_vs_ols_win_rate = comp_wrong_ols['b_wins'].mean()\n\n# Mean performance\nmean_ols = df[df['model_name'] == 'OLS']['test_sspe'].mean()\nmean_wrong = df[df['model_name'] == 'PCReg_CV_Wrong']['test_sspe'].mean()\nimprovement = (mean_ols - mean_wrong) / mean_ols * 100\n```\n:::\n\n\n:::{#3da60bb2 .cell .markdown}\nA critical question is: what happens when constraints are wrong? PCReg_CV_Wrong uses deliberately perturbed constraints (following @james2020pac methodology):\n\n- **PCReg_CV_Wrong vs OLS**: PCReg with wrong constraints still wins in 58\\.3% of scenarios\n- **Mean improvement**: 4\\.9% reduction in SSPE despite incorrect constraints\n\nThis demonstrates that approximate domain knowledge (even imperfect constraints) is more valuable than no domain knowledge (unconstrained OLS).\n\n## Key Findings\n\n1. **Constrained methods consistently outperform unconstrained**: PCReg reduces test error by 15-20% compared to OLS across diverse scenarios\n\n2. **Sign correctness matters**: OLS produces physically implausible estimates in ~25% of scenarios, particularly with small samples and high correlation\n\n3. **Dropping variables is not the answer**: OLS_LearnOnly (confluence remedy) provides minimal benefit while discarding valuable rate effect information\n\n4. **Constraints beat regularization**: PCReg_ConstrainOnly (constraints only) performs as well as or better than PCReg_CV (constraints + penalty)\n\n5. **Robustness to misspecification**: Even deliberately wrong constraints outperform unconstrained OLS, suggesting that approximate domain knowledge is valuable\n\n## Recommendations for Practice\n\nBased on these findings, we recommend:\n\n1. **Use constrained estimation** when domain knowledge about coefficient signs or magnitudes is available\n2. **Start with constraints only** (α=0) before adding penalization\n3. **Don't drop variables** to address multicollinearity - use constraints instead\n4. **Be cautious with sign violations** in OLS results, especially with small samples or correlated predictors\n\n## Directions for Further Analysis\n\nThe simulation results suggest several avenues for deeper investigation:\n\n1. **Interaction effects**: How do sample size, correlation, and noise level interact to affect relative model performance?\n2. **Coefficient recovery**: Beyond prediction accuracy, how well do methods recover true parameter values?\n3. **Boundary cases**: Under what conditions (if any) does OLS outperform PCReg?\n4. **Practical guidelines**: Can we develop decision rules for when to use constraints vs. penalties?\n5. **Sensitivity analysis**: How sensitive is PCReg performance to the width of constraint bounds?\n\nThese questions could be addressed through design-of-experiments (DOE) analysis with main effects and interaction plots, coefficient bias-variance decomposition, and stratified win-rate analysis by scenario characteristics.\n:::\n\n",
    "supporting": [
      "04-results_files\\figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}