{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "# Introduction\n",
    "\n",
    "## The Problem: Small, Correlated Datasets in Cost Estimation\n",
    "\n",
    "Developing hundreds of Cost Estimating Relationships (CERs) for small datasets ranging from 5-30 data points, a recurring pattern emerges: strong fit statistics (R², F-Statistics) but nonsensical coefficients—wrong signs, implausible magnitudes, and poor p-values for critical predictors. As Department of Defense analysts know, this story may feel too close to home.\n",
    "\n",
    "Multicollinear datasets are a frequent presence in cost analysis, causing models to misbehave. The consequences of multicollinearity in small samples are well-documented ([Flynn and James 2016](#ref-flynn2016multicollinearity)):\n",
    "\n",
    "-   **“Bouncing β’s”**—unstable coefficient estimates that swing wildly with small changes in the data\n",
    "-   **Wrong coefficient signs**—estimates flip positive/negative contrary to domain knowledge\n",
    "-   **Unreliable hypothesis testing**—high F-statistic but statistically insignificant individual t-statistics\n",
    "-   **Hidden extrapolation**—predictions fall outside the convex hull of training data despite appearing within variable ranges\n",
    "-   **Inflated variance**—coefficient variance increases by factor $1/(1-R^2)$ where $R^2$ is correlation between predictors\n",
    "\n",
    "### Motivating Example: Cost Improvement Curve with Rate Effect\n",
    "\n",
    "Learning curves are fundamental to cost estimation in manufacturing and aerospace industries. The classic power-law model describes how unit costs decrease with cumulative production:\n",
    "\n",
    "$$Y = T_1 \\cdot X^b$$\n",
    "\n",
    "where $Y$ is the unit cost, $T_1$ is the theoretical first unit cost, $X$ is the cumulative unit number, and $b$ is the learning curve slope (typically negative, indicating cost reduction with experience).\n",
    "\n",
    "When multiple factors affect costs (e.g., production rate effects), the model extends to:\n",
    "\n",
    "$$Y = T_1 \\cdot X_1^b \\cdot X_2^c \\cdot \\varepsilon$$ {#eq-learning-rate}\n",
    "\n",
    "where $X_1$ represents lot midpoint (Learning variable) and $X_2$ represents lot quantity (Rate variable). In this specification, lot midpoint is inherently correlated with lot size as production ramps up from Prototypes to Low-Rate Initial Production (LRIP) to Full-Rate Production (FRP). Correlations of $\\rho = (-0.3, 0.88)$ have been found in Selected Acquisition Reports comparing lot size to cumulative quantities. Domain knowledge establishes that learning and rate slopes should be $\\leq 100\\%$ (i.e., costs should not increase with cumulative production).\n",
    "\n",
    "## Diagnosing Multicollinearity\n",
    "\n",
    "Standard diagnostic measures for detecting multicollinearity include:\n",
    "\n",
    "| Diagnostic | Threshold | Interpretation |\n",
    "|----------------------|---------------------|-----------------------------|\n",
    "| Simple correlation $r_{X_i,X_j}$ | \\> 0.8 | Potential multicollinearity |\n",
    "| VIF (Variance Inflation Factor) | \\> 10 | Likely harmful (CEBoK threshold) |\n",
    "| Condition number $\\kappa$ | \\> 30 | Collinearity harmful |\n",
    "| R² among predictors | \\> 0.90 | Harmful if $R^2$ for $X_i \\vert \\text{other } X$’s \\> 0.90 |\n",
    "| F vs t mismatch | High F, low t | Classic multicollinearity symptom |\n",
    "\n",
    "The Variance Inflation Factor is defined as $\\text{VIF} = 1/(1-R^2_{X_i|\\text{other}X})$, and the condition number $\\kappa = \\lambda_{\\max}/\\lambda_{\\min}$ from eigenvalue decomposition of $X'X$.\n",
    "\n",
    "## Traditional Remedies and Their Limitations\n",
    "\n",
    "Traditional approaches each come with trade-offs ([Flynn and James 2016](#ref-flynn2016multicollinearity)):\n",
    "\n",
    "| Remedy | Description | Limitation |\n",
    "|------------------|----------------------------|--------------------------|\n",
    "| Collect more data | Increases sample size, reduces variance | Often infeasible in defense cost analysis |\n",
    "| Drop variables | Remove collinear predictors via confluence analysis | May lose domain-required variables |\n",
    "| Centering/Scaling | Reduces structural multicollinearity | Only helps polynomial/interaction terms |\n",
    "| Ridge Regression | L2 penalty shrinks coefficients | No sparsity; no domain constraints |\n",
    "| Lasso Regression | L1 penalty enables variable selection | Arbitrary selection among correlated vars; no constraints |\n",
    "| Elastic Net | Combined L1+L2 penalties | Still no explicit domain constraints |\n",
    "| PCA / PLS | Transforms to uncorrelated components | Loses coefficient interpretability |\n",
    "\n",
    "## Theoretical Foundation\n",
    "\n",
    "### Why Some Regularization is Always Optimal\n",
    "\n",
    "> **Theobald-Farebrother Theorem (1974, 1976)**\n",
    ">\n",
    "> For any OLS problem, there exists a ridge parameter $\\lambda^* > 0$ such that the ridge estimator has strictly lower Mean Squared Error (MSE) than OLS. This result holds for the population MSE (true prediction risk), not merely training error ([Theobald 1974](#ref-theobald1974); [Farebrother 1976](#ref-farebrother1976)).\n",
    "\n",
    "**Why This Matters**: OLS minimizes training error but may overfit—especially with correlated predictors. Ridge introduces bias but reduces variance. The theorem guarantees that for some $\\lambda > 0$, the variance reduction exceeds the bias increase, yielding lower total error.\n",
    "\n",
    "**The Practical Challenge**: The optimal $\\lambda^*$ depends on unknown population parameters ($\\beta$, $\\sigma^2$). Cross-validation provides an empirical estimate. When CV selects $\\lambda \\approx 0$, OLS was already near-optimal. The framework adapts automatically.\n",
    "\n",
    "### Constrained Methods Superior to Unconstrained\n",
    "\n",
    "The Penalized and Constrained (PAC) optimization method developed by James, Paulson, and Rusmevichientong ([2020](#ref-james2020pac)) found:\n",
    "\n",
    "> “The results suggest that PAC and relaxed PAC are surprisingly robust to random violations in the constraints. While both methods deteriorated slightly as \\[constraint error\\] increased, they were still both superior to their unconstrained counterparts for all values of \\[error\\] and all settings.”\n",
    "\n",
    "## Research Contribution\n",
    "\n",
    "This paper provides a practical guide and framework that combines penalized regularization (Lasso, Ridge, Elastic Net) with constrained optimization in the context of cost estimation. Key contributions include:\n",
    "\n",
    "1.  **Investigation of small-sample applicability**: Most research on penalized methods uses large datasets—does it apply to cost estimation’s typical 5-30 point samples?\n",
    "\n",
    "2.  **Python package implementation**: The `penalized_constrained` package combines Elastic Net penalties with lower and upper bound constraints on coefficients\n",
    "\n",
    "3.  **Proper diagnostic adjustments**: Via Generalized Degrees of Freedom (GDF) for constraint-driven regression ([Hu 2010](#ref-hu2010gdf))\n",
    "\n",
    "4.  **Cross-validation framework**: For likelihood-free hyperparameter selection\n",
    "\n",
    "5.  **sklearn-compatible implementation**: For practical application in existing workflows\n",
    "\n",
    "6.  **Comprehensive benchmarks**: Simulation data comparing proposed method against OLS, Ridge, Lasso, and constrained-only approaches across varying sample sizes, correlations, and error variances\n",
    "\n",
    "7.  **Practical guidance**: On when and how to implement these algorithms\n",
    "\n",
    "**Future Work**: Validation on actual program data using publicly available Selected Acquisition Reports (SARs), which are not subject to CUI restrictions.\n",
    "\n",
    "## Paper Organization\n",
    "\n",
    "The remainder of this paper is organized as follows: ([**sec-methodology?**](#ref-sec-methodology)) presents the mathematical formulation and algorithm details, ([**sec-simulation-design?**](#ref-sec-simulation-design)) describes our Monte Carlo study design, ([**sec-results?**](#ref-sec-results)) presents the empirical findings, ([**sec-doe-analysis?**](#ref-sec-doe-analysis)) provides rigorous statistical analysis using DOE methodology, and ([**sec-discussion?**](#ref-sec-discussion)) offers practical recommendations and discusses limitations."
   ],
   "id": "b62d7629-edd7-4e62-a4c2-8efadb6678e5"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!-- -->"
   ],
   "id": "2f9c56ba-bcff-40f5-883a-c3f5646ed03a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` markdown\n",
    "# Introduction {#sec-introduction}\n",
    "\n",
    "## The Problem: Small, Correlated Datasets in Cost Estimation\n",
    "\n",
    "Developing hundreds of Cost Estimating Relationships (CERs) for small datasets ranging from 5-30 data points, a recurring pattern emerges: strong fit statistics (R², F-Statistics) but nonsensical coefficients---wrong signs, implausible magnitudes, and poor p-values for critical predictors. As Department of Defense analysts know, this story may feel too close to home.\n",
    "\n",
    "Multicollinear datasets are a frequent presence in cost analysis, causing models to misbehave. The consequences of multicollinearity in small samples are well-documented [@flynn2016multicollinearity]:\n",
    "\n",
    "- **\"Bouncing β's\"**---unstable coefficient estimates that swing wildly with small changes in the data\n",
    "- **Wrong coefficient signs**---estimates flip positive/negative contrary to domain knowledge\n",
    "- **Unreliable hypothesis testing**---high F-statistic but statistically insignificant individual t-statistics\n",
    "- **Hidden extrapolation**---predictions fall outside the convex hull of training data despite appearing within variable ranges\n",
    "- **Inflated variance**---coefficient variance increases by factor $1/(1-R^2)$ where $R^2$ is correlation between predictors\n",
    "\n",
    "### Motivating Example: Cost Improvement Curve with Rate Effect\n",
    "\n",
    "Learning curves are fundamental to cost estimation in manufacturing and aerospace industries. The classic power-law model describes how unit costs decrease with cumulative production:\n",
    "\n",
    "$$Y = T_1 \\cdot X^b$$\n",
    "\n",
    "where $Y$ is the unit cost, $T_1$ is the theoretical first unit cost, $X$ is the cumulative unit number, and $b$ is the learning curve slope (typically negative, indicating cost reduction with experience).\n",
    "\n",
    "When multiple factors affect costs (e.g., production rate effects), the model extends to:\n",
    "\n",
    "$$Y = T_1 \\cdot X_1^b \\cdot X_2^c \\cdot \\varepsilon$$ {#eq-learning-rate}\n",
    "\n",
    "where $X_1$ represents lot midpoint (Learning variable) and $X_2$ represents lot quantity (Rate variable). In this specification, lot midpoint is inherently correlated with lot size as production ramps up from Prototypes to Low-Rate Initial Production (LRIP) to Full-Rate Production (FRP). Correlations of $\\rho = (-0.3, 0.88)$ have been found in Selected Acquisition Reports comparing lot size to cumulative quantities. Domain knowledge establishes that learning and rate slopes should be $\\leq 100\\%$ (i.e., costs should not increase with cumulative production).\n",
    "\n",
    "## Diagnosing Multicollinearity\n",
    "\n",
    "Standard diagnostic measures for detecting multicollinearity include:\n",
    "\n",
    "| Diagnostic | Threshold | Interpretation |\n",
    "|------------|-----------|----------------|\n",
    "| Simple correlation $r_{X_i,X_j}$ | > 0.8 | Potential multicollinearity |\n",
    "| VIF (Variance Inflation Factor) | > 10 | Likely harmful (CEBoK threshold) |\n",
    "| Condition number $\\kappa$ | > 30 | Collinearity harmful |\n",
    "| R² among predictors | > 0.90 | Harmful if $R^2$ for $X_i \\vert \\text{other } X$'s > 0.90 |\n",
    "| F vs t mismatch | High F, low t | Classic multicollinearity symptom |\n",
    "\n",
    ": Multicollinearity diagnostic thresholds {#tbl-diagnostics}\n",
    "\n",
    "The Variance Inflation Factor is defined as $\\text{VIF} = 1/(1-R^2_{X_i|\\text{other}X})$, and the condition number $\\kappa = \\lambda_{\\max}/\\lambda_{\\min}$ from eigenvalue decomposition of $X'X$.\n",
    "\n",
    "## Traditional Remedies and Their Limitations\n",
    "\n",
    "Traditional approaches each come with trade-offs [@flynn2016multicollinearity]:\n",
    "\n",
    "| Remedy | Description | Limitation |\n",
    "|--------|-------------|------------|\n",
    "| Collect more data | Increases sample size, reduces variance | Often infeasible in defense cost analysis |\n",
    "| Drop variables | Remove collinear predictors via confluence analysis | May lose domain-required variables |\n",
    "| Centering/Scaling | Reduces structural multicollinearity | Only helps polynomial/interaction terms |\n",
    "| Ridge Regression | L2 penalty shrinks coefficients | No sparsity; no domain constraints |\n",
    "| Lasso Regression | L1 penalty enables variable selection | Arbitrary selection among correlated vars; no constraints |\n",
    "| Elastic Net | Combined L1+L2 penalties | Still no explicit domain constraints |\n",
    "| PCA / PLS | Transforms to uncorrelated components | Loses coefficient interpretability |\n",
    "\n",
    ": Traditional multicollinearity remedies {#tbl-remedies}\n",
    "\n",
    "## Theoretical Foundation\n",
    "\n",
    "### Why Some Regularization is Always Optimal\n",
    "\n",
    "::: {.callout-note title=\"Theobald-Farebrother Theorem (1974, 1976)\"}\n",
    "For any OLS problem, there exists a ridge parameter $\\lambda^* > 0$ such that the ridge estimator has strictly lower Mean Squared Error (MSE) than OLS. This result holds for the population MSE (true prediction risk), not merely training error [@theobald1974; @farebrother1976].\n",
    ":::\n",
    "\n",
    "**Why This Matters**: OLS minimizes training error but may overfit---especially with correlated predictors. Ridge introduces bias but reduces variance. The theorem guarantees that for some $\\lambda > 0$, the variance reduction exceeds the bias increase, yielding lower total error.\n",
    "\n",
    "**The Practical Challenge**: The optimal $\\lambda^*$ depends on unknown population parameters ($\\beta$, $\\sigma^2$). Cross-validation provides an empirical estimate. When CV selects $\\lambda \\approx 0$, OLS was already near-optimal. The framework adapts automatically.\n",
    "\n",
    "### Constrained Methods Superior to Unconstrained\n",
    "\n",
    "The Penalized and Constrained (PAC) optimization method developed by @james2020pac found:\n",
    "\n",
    "> \"The results suggest that PAC and relaxed PAC are surprisingly robust to random violations in the constraints. While both methods deteriorated slightly as [constraint error] increased, they were still both superior to their unconstrained counterparts for all values of [error] and all settings.\"\n",
    "\n",
    "## Research Contribution\n",
    "\n",
    "This paper provides a practical guide and framework that combines penalized regularization (Lasso, Ridge, Elastic Net) with constrained optimization in the context of cost estimation. Key contributions include:\n",
    "\n",
    "1. **Investigation of small-sample applicability**: Most research on penalized methods uses large datasets---does it apply to cost estimation's typical 5-30 point samples?\n",
    "\n",
    "2. **Python package implementation**: The `penalized_constrained` package combines Elastic Net penalties with lower and upper bound constraints on coefficients\n",
    "\n",
    "3. **Proper diagnostic adjustments**: Via Generalized Degrees of Freedom (GDF) for constraint-driven regression [@hu2010gdf]\n",
    "\n",
    "4. **Cross-validation framework**: For likelihood-free hyperparameter selection\n",
    "\n",
    "5. **sklearn-compatible implementation**: For practical application in existing workflows\n",
    "\n",
    "6. **Comprehensive benchmarks**: Simulation data comparing proposed method against OLS, Ridge, Lasso, and constrained-only approaches across varying sample sizes, correlations, and error variances\n",
    "\n",
    "7. **Practical guidance**: On when and how to implement these algorithms\n",
    "\n",
    "**Future Work**: Validation on actual program data using publicly available Selected Acquisition Reports (SARs), which are not subject to CUI restrictions.\n",
    "\n",
    "## Paper Organization\n",
    "\n",
    "The remainder of this paper is organized as follows: @sec-methodology presents the mathematical formulation and algorithm details, @sec-simulation-design describes our Monte Carlo study design, @sec-results presents the empirical findings, @sec-doe-analysis provides rigorous statistical analysis using DOE methodology, and @sec-discussion offers practical recommendations and discusses limitations.\n",
    "```\n",
    "\n",
    "Farebrother, R. W. 1976. “Further Results on the Mean Square Error of Ridge Regression.” *Journal of the Royal Statistical Society: Series B* 38 (3): 248–50. <https://doi.org/10.1111/j.2517-6161.1976.tb01588.x>.\n",
    "\n",
    "Flynn, Bernard, and Andrew James. 2016. “Multicollinearity in CER Development.” In *ICEAA Professional Development & Training Workshop*.\n",
    "\n",
    "Hu, Shu-Ping. 2010. “Generalized Degrees of Freedom for Constrained CERs.” PRT-191. Tecolote Research.\n",
    "\n",
    "James, Gareth M., Courtney Paulson, and Paat Rusmevichientong. 2020. “Penalized and Constrained Optimization: An Application to High-Dimensional Website Advertising.” *Journal of the American Statistical Association* 115 (529): 107–22. <https://doi.org/10.1080/01621459.2019.1609970>.\n",
    "\n",
    "Theobald, C. M. 1974. “Generalizations of Mean Square Error Applied to Ridge Regression.” *Journal of the Royal Statistical Society: Series B* 36 (1): 103–6. <https://doi.org/10.1111/j.2517-6161.1974.tb00990.x>."
   ],
   "id": "96ddca37-e70b-4c6f-b801-4d36972ed723"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
