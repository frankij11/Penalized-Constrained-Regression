# Statistical Analysis {#sec-doe-analysis}

```{python}
#| label: setup-doe
#| include: false
import sys
from pathlib import Path

# Find project root by looking for pyproject.toml
def find_project_root():
    current = Path.cwd()
    for parent in [current] + list(current.parents):
        if (parent / "pyproject.toml").exists():
            return parent
    return current.parent.parent  # Fallback

project_root = find_project_root()
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "scripts"))

import pandas as pd
import numpy as np
from scipy import stats

from scripts.ICEAA.analysis import load_simulation_results

df = load_simulation_results()

# Try importing pingouin for statistical tests
try:
    import pingouin as pg
    HAS_PINGOUIN = True
except ImportError:
    HAS_PINGOUIN = False
    print("Note: Install pingouin for full statistical analysis: pip install pingouin")
```

This section provides rigorous statistical analysis using Design of Experiments (DOE) methodology.

## Repeated Measures ANOVA

We use repeated measures ANOVA to test whether model choice significantly affects performance, accounting for the fact that all models are evaluated on the same data scenarios.

```{python}
#| label: rm-anova
#| eval: !expr HAS_PINGOUIN

# Create scenario ID for repeated measures structure
scenario_cols = ['n_lots', 'target_correlation', 'cv_error', 'learning_rate', 'rate_effect', 'replication']
df['scenario_id'] = df.groupby(scenario_cols).ngroup()

# Focus on key models for statistical comparison
DOE_MODELS = ['OLS', 'PCReg_ConstrainOnly', 'PCReg_CV', 'PCReg_CV_Tight']
df_doe = df[df['model_name'].isin(DOE_MODELS)].copy()

# Repeated measures ANOVA
rm_aov = pg.rm_anova(
    data=df_doe,
    dv='test_sspe',
    within='model_name',
    subject='scenario_id',
    correction=True
)

print("Repeated Measures ANOVA Results:")
print("="*60)
display(rm_aov)

# Extract key statistics
eta_sq_col = 'np2' if 'np2' in rm_aov.columns else 'ng2'
eta_sq = rm_aov[eta_sq_col].values[0]
f_val = rm_aov['F'].values[0]
p_col = 'p-GG-corr' if 'p-GG-corr' in rm_aov.columns else 'p-unc'
p_val = rm_aov[p_col].values[0]

print(f"\nInterpretation:")
print(f"  F-statistic: {f_val:.2f}")
print(f"  p-value: {p_val:.4e}")
print(f"  Effect size (η²): {eta_sq:.4f}")
print(f"  Model choice explains {eta_sq*100:.1f}% of variance in test SSPE")
```

## Pairwise Comparisons

```{python}
#| label: pairwise
#| eval: !expr HAS_PINGOUIN

pairwise = pg.pairwise_tests(
    data=df_doe,
    dv='test_sspe',
    within='model_name',
    subject='scenario_id',
    padjust='holm',
    effsize='hedges'
)

print("Pairwise Comparisons (Holm-Bonferroni corrected):")
print("="*60)

# Focus on key comparisons
key_pairs = [
    ('PCReg_CV', 'OLS'),
    ('PCReg_ConstrainOnly', 'OLS'),
    ('PCReg_CV_Tight', 'OLS'),
    ('PCReg_CV_Tight', 'PCReg_CV'),
]

results = []
for a, b in key_pairs:
    row = pairwise[(pairwise['A'] == a) & (pairwise['B'] == b)]
    if len(row) == 0:
        row = pairwise[(pairwise['A'] == b) & (pairwise['B'] == a)]

    if len(row) > 0:
        row = row.iloc[0]
        g = row['hedges']
        p = row['p-corr']

        sig = '***' if p < 0.001 else ('**' if p < 0.01 else ('*' if p < 0.05 else 'ns'))
        g_abs = abs(g)
        g_size = 'negligible' if g_abs < 0.2 else ('small' if g_abs < 0.5 else ('medium' if g_abs < 0.8 else 'large'))
        better = a if g < 0 else b

        results.append({
            'Comparison': f'{a} vs {b}',
            'Hedges g': f'{g:.4f}',
            'p-value': f'{p:.4f}',
            'Significance': sig,
            'Effect Size': g_size,
            'Better Model': better
        })

results_df = pd.DataFrame(results)
display(results_df)
```

## Win Rate Analysis

```{python}
#| label: win-rates

# Prepare wide format for win rate calculation
scenario_cols = ['n_lots', 'target_correlation', 'cv_error', 'learning_rate', 'rate_effect', 'replication']
df_wide = df.pivot_table(
    index=scenario_cols,
    columns='model_name',
    values='test_sspe'
).reset_index()

# Overall win rate: PCReg_ConstrainOnly vs OLS
if 'PCReg_ConstrainOnly' in df_wide.columns and 'OLS' in df_wide.columns:
    df_wide['pcreg_wins'] = df_wide['PCReg_ConstrainOnly'] < df_wide['OLS']

    overall_wins = df_wide['pcreg_wins'].sum()
    overall_total = len(df_wide)
    overall_rate = overall_wins / overall_total

    # Binomial test
    binom_result = stats.binomtest(overall_wins, overall_total, p=0.5, alternative='greater')

    print("PCReg_ConstrainOnly vs OLS Win Rate Analysis")
    print("="*60)
    print(f"  Overall Win Rate: {overall_rate:.1%} ({overall_wins}/{overall_total})")
    print(f"  Binomial Test p-value: {binom_result.pvalue:.4e}")

    if binom_result.pvalue < 0.05:
        print("  ✓ PCReg significantly outperforms OLS (p < 0.05)")
```

## Win Rates by Design Factor

```{python}
#| label: win-rates-by-factor

FACTORS = ['n_lots', 'cv_error', 'target_correlation']

print("Win Rates by Design Factor")
print("="*60)

win_rates = []
for factor in FACTORS:
    print(f"\n{factor}:")
    for level in sorted(df_wide[factor].unique()):
        mask = df_wide[factor] == level
        level_wins = df_wide.loc[mask, 'pcreg_wins'].sum()
        level_total = mask.sum()
        level_rate = level_wins / level_total

        binom = stats.binomtest(level_wins, level_total, p=0.5, alternative='greater')
        sig = '*' if binom.pvalue < 0.05 else ''

        print(f"  {level}: {level_rate:.1%} ({level_wins}/{level_total}) {sig}")

        win_rates.append({
            'Factor': factor,
            'Level': level,
            'Win Rate': f'{level_rate:.1%}',
            'p-value': f'{binom.pvalue:.4f}',
            'Significant': 'Yes' if binom.pvalue < 0.05 else 'No'
        })

# Display as table
print("\n")
win_rates_df = pd.DataFrame(win_rates)
display(win_rates_df)
```

## Key Statistical Findings

Based on the statistical analysis:

1. **Model choice matters**: The repeated measures ANOVA confirms significant differences between models (p < 0.001)

2. **PCReg outperforms OLS**: The overall win rate exceeds 50% and is statistically significant

3. **Effect sizes are meaningful**: Hedges' g values indicate practically important differences

4. **Context matters**: Win rates vary substantially by design factor levels, suggesting PCReg is particularly valuable in specific conditions
