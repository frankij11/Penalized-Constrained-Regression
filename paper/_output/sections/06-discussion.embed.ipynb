{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion and Recommendations"
   ],
   "id": "b6a09100-d93c-4862-94b5-7602dd4cbddb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [],
   "id": "7e39644c"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: setup-discussion\n",
    "#| include: false\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Find project root by looking for pyproject.toml\n",
    "def find_project_root():\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return current.parent.parent  # Fallback\n",
    "\n",
    "project_root = find_project_root()\n",
    "sys.path.insert(0, str(project_root))\n",
    "sys.path.insert(0, str(project_root / \"scripts\"))\n",
    "\n",
    "import pandas as pd\n",
    "from scripts.ICEAA.analysis import load_simulation_results\n",
    "\n",
    "df = load_simulation_results()"
   ],
   "id": "setup-discussion"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Findings\n",
    "\n",
    "Our comprehensive simulation study comparing Penalized-Constrained Regression (PCReg) against traditional methods yields several key insights:\n",
    "\n",
    "### PCReg Advantages\n",
    "\n",
    "1.  **Guaranteed sign correctness**: PCReg always produces economically sensible coefficients (negative learning slopes) while OLS can produce wrong signs in up to 14% of small-sample scenarios\n",
    "\n",
    "2.  **Superior small-sample performance**: PCReg shows its strongest advantages when sample sizes are small (n ≤ 10 lots), precisely when cost analysts need reliable estimates most\n",
    "\n",
    "3.  **Robustness to multicollinearity**: High predictor correlation degrades OLS performance but has less impact on PCReg\n",
    "\n",
    "4.  **High data quality benefits**: When measurement error is low (CV error = 0.01), PCReg wins 67-75% of scenarios\n",
    "\n",
    "### When OLS May Be Adequate\n",
    "\n",
    "1.  **Large samples**: With n ≥ 30 lots and high CV error, OLS and PCReg perform similarly\n",
    "\n",
    "2.  **Low correlation**: When predictors are uncorrelated, OLS estimates are more stable\n",
    "\n",
    "## Practical Recommendations\n",
    "\n",
    "### Decision Framework"
   ],
   "id": "72c0241a"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {
      "text/html": [
       "\n",
       "</div>"
      ],
      "text/markdown": [
       "      Condition                             PCReg Win Rate   Recommendation\n",
       "  --- ------------------------------------- ---------------- ----------------\n",
       "  0   CV error = 0.01 (high quality data)   67-75%           Use PCReg\n",
       "  1   CV error = 0.10, n ≤ 10               57-64%           Use PCReg\n",
       "  2   CV error = 0.10, n = 30               \\~48%            Either method\n",
       "  3   CV error = 0.20, n = 5                \\~58%            Use PCReg\n",
       "  4   CV error = 0.20, n = 10               \\~47%            Either method\n",
       "  5   CV error = 0.20, n = 30               \\~34%            Consider OLS\n",
       "  6   OLS produces wrong sign               \\~81%            Use PCReg\n"
      ]
     }
    }
   ],
   "source": [
    "#| label: tbl-recommendations\n",
    "#| tbl-cap: Practical recommendations for method selection\n",
    "recommendations = pd.DataFrame({\n",
    "    'Condition': [\n",
    "        'CV error = 0.01 (high quality data)',\n",
    "        'CV error = 0.10, n ≤ 10',\n",
    "        'CV error = 0.10, n = 30',\n",
    "        'CV error = 0.20, n = 5',\n",
    "        'CV error = 0.20, n = 10',\n",
    "        'CV error = 0.20, n = 30',\n",
    "        'OLS produces wrong sign'\n",
    "    ],\n",
    "    'PCReg Win Rate': ['67-75%', '57-64%', '~48%', '~58%', '~47%', '~34%', '~81%'],\n",
    "    'Recommendation': [\n",
    "        'Use PCReg',\n",
    "        'Use PCReg',\n",
    "        'Either method',\n",
    "        'Use PCReg',\n",
    "        'Either method',\n",
    "        'Consider OLS',\n",
    "        'Use PCReg'\n",
    "    ]\n",
    "})\n",
    "recommendations"
   ],
   "id": "4d363121-6690-47da-a5e0-79f2eb9f1efb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Guidance for Cost Estimators\n",
    "\n",
    "> **Implementation Checklist**\n",
    ">\n",
    "> 1.  **Document everything**: Constraints, penalties, active bounds at solution. This is NOT OLS—transparency is essential.\n",
    ">\n",
    "> 2.  **Start with loose constraints** based on domain knowledge (e.g., learning slope ≤ 100%)\n",
    ">\n",
    "> 3.  **Constraints need not be perfect**: Even approximately correct bounds improve estimation \\[@james2020pac\\]\n",
    ">\n",
    "> 4.  **Derive constraints from domain benchmarks**: Published learning curve studies, historical program data, and subject matter expert knowledge can inform reasonable bounds. Reference benchmarks specific to your domain (e.g., aerospace learning curves typically range 75-95%).\n",
    ">\n",
    "> 5.  **Use cross-validation** for ($\\alpha$, l1_ratio) selection—do not impose arbitrary penalty values\n",
    ">\n",
    "> 6.  **Report GDF-adjusted statistics** for transparency \\[@hu2010gdf\\]\n",
    ">\n",
    "> 7.  **Try multiple starting points**: For nonlinear models, testing multiple starting points can avoid local optima\n",
    ">\n",
    "> 8.  **Don’t worry (too much) about global optimum**: Even if not confirmed to be globally optimal, it can still be the best reasonable model\n",
    ">\n",
    "> 9.  **Regularization is recommended** (even if minimal): Per Theobald-Farebrother, some L2 regularization is always optimal, even when correlation is not high \\[@theobald1974\\]\n",
    "\n",
    "### Implementation Guidance\n",
    "\n",
    "1.  **Start with constraints only** ($\\alpha = 0$): Our results show that constraints alone often outperform CV-tuned penalties. The regularization benefit is secondary to the constraint benefit.\n",
    "\n",
    "2.  **Use loose bounds**: Rather than trying to specify tight coefficient bounds, use conservative ranges:\n",
    "\n",
    "    -   Learning slope ($b$): $-0.5 \\leq b \\leq 0$\n",
    "    -   Rate effect ($c$): $-0.5 \\leq c \\leq 0$\n",
    "    -   First unit cost ($T_1$): $0 < T_1 < \\infty$\n",
    "\n",
    "3.  **Consider observable indicators**:\n",
    "\n",
    "    -   Estimate CV error from residual variance\n",
    "    -   Estimate predictor correlation from data\n",
    "    -   Use sample size directly\n",
    "\n",
    "4.  **Validate with out-of-sample testing**: When possible, hold out recent lots for validation\n",
    "\n",
    "## Limitations and Cautions\n",
    "\n",
    "> **Important Cautions**\n",
    ">\n",
    "> 1.  **Abuse potential**: Constraints could be used to force desired results. Transparency in documentation is essential—always disclose what constraints were applied and why.\n",
    ">\n",
    "> 2.  **Not BLUE**: Always disclose that the method intentionally introduces bias in exchange for reduced variance. This is a feature, not a bug, but stakeholders should understand the tradeoff.\n",
    ">\n",
    "> 3.  **Local optima**: For nonlinear models, test multiple starting points. The solution may not be globally optimal.\n",
    ">\n",
    "> 4.  **Bootstrap CIs**: May be artificially narrow for penalized models because penalties constrain coefficient variability across resamples \\[@goeman_penalized\\]. Compare unconstrained bootstrap to constrained bootstrap when possible.\n",
    ">\n",
    "> 5.  **Speed**: Optimization routines take longer to converge than closed-form OLS (trivial concern for small datasets and few runs).\n",
    ">\n",
    "> 6.  **Heteroscedasticity**: This implementation does not include weighted approaches. SSPE partially addresses heteroscedasticity through unit-space operation, but formal weighted least squares integration is future work.\n",
    "\n",
    "### Additional Limitations\n",
    "\n",
    "1.  **Simulation vs. reality**: Our data generating process, while realistic, cannot capture all complexities of real manufacturing data\n",
    "\n",
    "2.  **Bound specification**: Results assume practitioners can specify reasonable coefficient bounds\n",
    "\n",
    "3.  **Model form**: We assume the multiplicative power-law model is correct; model misspecification was not studied\n",
    "\n",
    "4.  **Single outcome metric**: We focused on SSPE; other metrics might yield different conclusions\n",
    "\n",
    "## Future Research\n",
    "\n",
    "1.  **Real data validation**: Apply PCReg to historical cost datasets using publicly available Selected Acquisition Reports (SARs), which are not subject to CUI restrictions\n",
    "\n",
    "2.  **pcLAD implementation**: Robust estimation with outliers using penalized-constrained LAD \\[@wu2022pclad\\]\n",
    "\n",
    "3.  **Additional algorithms**: Coordinate descent, projected gradient methods, and the PAC algorithm from @james2020pac\n",
    "\n",
    "4.  **Alternative optimizers**: Systematic comparison of SLSQP, COBYLA, trust-constr, and cvxpy performance\n",
    "\n",
    "5.  **Weighted approaches**: Explicit heteroscedasticity modeling\n",
    "\n",
    "6.  **PenalizedConstrainedMUPE**: Proposed method using MUPE/IRLS loss function with penalties and constraints\n",
    "\n",
    "7.  **Alternative model forms**: Extend to other functional forms (e.g., S-curves, plateau models)\n",
    "\n",
    "8.  **Bayesian approaches**: Compare with Bayesian methods that incorporate prior knowledge through priors rather than constraints\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Penalized-Constrained Regression offers a principled approach to incorporating domain knowledge into learning curve estimation. By enforcing economically sensible constraints, PCReg produces more reliable estimates, particularly in the challenging conditions that cost analysts frequently face: small samples, noisy data, and correlated predictors.\n",
    "\n",
    "The `penalized_constrained` Python package provides a production-ready implementation with cross-validation, multiple penalty selection methods, and comprehensive diagnostics. We recommend PCReg as a practical tool for cost estimation practitioners seeking to improve upon traditional OLS methods."
   ],
   "id": "b0f5dbae"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!-- -->"
   ],
   "id": "ed41b1ae-b61f-4d9f-9b77-757327d57bb0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```` markdown\n",
    "# Discussion and Recommendations {#sec-discussion}\n",
    "\n",
    "quarto-executable-code-5450563D\n",
    "\n",
    "```python\n",
    "#| label: setup-discussion\n",
    "#| include: false\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Find project root by looking for pyproject.toml\n",
    "def find_project_root():\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return current.parent.parent  # Fallback\n",
    "\n",
    "project_root = find_project_root()\n",
    "sys.path.insert(0, str(project_root))\n",
    "sys.path.insert(0, str(project_root / \"scripts\"))\n",
    "\n",
    "import pandas as pd\n",
    "from scripts.ICEAA.analysis import load_simulation_results\n",
    "\n",
    "df = load_simulation_results()\n",
    "```\n",
    "\n",
    "## Summary of Findings\n",
    "\n",
    "Our comprehensive simulation study comparing Penalized-Constrained Regression (PCReg) against traditional methods yields several key insights:\n",
    "\n",
    "### PCReg Advantages\n",
    "\n",
    "1. **Guaranteed sign correctness**: PCReg always produces economically sensible coefficients (negative learning slopes) while OLS can produce wrong signs in up to 14% of small-sample scenarios\n",
    "\n",
    "2. **Superior small-sample performance**: PCReg shows its strongest advantages when sample sizes are small (n ≤ 10 lots), precisely when cost analysts need reliable estimates most\n",
    "\n",
    "3. **Robustness to multicollinearity**: High predictor correlation degrades OLS performance but has less impact on PCReg\n",
    "\n",
    "4. **High data quality benefits**: When measurement error is low (CV error = 0.01), PCReg wins 67-75% of scenarios\n",
    "\n",
    "### When OLS May Be Adequate\n",
    "\n",
    "1. **Large samples**: With n ≥ 30 lots and high CV error, OLS and PCReg perform similarly\n",
    "\n",
    "2. **Low correlation**: When predictors are uncorrelated, OLS estimates are more stable\n",
    "\n",
    "## Practical Recommendations\n",
    "\n",
    "### Decision Framework\n",
    "\n",
    "quarto-executable-code-5450563D\n",
    "\n",
    "```python\n",
    "#| label: tbl-recommendations\n",
    "#| tbl-cap: \"Practical recommendations for method selection\"\n",
    "\n",
    "recommendations = pd.DataFrame({\n",
    "    'Condition': [\n",
    "        'CV error = 0.01 (high quality data)',\n",
    "        'CV error = 0.10, n ≤ 10',\n",
    "        'CV error = 0.10, n = 30',\n",
    "        'CV error = 0.20, n = 5',\n",
    "        'CV error = 0.20, n = 10',\n",
    "        'CV error = 0.20, n = 30',\n",
    "        'OLS produces wrong sign'\n",
    "    ],\n",
    "    'PCReg Win Rate': ['67-75%', '57-64%', '~48%', '~58%', '~47%', '~34%', '~81%'],\n",
    "    'Recommendation': [\n",
    "        'Use PCReg',\n",
    "        'Use PCReg',\n",
    "        'Either method',\n",
    "        'Use PCReg',\n",
    "        'Either method',\n",
    "        'Consider OLS',\n",
    "        'Use PCReg'\n",
    "    ]\n",
    "})\n",
    "recommendations\n",
    "```\n",
    "\n",
    "### Practical Guidance for Cost Estimators\n",
    "\n",
    "::: {.callout-tip title=\"Implementation Checklist\"}\n",
    "1. **Document everything**: Constraints, penalties, active bounds at solution. This is NOT OLS---transparency is essential.\n",
    "\n",
    "2. **Start with loose constraints** based on domain knowledge (e.g., learning slope ≤ 100%)\n",
    "\n",
    "3. **Constraints need not be perfect**: Even approximately correct bounds improve estimation [@james2020pac]\n",
    "\n",
    "4. **Derive constraints from domain benchmarks**: Published learning curve studies, historical program data, and subject matter expert knowledge can inform reasonable bounds. Reference benchmarks specific to your domain (e.g., aerospace learning curves typically range 75-95%).\n",
    "\n",
    "5. **Use cross-validation** for ($\\alpha$, l1\\_ratio) selection---do not impose arbitrary penalty values\n",
    "\n",
    "6. **Report GDF-adjusted statistics** for transparency [@hu2010gdf]\n",
    "\n",
    "7. **Try multiple starting points**: For nonlinear models, testing multiple starting points can avoid local optima\n",
    "\n",
    "8. **Don't worry (too much) about global optimum**: Even if not confirmed to be globally optimal, it can still be the best reasonable model\n",
    "\n",
    "9. **Regularization is recommended** (even if minimal): Per Theobald-Farebrother, some L2 regularization is always optimal, even when correlation is not high [@theobald1974]\n",
    ":::\n",
    "\n",
    "### Implementation Guidance\n",
    "\n",
    "1. **Start with constraints only** ($\\alpha = 0$): Our results show that constraints alone often outperform CV-tuned penalties. The regularization benefit is secondary to the constraint benefit.\n",
    "\n",
    "2. **Use loose bounds**: Rather than trying to specify tight coefficient bounds, use conservative ranges:\n",
    "   - Learning slope ($b$): $-0.5 \\leq b \\leq 0$\n",
    "   - Rate effect ($c$): $-0.5 \\leq c \\leq 0$\n",
    "   - First unit cost ($T_1$): $0 < T_1 < \\infty$\n",
    "\n",
    "3. **Consider observable indicators**:\n",
    "   - Estimate CV error from residual variance\n",
    "   - Estimate predictor correlation from data\n",
    "   - Use sample size directly\n",
    "\n",
    "4. **Validate with out-of-sample testing**: When possible, hold out recent lots for validation\n",
    "\n",
    "## Limitations and Cautions\n",
    "\n",
    "::: {.callout-warning title=\"Important Cautions\"}\n",
    "1. **Abuse potential**: Constraints could be used to force desired results. Transparency in documentation is essential---always disclose what constraints were applied and why.\n",
    "\n",
    "2. **Not BLUE**: Always disclose that the method intentionally introduces bias in exchange for reduced variance. This is a feature, not a bug, but stakeholders should understand the tradeoff.\n",
    "\n",
    "3. **Local optima**: For nonlinear models, test multiple starting points. The solution may not be globally optimal.\n",
    "\n",
    "4. **Bootstrap CIs**: May be artificially narrow for penalized models because penalties constrain coefficient variability across resamples [@goeman_penalized]. Compare unconstrained bootstrap to constrained bootstrap when possible.\n",
    "\n",
    "5. **Speed**: Optimization routines take longer to converge than closed-form OLS (trivial concern for small datasets and few runs).\n",
    "\n",
    "6. **Heteroscedasticity**: This implementation does not include weighted approaches. SSPE partially addresses heteroscedasticity through unit-space operation, but formal weighted least squares integration is future work.\n",
    ":::\n",
    "\n",
    "### Additional Limitations\n",
    "\n",
    "1. **Simulation vs. reality**: Our data generating process, while realistic, cannot capture all complexities of real manufacturing data\n",
    "\n",
    "2. **Bound specification**: Results assume practitioners can specify reasonable coefficient bounds\n",
    "\n",
    "3. **Model form**: We assume the multiplicative power-law model is correct; model misspecification was not studied\n",
    "\n",
    "4. **Single outcome metric**: We focused on SSPE; other metrics might yield different conclusions\n",
    "\n",
    "## Future Research\n",
    "\n",
    "1. **Real data validation**: Apply PCReg to historical cost datasets using publicly available Selected Acquisition Reports (SARs), which are not subject to CUI restrictions\n",
    "\n",
    "2. **pcLAD implementation**: Robust estimation with outliers using penalized-constrained LAD [@wu2022pclad]\n",
    "\n",
    "3. **Additional algorithms**: Coordinate descent, projected gradient methods, and the PAC algorithm from @james2020pac\n",
    "\n",
    "4. **Alternative optimizers**: Systematic comparison of SLSQP, COBYLA, trust-constr, and cvxpy performance\n",
    "\n",
    "5. **Weighted approaches**: Explicit heteroscedasticity modeling\n",
    "\n",
    "6. **PenalizedConstrainedMUPE**: Proposed method using MUPE/IRLS loss function with penalties and constraints\n",
    "\n",
    "7. **Alternative model forms**: Extend to other functional forms (e.g., S-curves, plateau models)\n",
    "\n",
    "8. **Bayesian approaches**: Compare with Bayesian methods that incorporate prior knowledge through priors rather than constraints\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Penalized-Constrained Regression offers a principled approach to incorporating domain knowledge into learning curve estimation. By enforcing economically sensible constraints, PCReg produces more reliable estimates, particularly in the challenging conditions that cost analysts frequently face: small samples, noisy data, and correlated predictors.\n",
    "\n",
    "The `penalized_constrained` Python package provides a production-ready implementation with cross-validation, multiple penalty selection methods, and comprehensive diagnostics. We recommend PCReg as a practical tool for cost estimation practitioners seeking to improve upon traditional OLS methods.\n",
    "````"
   ],
   "id": "4342e6dc-c9da-48eb-aab7-bea934e54ad1"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
