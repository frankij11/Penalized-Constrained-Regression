# Simulation Study Design {#sec-simulation-design}

```{python}
#| label: setup-simulation
#| include: false
import sys
from pathlib import Path

# Find project root by looking for pyproject.toml
def find_project_root():
    current = Path.cwd()
    for parent in [current] + list(current.parents):
        if (parent / "pyproject.toml").exists():
            return parent
    return current.parent.parent  # Fallback

project_root = find_project_root()
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "scripts"))
```

## Data Generating Process

We simulate learning curve data using the multiplicative power-law model:

$$Y = T_1 \cdot X_1^b \cdot X_2^c \cdot \exp(\epsilon)$$

where:

- $T_1 = 100$ (first unit cost)---**Note**: $T_1$ is estimated in all examples, not fixed
- $X_1$ = lot midpoint (cumulative units)
- $X_2$ = lot quantity (rate variable)
- $\epsilon \sim N(0, \sigma^2)$ with $\sigma^2 = \log(1 + \text{cv\_error}^2)$

The correlation between $\log(X_1)$ and $\log(X_2)$ is controlled to study multicollinearity effects. Correlated predictor datasets are generated using a custom function that creates realistic production ramp-up schedules where lot midpoint and quantity naturally correlate at the specified $\rho$ levels.

## Experimental Design

We employ a full factorial design with 5 factors:

| Factor | Levels | Values |
|--------|--------|--------|
| Sample size (n_lots) | 3 | 5, 10, 30 |
| Predictor correlation | 3 | 0.0, 0.5, 0.9 |
| CV error (noise) | 3 | 0.01, 0.10, 0.20 |
| Learning rate (b) | 3 | log(0.85), log(0.90), log(0.95) / log(2) |
| Rate effect (c) | 3 | log(0.80), log(0.85), log(0.90) / log(2) |

This yields $3^5 = 243$ unique scenarios. Each scenario is replicated 25 times with different random seeds, producing **6,075 total scenario-replications**.

## Models Compared

We evaluate 18 regression approaches:

### Traditional Methods (Log-Space)
- **OLS**: Ordinary Least Squares on log-transformed data (base case, unconstrained, unpenalized)
- **OLS_LearnOnly**: OLS with only the learning variable (confluence analysis remedy---drop rate variable)
- **Ridge**: Ridge regression with CV-selected $\lambda$, no constraints
- **Lasso**: Lasso regression with CV-selected $\lambda$, no constraints
- **BayesianRidge**: sklearn implementation using spherical Gaussian prior $p(w|\lambda) = N(w|0, \lambda^{-1}I)$ centered at zero. Iteratively maximizes marginal log-likelihood to optimize regularization parameters [@mackay1992bayesian; @tipping2001sparse]. *Note*: Uses uninformative priors without domain-specific constraints, serving as comparison to automatic regularization without domain knowledge.
- **PLS / PCA_Linear**: Both produce identical results for two predictors

### PCReg Variants (Unit-Space)
- **PCReg_ConstrainOnly**: Constraints only, no penalty ($\alpha = 0$)
- **PCReg_CV**: Constraints + CV-tuned elastic net with loose bounds ($\beta \in [-1, 0]$)
- **PCReg_AICc**: Constraints + AICc-selected penalty
- **PCReg_GCV**: Constraints + GCV-selected penalty
- **PCReg_CV_Tight**: With tight bounds near true values. This serves as an approximate upper bound on performance---analogous to an "oracle" benchmark that knows the true parameter region.
- **PCReg_CV_Wrong**: With deliberately incorrect constraints. Following @james2020pac methodology, "wrong" constraints are generated by adding random noise to the true bounds: $\text{constraint}_{\text{wrong}} = \text{constraint}_{\text{true}} + a \cdot U(-1,1)$, where $a$ controls the magnitude of constraint error. This tests robustness to constraint misspecification.

### PCReg MSE Loss Variants
- **PCReg_MSE**: MSE loss with constraints
- **PCReg_MSE_CV**: MSE loss with CV-tuned penalty

### PCReg Log-Space Variants
- **PCReg_LogMSE**: Log-transformed MSE loss
- **PCReg_LogMSE_CV**: Log-transformed with CV penalty

## Evaluation Metrics

All models are evaluated on 5 held-out test lots using:

1. **Test SSPE**: Sum of squared percentage errors (primary metric)
2. **Test MAPE**: Mean absolute percentage error
3. **Test MSE**: Mean squared error
4. **Coefficient bias**: $|\hat{\beta} - \beta_{\text{true}}|$ for $b$ (learning) and $c$ (rate)
5. **Coefficient variance**: Stability across replications
6. **Sign correctness (Domain consistency rate)**: Whether $\hat{b} \leq 0$ and $\hat{c} \leq 0$---percentage of replications with correct coefficient signs

## Computational Details

- **Parallelization**: joblib with all available CPU cores
- **Resume capability**: Results saved in batches to parquet files
- **Model hashing**: Automatic re-run if model definitions change
- **Total model fits**: $243 \times 25 \times 18 = 109,350$
