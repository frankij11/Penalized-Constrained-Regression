{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "# Appendix D: Key Quotes for Reference\n",
    "\n",
    "This appendix compiles key quotes from the literature that support the methodology and findings presented in this paper. These quotes may be useful for presentations, discussions, and future research.\n",
    "\n",
    "## D.1 On Multicollinearity Severity\n",
    "\n",
    "> “The statistical literature regards multicollinearity as one of the most vexing and intractable problems in all of regression analysis.” — Flynn and James ([2016](#ref-flynn2016multicollinearity))\n",
    "\n",
    "## D.2 On Coefficient Instability\n",
    "\n",
    "> “Generate unstable models where small changes in the data produce big changes in parameter estimates \\[bouncing β’s\\].” — Flynn and James ([2016](#ref-flynn2016multicollinearity))\n",
    "\n",
    "## D.3 On GDF Importance\n",
    "\n",
    "> “ZMPE users do not adjust the degrees of freedom (DF) to account for constraints included in the regression process. As a result, fit statistics for the ZMPE equations, e.g., the standard percent error (SPE) and generalized R² (GRSQ), can be incorrect and misleading.” — Hu ([2010](#ref-hu2010gdf))\n",
    "\n",
    "> “Using ZMPE CERs in cost uncertainty analysis may unduly tighten the S-curve because their SPEs underestimate the CER error distribution.” — Hu ([2010](#ref-hu2010gdf))\n",
    "\n",
    "## D.4 On Robustness to Imperfect Constraints\n",
    "\n",
    "> “The results suggest that PAC and relaxed PAC are surprisingly robust to random violations in the constraints. While both methods deteriorated slightly as \\[constraint error\\] increased, they were still both superior to their unconstrained counterparts for all values of \\[error\\] and all settings.” — James, Paulson, and Rusmevichientong ([2020](#ref-james2020pac))\n",
    "\n",
    "## D.5 On Constrained Lasso Flexibility\n",
    "\n",
    "> “The constrained lasso is a very flexible framework for imposing additional knowledge and structure onto the lasso coefficient estimates.” — Gaines, Kim, and Zhou ([2018](#ref-gaines2018constrained))\n",
    "\n",
    "## D.6 On pcLAD with Heavy-Tailed Errors\n",
    "\n",
    "> “pcLAD enjoys the Oracle property even with Cauchy-distributed errors… particularly effective for monotone curve fitting and non-negative constraints.” — Wu, Liang, and Yang ([2022](#ref-wu2022pclad))\n",
    "\n",
    "## D.7 On Ridge Regression Optimality\n",
    "\n",
    "The Theobald-Farebrother theorem establishes:\n",
    "\n",
    "> For any OLS problem, there exists a ridge parameter $\\lambda^* > 0$ such that the ridge estimator has strictly lower Mean Squared Error (MSE) than OLS. — Theobald ([1974](#ref-theobald1974)); Farebrother ([1976](#ref-farebrother1976))\n",
    "\n",
    "## D.8 On Bayesian Regularization\n",
    "\n",
    "> “Bayesian interpolation using a spherical Gaussian prior $p(w|\\lambda) = N(w|0, \\lambda^{-1}I)$ iteratively maximizes marginal log-likelihood to optimize regularization parameters.” — MacKay ([1992](#ref-mackay1992bayesian))\n",
    "\n",
    "## D.9 Key Definitions\n",
    "\n",
    "### Oracle Property\n",
    "\n",
    "An estimator has the **Oracle property** if it:\n",
    "\n",
    "1.  Correctly identifies which coefficients are truly zero (variable selection consistency)\n",
    "2.  Estimates non-zero coefficients as efficiently as if an “oracle” revealed the true model in advance\n",
    "\n",
    "This is the gold standard for high-dimensional estimators—the Lasso achieves it under certain conditions.\n",
    "\n",
    "### Cauchy-Distributed Errors\n",
    "\n",
    "The **Cauchy distribution** has extremely heavy tails—so heavy that its mean and variance are mathematically undefined (infinite). Outliers occur far more frequently than with normal distributions.\n",
    "\n",
    "-   Squared-error methods (OLS, Ridge, Lasso) perform poorly with Cauchy errors because outliers dominate the objective\n",
    "-   LAD methods minimize absolute errors, so outliers have linear rather than quadratic influence—making them robust to such extremes\n",
    "\n",
    "### BLUE (Best Linear Unbiased Estimator)\n",
    "\n",
    "By the **Gauss-Markov theorem**, OLS is the Best Linear Unbiased Estimator under classical assumptions:\n",
    "\n",
    "-   Linear in the parameters\n",
    "-   Unbiased: $E[\\hat{\\beta}] = \\beta$\n",
    "-   Minimum variance among all linear unbiased estimators\n",
    "\n",
    "Introducing penalties and/or constraints means the resulting estimator is **no longer BLUE**. This is an intentional tradeoff accepting bias for reduced variance.\n",
    "\n",
    "## D.10 Summary Table\n",
    "\n",
    "| Topic | Key Finding | Source |\n",
    "|-------------------|---------------------------------|---------------------|\n",
    "| Multicollinearity | “Most vexing problem in regression” | Flynn & James (2016) |\n",
    "| Constraint robustness | PAC outperforms unconstrained even with wrong constraints | James et al. (2020) |\n",
    "| GDF adjustment | Unadjusted fit stats are misleading | Hu (2010) |\n",
    "| Ridge optimality | Always exists λ\\* \\> 0 with lower MSE than OLS | Theobald-Farebrother |\n",
    "| pcLAD robustness | Oracle property even with Cauchy errors | Wu et al. (2022) |"
   ],
   "id": "e25e5ff6-d115-4bb0-9f72-9c0b9a9bfb07"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!-- -->"
   ],
   "id": "4f8bb0af-3aa3-4e2c-966b-740a5ca18662"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` markdown\n",
    "# Appendix D: Key Quotes for Reference {#sec-appendix-quotes .unnumbered}\n",
    "\n",
    "This appendix compiles key quotes from the literature that support the methodology and findings presented in this paper. These quotes may be useful for presentations, discussions, and future research.\n",
    "\n",
    "## D.1 On Multicollinearity Severity {.unnumbered}\n",
    "\n",
    "> \"The statistical literature regards multicollinearity as one of the most vexing and intractable problems in all of regression analysis.\"\n",
    "> --- @flynn2016multicollinearity\n",
    "\n",
    "## D.2 On Coefficient Instability {.unnumbered}\n",
    "\n",
    "> \"Generate unstable models where small changes in the data produce big changes in parameter estimates [bouncing β's].\"\n",
    "> --- @flynn2016multicollinearity\n",
    "\n",
    "## D.3 On GDF Importance {.unnumbered}\n",
    "\n",
    "> \"ZMPE users do not adjust the degrees of freedom (DF) to account for constraints included in the regression process. As a result, fit statistics for the ZMPE equations, e.g., the standard percent error (SPE) and generalized R² (GRSQ), can be incorrect and misleading.\"\n",
    "> --- @hu2010gdf\n",
    "\n",
    "> \"Using ZMPE CERs in cost uncertainty analysis may unduly tighten the S-curve because their SPEs underestimate the CER error distribution.\"\n",
    "> --- @hu2010gdf\n",
    "\n",
    "## D.4 On Robustness to Imperfect Constraints {.unnumbered}\n",
    "\n",
    "> \"The results suggest that PAC and relaxed PAC are surprisingly robust to random violations in the constraints. While both methods deteriorated slightly as [constraint error] increased, they were still both superior to their unconstrained counterparts for all values of [error] and all settings.\"\n",
    "> --- @james2020pac\n",
    "\n",
    "## D.5 On Constrained Lasso Flexibility {.unnumbered}\n",
    "\n",
    "> \"The constrained lasso is a very flexible framework for imposing additional knowledge and structure onto the lasso coefficient estimates.\"\n",
    "> --- @gaines2018constrained\n",
    "\n",
    "## D.6 On pcLAD with Heavy-Tailed Errors {.unnumbered}\n",
    "\n",
    "> \"pcLAD enjoys the Oracle property even with Cauchy-distributed errors... particularly effective for monotone curve fitting and non-negative constraints.\"\n",
    "> --- @wu2022pclad\n",
    "\n",
    "## D.7 On Ridge Regression Optimality {.unnumbered}\n",
    "\n",
    "The Theobald-Farebrother theorem establishes:\n",
    "\n",
    "> For any OLS problem, there exists a ridge parameter $\\lambda^* > 0$ such that the ridge estimator has strictly lower Mean Squared Error (MSE) than OLS.\n",
    "> --- @theobald1974; @farebrother1976\n",
    "\n",
    "## D.8 On Bayesian Regularization {.unnumbered}\n",
    "\n",
    "> \"Bayesian interpolation using a spherical Gaussian prior $p(w|\\lambda) = N(w|0, \\lambda^{-1}I)$ iteratively maximizes marginal log-likelihood to optimize regularization parameters.\"\n",
    "> --- @mackay1992bayesian\n",
    "\n",
    "## D.9 Key Definitions {.unnumbered}\n",
    "\n",
    "### Oracle Property\n",
    "\n",
    "An estimator has the **Oracle property** if it:\n",
    "\n",
    "1. Correctly identifies which coefficients are truly zero (variable selection consistency)\n",
    "2. Estimates non-zero coefficients as efficiently as if an \"oracle\" revealed the true model in advance\n",
    "\n",
    "This is the gold standard for high-dimensional estimators---the Lasso achieves it under certain conditions.\n",
    "\n",
    "### Cauchy-Distributed Errors\n",
    "\n",
    "The **Cauchy distribution** has extremely heavy tails---so heavy that its mean and variance are mathematically undefined (infinite). Outliers occur far more frequently than with normal distributions.\n",
    "\n",
    "- Squared-error methods (OLS, Ridge, Lasso) perform poorly with Cauchy errors because outliers dominate the objective\n",
    "- LAD methods minimize absolute errors, so outliers have linear rather than quadratic influence---making them robust to such extremes\n",
    "\n",
    "### BLUE (Best Linear Unbiased Estimator)\n",
    "\n",
    "By the **Gauss-Markov theorem**, OLS is the Best Linear Unbiased Estimator under classical assumptions:\n",
    "\n",
    "- Linear in the parameters\n",
    "- Unbiased: $E[\\hat{\\beta}] = \\beta$\n",
    "- Minimum variance among all linear unbiased estimators\n",
    "\n",
    "Introducing penalties and/or constraints means the resulting estimator is **no longer BLUE**. This is an intentional tradeoff accepting bias for reduced variance.\n",
    "\n",
    "## D.10 Summary Table {.unnumbered}\n",
    "\n",
    "| Topic | Key Finding | Source |\n",
    "|-------|-------------|--------|\n",
    "| Multicollinearity | \"Most vexing problem in regression\" | Flynn & James (2016) |\n",
    "| Constraint robustness | PAC outperforms unconstrained even with wrong constraints | James et al. (2020) |\n",
    "| GDF adjustment | Unadjusted fit stats are misleading | Hu (2010) |\n",
    "| Ridge optimality | Always exists λ* > 0 with lower MSE than OLS | Theobald-Farebrother |\n",
    "| pcLAD robustness | Oracle property even with Cauchy errors | Wu et al. (2022) |\n",
    "```\n",
    "\n",
    "Farebrother, R. W. 1976. “Further Results on the Mean Square Error of Ridge Regression.” *Journal of the Royal Statistical Society: Series B* 38 (3): 248–50. <https://doi.org/10.1111/j.2517-6161.1976.tb01588.x>.\n",
    "\n",
    "Flynn, Bernard, and Andrew James. 2016. “Multicollinearity in CER Development.” In *ICEAA Professional Development & Training Workshop*.\n",
    "\n",
    "Gaines, Brian R., Juhyun Kim, and Hua Zhou. 2018. “Algorithms for Fitting the Constrained Lasso.” *Journal of Computational and Graphical Statistics* 27 (4): 861–71. <https://doi.org/10.1080/10618600.2018.1473777>.\n",
    "\n",
    "Hu, Shu-Ping. 2010. “Generalized Degrees of Freedom for Constrained CERs.” PRT-191. Tecolote Research.\n",
    "\n",
    "James, Gareth M., Courtney Paulson, and Paat Rusmevichientong. 2020. “Penalized and Constrained Optimization: An Application to High-Dimensional Website Advertising.” *Journal of the American Statistical Association* 115 (529): 107–22. <https://doi.org/10.1080/01621459.2019.1609970>.\n",
    "\n",
    "MacKay, David J. C. 1992. “Bayesian Interpolation.” *Neural Computation* 4 (3): 415–47. <https://doi.org/10.1162/neco.1992.4.3.415>.\n",
    "\n",
    "Theobald, C. M. 1974. “Generalizations of Mean Square Error Applied to Ridge Regression.” *Journal of the Royal Statistical Society: Series B* 36 (1): 103–6. <https://doi.org/10.1111/j.2517-6161.1974.tb00990.x>.\n",
    "\n",
    "Wu, Xiaofei, Rongmei Liang, and Hu Yang. 2022. “Penalized and Constrained LAD Estimation in Fixed and High Dimension.” *Statistical Papers* 63: 53–95. <https://doi.org/10.1007/s00362-021-01229-0>."
   ],
   "id": "59b4160d-0c94-4122-92b0-bb7705c0af3f"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
