{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "# Appendix B: Algorithm Details\n",
    "\n",
    "This appendix provides detailed algorithm information for Penalized-Constrained Regression implementation.\n",
    "\n",
    "## B.1 High-Level Algorithm\n",
    "\n",
    "    Input:  Data (X, y), functional form f(X, β), penalty parameters (α, l1_ratio),\n",
    "            bounds/constraints, error function, optimizer choice\n",
    "\n",
    "    1. Scale (optional):\n",
    "       - Standardize X (mean=0, std=1) if scale=True\n",
    "       - Store scaling parameters for unscaling\n",
    "\n",
    "    2. Initialize:\n",
    "       - Compute OLS coefficients when possible (X'X invertible)\n",
    "       - Trim starting values to satisfy bounds\n",
    "       - Alternative: zeros or user-specified starting point\n",
    "\n",
    "    3. Optimize:\n",
    "       - Solve constrained penalized minimization via selected optimizer\n",
    "       - Monitor convergence and constraint satisfaction\n",
    "\n",
    "    4. Unscale:\n",
    "       - Transform coefficients back to original units\n",
    "       - β_original = β_scaled / σ\n",
    "\n",
    "    Output: Coefficient estimates β̂, fit statistics (GDF-adjusted),\n",
    "            active_constraints_ flag\n",
    "\n",
    "## B.2 Initialization Strategy\n",
    "\n",
    "The default initialization uses OLS coefficients when the problem is well-conditioned ($X'X$ invertible). These starting values are then trimmed to satisfy bounds—any coefficient outside the specified bounds is clamped to the nearest boundary.\n",
    "\n",
    "**Benefits**:\n",
    "\n",
    "-   Provides a warm start that respects domain constraints from the first iteration\n",
    "-   Improves convergence speed\n",
    "-   Reduces risk of local minima\n",
    "\n",
    "**Alternative initializations**:\n",
    "\n",
    "-   Zeros: Simple but may require more iterations\n",
    "-   User-specified: When domain expertise suggests better starting point\n",
    "-   Random: For testing multiple starting points in non-convex problems\n",
    "\n",
    "## B.3 Why Scaling Matters\n",
    "\n",
    "Scaling ensures that penalty terms (L1 and L2) are applied fairly across features. Without scaling:\n",
    "\n",
    "-   Features with larger magnitudes may dominate the optimization\n",
    "-   Leads to biased shrinkage and poor variable selection\n",
    "-   Degrades conditioning of the optimization problem\n",
    "\n",
    "**For linear models**: Standardization (zero mean, unit variance) is typically sufficient.\n",
    "\n",
    "**For nonlinear models** like $Y = AX^b$:\n",
    "\n",
    "-   Log-transformations (common in power-law modeling)\n",
    "-   Min-max scaling or domain-specific normalization\n",
    "-   Careful unscaling of both $X$ and $\\beta$ to preserve interpretability\n",
    "\n",
    "> **Warning**\n",
    ">\n",
    "> In nonlinear models such as $Y = AX^b$, scaling the predictor $X$ alters the curvature and interpretation of the exponent $b$. When applying penalized regression with L1 and L2 penalties to such models, scaling affects both the error function and the regularization terms. Scaling must be applied with caution, and unscaling procedures should be explicitly defined.\n",
    "\n",
    "## B.4 Optimization Methods\n",
    "\n",
    "### SLSQP (Sequential Least-Squares Quadratic Programming)\n",
    "\n",
    "**Current default**. Handles bounds and linear constraints efficiently.\n",
    "\n",
    "-   Uses sequential quadratic programming approach\n",
    "-   Requires gradient computation (finite differences if not provided)\n",
    "-   Efficient for small to medium problems\n",
    "\n",
    "### COBYLA (Constrained Optimization BY Linear Approximation)\n",
    "\n",
    "**Derivative-free**; recommended for ZMPE-type problems \\[@schiavoni2021assessing\\].\n",
    "\n",
    "-   Does not require gradient computation\n",
    "-   Handles nonlinear constraints\n",
    "-   More robust for non-smooth objectives\n",
    "-   Slower convergence than gradient-based methods\n",
    "\n",
    "### trust-constr\n",
    "\n",
    "Interior point method; suitable for larger-scale problems with many constraints.\n",
    "\n",
    "-   Modern implementation with trust-region approach\n",
    "-   Handles equality and inequality constraints\n",
    "-   Good numerical stability\n",
    "-   Requires more computational resources\n",
    "\n",
    "## B.5 Note on L1 Penalty Implementation\n",
    "\n",
    "> **Technical Note**\n",
    ">\n",
    "> The L1 penalty $\\|\\beta\\|_1$ is convex but not differentiable at zero. This non-smoothness introduces challenges for gradient-based optimization algorithms. Standard approaches include:\n",
    ">\n",
    "> 1.  **Coordinate descent**: Most popular approach; for constrained problems, use projected or constrained coordinate descent\n",
    "> 2.  **Subgradient methods**: Handle non-differentiability directly\n",
    "> 3.  **Proximal algorithms**: Efficient for structured sparsity problems\n",
    "> 4.  **CVXPY**: Convex optimization framework that handles non-smooth objectives\n",
    ">\n",
    "> This implementation uses general-purpose scipy optimizers. Despite theoretical limitations, they yield reasonable and stable solutions when problem size is moderate and constraints are well-posed.\n",
    ">\n",
    "> Future work will evaluate coordinate descent and PAC algorithm implementations.\n",
    "\n",
    "## B.6 Convergence Criteria\n",
    "\n",
    "The optimizer terminates when any of these conditions are met:\n",
    "\n",
    "1.  **Function tolerance**: Change in objective function $< \\text{ftol}$ (default: $10^{-9}$)\n",
    "2.  **Parameter tolerance**: Change in parameters $< \\text{xtol}$ (default: $10^{-9}$)\n",
    "3.  **Maximum iterations**: Exceeds `maxiter` (default: 1000)\n",
    "4.  **Constraint satisfaction**: All constraints satisfied within tolerance\n",
    "\n",
    "## B.7 Post-Optimization Checks\n",
    "\n",
    "After optimization completes:\n",
    "\n",
    "1.  **Constraint satisfaction**: Verify all bounds are satisfied within numerical tolerance\n",
    "2.  **Active constraints**: Identify which inequality constraints are binding (within tolerance of bound)\n",
    "3.  **Gradient check**: Verify KKT conditions are approximately satisfied\n",
    "4.  **Hessian conditioning**: Check for numerical issues in uncertainty estimation\n",
    "\n",
    "The `active_constraints_` attribute indicates which constraints are binding at the solution, informing degrees of freedom calculation."
   ],
   "id": "4afcd365-f748-46c3-9eef-25723002a978"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!-- -->"
   ],
   "id": "281cc469-20ea-4855-a79a-73cbfc9b88fa"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```` markdown\n",
    "# Appendix B: Algorithm Details {#sec-appendix-algorithm .unnumbered}\n",
    "\n",
    "This appendix provides detailed algorithm information for Penalized-Constrained Regression implementation.\n",
    "\n",
    "## B.1 High-Level Algorithm {.unnumbered}\n",
    "\n",
    "```\n",
    "Input:  Data (X, y), functional form f(X, β), penalty parameters (α, l1_ratio),\n",
    "        bounds/constraints, error function, optimizer choice\n",
    "\n",
    "1. Scale (optional):\n",
    "   - Standardize X (mean=0, std=1) if scale=True\n",
    "   - Store scaling parameters for unscaling\n",
    "\n",
    "2. Initialize:\n",
    "   - Compute OLS coefficients when possible (X'X invertible)\n",
    "   - Trim starting values to satisfy bounds\n",
    "   - Alternative: zeros or user-specified starting point\n",
    "\n",
    "3. Optimize:\n",
    "   - Solve constrained penalized minimization via selected optimizer\n",
    "   - Monitor convergence and constraint satisfaction\n",
    "\n",
    "4. Unscale:\n",
    "   - Transform coefficients back to original units\n",
    "   - β_original = β_scaled / σ\n",
    "\n",
    "Output: Coefficient estimates β̂, fit statistics (GDF-adjusted),\n",
    "        active_constraints_ flag\n",
    "```\n",
    "\n",
    "## B.2 Initialization Strategy {.unnumbered}\n",
    "\n",
    "The default initialization uses OLS coefficients when the problem is well-conditioned ($X'X$ invertible). These starting values are then trimmed to satisfy bounds---any coefficient outside the specified bounds is clamped to the nearest boundary.\n",
    "\n",
    "**Benefits**:\n",
    "\n",
    "- Provides a warm start that respects domain constraints from the first iteration\n",
    "- Improves convergence speed\n",
    "- Reduces risk of local minima\n",
    "\n",
    "**Alternative initializations**:\n",
    "\n",
    "- Zeros: Simple but may require more iterations\n",
    "- User-specified: When domain expertise suggests better starting point\n",
    "- Random: For testing multiple starting points in non-convex problems\n",
    "\n",
    "## B.3 Why Scaling Matters {.unnumbered}\n",
    "\n",
    "Scaling ensures that penalty terms (L1 and L2) are applied fairly across features. Without scaling:\n",
    "\n",
    "- Features with larger magnitudes may dominate the optimization\n",
    "- Leads to biased shrinkage and poor variable selection\n",
    "- Degrades conditioning of the optimization problem\n",
    "\n",
    "**For linear models**: Standardization (zero mean, unit variance) is typically sufficient.\n",
    "\n",
    "**For nonlinear models** like $Y = AX^b$:\n",
    "\n",
    "- Log-transformations (common in power-law modeling)\n",
    "- Min-max scaling or domain-specific normalization\n",
    "- Careful unscaling of both $X$ and $\\beta$ to preserve interpretability\n",
    "\n",
    "::: {.callout-warning}\n",
    "In nonlinear models such as $Y = AX^b$, scaling the predictor $X$ alters the curvature and interpretation of the exponent $b$. When applying penalized regression with L1 and L2 penalties to such models, scaling affects both the error function and the regularization terms. Scaling must be applied with caution, and unscaling procedures should be explicitly defined.\n",
    ":::\n",
    "\n",
    "## B.4 Optimization Methods {.unnumbered}\n",
    "\n",
    "### SLSQP (Sequential Least-Squares Quadratic Programming)\n",
    "\n",
    "**Current default**. Handles bounds and linear constraints efficiently.\n",
    "\n",
    "- Uses sequential quadratic programming approach\n",
    "- Requires gradient computation (finite differences if not provided)\n",
    "- Efficient for small to medium problems\n",
    "\n",
    "### COBYLA (Constrained Optimization BY Linear Approximation)\n",
    "\n",
    "**Derivative-free**; recommended for ZMPE-type problems [@schiavoni2021assessing].\n",
    "\n",
    "- Does not require gradient computation\n",
    "- Handles nonlinear constraints\n",
    "- More robust for non-smooth objectives\n",
    "- Slower convergence than gradient-based methods\n",
    "\n",
    "### trust-constr\n",
    "\n",
    "Interior point method; suitable for larger-scale problems with many constraints.\n",
    "\n",
    "- Modern implementation with trust-region approach\n",
    "- Handles equality and inequality constraints\n",
    "- Good numerical stability\n",
    "- Requires more computational resources\n",
    "\n",
    "## B.5 Note on L1 Penalty Implementation {.unnumbered}\n",
    "\n",
    "::: {.callout-note title=\"Technical Note\"}\n",
    "The L1 penalty $\\|\\beta\\|_1$ is convex but not differentiable at zero. This non-smoothness introduces challenges for gradient-based optimization algorithms. Standard approaches include:\n",
    "\n",
    "1. **Coordinate descent**: Most popular approach; for constrained problems, use projected or constrained coordinate descent\n",
    "2. **Subgradient methods**: Handle non-differentiability directly\n",
    "3. **Proximal algorithms**: Efficient for structured sparsity problems\n",
    "4. **CVXPY**: Convex optimization framework that handles non-smooth objectives\n",
    "\n",
    "This implementation uses general-purpose scipy optimizers. Despite theoretical limitations, they yield reasonable and stable solutions when problem size is moderate and constraints are well-posed.\n",
    "\n",
    "Future work will evaluate coordinate descent and PAC algorithm implementations.\n",
    ":::\n",
    "\n",
    "## B.6 Convergence Criteria {.unnumbered}\n",
    "\n",
    "The optimizer terminates when any of these conditions are met:\n",
    "\n",
    "1. **Function tolerance**: Change in objective function $< \\text{ftol}$ (default: $10^{-9}$)\n",
    "2. **Parameter tolerance**: Change in parameters $< \\text{xtol}$ (default: $10^{-9}$)\n",
    "3. **Maximum iterations**: Exceeds `maxiter` (default: 1000)\n",
    "4. **Constraint satisfaction**: All constraints satisfied within tolerance\n",
    "\n",
    "## B.7 Post-Optimization Checks {.unnumbered}\n",
    "\n",
    "After optimization completes:\n",
    "\n",
    "1. **Constraint satisfaction**: Verify all bounds are satisfied within numerical tolerance\n",
    "2. **Active constraints**: Identify which inequality constraints are binding (within tolerance of bound)\n",
    "3. **Gradient check**: Verify KKT conditions are approximately satisfied\n",
    "4. **Hessian conditioning**: Check for numerical issues in uncertainty estimation\n",
    "\n",
    "The `active_constraints_` attribute indicates which constraints are binding at the solution, informing degrees of freedom calculation.\n",
    "````"
   ],
   "id": "9ca6d4a2-e819-4e3b-b72c-d38b33c79597"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
