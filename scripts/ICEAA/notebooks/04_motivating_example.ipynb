{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivating Example: Why Penalized Constrained Regression?\n",
    "\n",
    "This notebook demonstrates the value of combining **penalization** with **economic constraints** in cost estimation.\n",
    "\n",
    "## The Problem\n",
    "\n",
    "Traditional regression methods like OLS can produce coefficients that are:\n",
    "- **Economically impossible** (e.g., learning curves > 100%) \n",
    "- **Unstable** when data is limited or noisy\n",
    "\n",
    "Caution / Note: ***If there is \"negative\" learning e.g >100% then you are missing an indepedent variable like capacity constraints or economic order quantity violations, catostrophic events. Always check for those independent variables before applying constraints***\n",
    "What is the checklist?\n",
    "## Methods Compared\n",
    "\n",
    "| Method | Penalization | Constraints | Description |\n",
    "|--------|-------------|-------------|-------------|\n",
    "| **OLS** | None | None | Baseline - unconstrained least squares |\n",
    "| **RidgeCV** | L2 (Ridge) | None | Penalization only - helps with overfitting |\n",
    "| **PCReg (Constraints Only)** | None | Yes | Constraints only - ensures economic validity |\n",
    "| **PCReg-GCV** | L1/L2 (Elastic Net) | Yes | Both - the best of both worlds |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "import penalized_constrained as pcreg\n",
    "\n",
    "# For presentation plots\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We use simulated manufacturing lot data with:\n",
    "- **lot_midpoint**: Cumulative unit midpoint of the lot\n",
    "- **lot_quantity**: Number of units in the lot\n",
    "- **observed_cost**: Per-unit cost (with realistic noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 5\n",
      "Test samples: 21\n",
      "\n",
      "True parameters:\n",
      "  T1 = 100\n",
      "  b (learning exponent) = -0.1520\n",
      "  c (rate exponent) = -0.1520\n",
      "  Learning Rate = 0.9000 (90.0%)\n",
      "  Rate Effect = 0.9000 (90.0%)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "DIR = Path('output_v2')\n",
    "df = pd.read_csv(DIR / 'motivational_example_data.csv')\n",
    "\n",
    "# Split into train and test\n",
    "df_train = df.query(\"lot_type == 'train'\").copy()\n",
    "df_test = df.query(\"lot_type == 'test'\").copy()\n",
    "\n",
    "X_train = df_train[['lot_midpoint', 'lot_quantity']]\n",
    "y_train = df_train['observed_cost']\n",
    "\n",
    "X_test = df_test[['lot_midpoint', 'lot_quantity']]\n",
    "y_test = df_test['observed_cost']\n",
    "\n",
    "print(f\"Training samples: {len(df_train)}\")\n",
    "print(f\"Test samples: {len(df_test)}\")\n",
    "print(f\"\\nTrue parameters:\")\n",
    "print(f\"  T1 = {df['T1_true'].iloc[0]}\")\n",
    "print(f\"  b (learning exponent) = {df['b_true'].iloc[0]:.4f}\")\n",
    "print(f\"  c (rate exponent) = {df['c_true'].iloc[0]:.4f}\")\n",
    "print(f\"  Learning Rate = {2**df['b_true'].iloc[0]:.4f} ({2**df['b_true'].iloc[0]*100:.1f}%)\")\n",
    "print(f\"  Rate Effect = {2**df['c_true'].iloc[0]:.4f} ({2**df['c_true'].iloc[0]*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario_id</th>\n",
       "      <th>n_lots</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>rate_effect</th>\n",
       "      <th>cv_error</th>\n",
       "      <th>replication</th>\n",
       "      <th>seed</th>\n",
       "      <th>program_id</th>\n",
       "      <th>total_program_lots</th>\n",
       "      <th>actual_correlation</th>\n",
       "      <th>...</th>\n",
       "      <th>lot_number</th>\n",
       "      <th>first_unit</th>\n",
       "      <th>last_unit</th>\n",
       "      <th>lot_quantity</th>\n",
       "      <th>lot_midpoint</th>\n",
       "      <th>log_midpoint</th>\n",
       "      <th>log_quantity</th>\n",
       "      <th>true_cost</th>\n",
       "      <th>observed_cost</th>\n",
       "      <th>log_observed_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2620.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1987917204</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>0.980223</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2.125088</td>\n",
       "      <td>0.753813</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>72.230851</td>\n",
       "      <td>68.380513</td>\n",
       "      <td>4.225088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2620.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1987917204</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>0.980223</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>7.691835</td>\n",
       "      <td>2.040159</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>54.558596</td>\n",
       "      <td>60.394482</td>\n",
       "      <td>4.100898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2620.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1987917204</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>0.980223</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>15.298691</td>\n",
       "      <td>2.727767</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>48.156666</td>\n",
       "      <td>46.881073</td>\n",
       "      <td>3.847614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2620.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1987917204</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>0.980223</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>25.683407</td>\n",
       "      <td>3.245845</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>41.343344</td>\n",
       "      <td>32.595040</td>\n",
       "      <td>3.484160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2620.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1987917204</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>0.980223</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>45</td>\n",
       "      <td>13</td>\n",
       "      <td>38.790661</td>\n",
       "      <td>3.658180</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>38.831639</td>\n",
       "      <td>33.773370</td>\n",
       "      <td>3.519673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   scenario_id  n_lots  learning_rate  rate_effect  cv_error  replication  \\\n",
       "0       2620.0     5.0            0.9          0.9       0.2         90.0   \n",
       "1       2620.0     5.0            0.9          0.9       0.2         90.0   \n",
       "2       2620.0     5.0            0.9          0.9       0.2         90.0   \n",
       "3       2620.0     5.0            0.9          0.9       0.2         90.0   \n",
       "4       2620.0     5.0            0.9          0.9       0.2         90.0   \n",
       "\n",
       "         seed  program_id  total_program_lots  actual_correlation  ...  \\\n",
       "0  1987917204          10                  26            0.980223  ...   \n",
       "1  1987917204          10                  26            0.980223  ...   \n",
       "2  1987917204          10                  26            0.980223  ...   \n",
       "3  1987917204          10                  26            0.980223  ...   \n",
       "4  1987917204          10                  26            0.980223  ...   \n",
       "\n",
       "   lot_number  first_unit  last_unit lot_quantity  lot_midpoint  log_midpoint  \\\n",
       "0           1           1          4            4      2.125088      0.753813   \n",
       "1           2           5         11            7      7.691835      2.040159   \n",
       "2           3          12         19            8     15.298691      2.727767   \n",
       "3           4          20         32           13     25.683407      3.245845   \n",
       "4           5          33         45           13     38.790661      3.658180   \n",
       "\n",
       "   log_quantity  true_cost  observed_cost  log_observed_cost  \n",
       "0      1.386294  72.230851      68.380513           4.225088  \n",
       "1      1.945910  54.558596      60.394482           4.100898  \n",
       "2      2.079442  48.156666      46.881073           3.847614  \n",
       "3      2.564949  41.343344      32.595040           3.484160  \n",
       "4      2.564949  38.831639      33.773370           3.519673  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cost Model\n",
    "\n",
    "We're fitting a **learning curve model** commonly used in manufacturing cost estimation:\n",
    "\n",
    "$$Y = T_1 \\cdot X_1^b \\cdot X_2^c$$\n",
    "\n",
    "Where:\n",
    "- $Y$ = per-unit cost\n",
    "- $T_1$ = theoretical first unit cost\n",
    "- $X_1$ = cumulative lot midpoint\n",
    "- $X_2$ = lot quantity\n",
    "- $b$ = learning exponent (negative = cost decreases with experience)\n",
    "- $c$ = rate effect exponent (negative = cost decreases with larger lots)\n",
    "\n",
    "### Economic Constraints\n",
    "\n",
    "The **learning rate** $= 2^b$ and **rate effect** $= 2^c$ must be $\\leq 1$ (i.e., $b, c \\leq 0$).\n",
    "\n",
    "A learning rate > 100% would mean costs *increase* with experience - economically nonsensical!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Method 1: Ordinary Least Squares (OLS)\n",
    "\n",
    "Log-transform to linearize: $\\ln(Y) = \\ln(T_1) + b \\cdot \\ln(X_1) + c \\cdot \\ln(X_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS Results:\n",
      "  T1 = 195.00\n",
      "  b = 0.0273\n",
      "  c = -0.7140\n",
      "  Learning Rate = 1.0191 (101.9%)\n",
      "  Rate Effect = 0.6096 (61.0%)\n",
      "\n",
      "  Train R² = 0.8851\n",
      "  Test R² = -0.1843\n",
      "\n",
      "  ⚠️  INVALID: Learning Rate or Rate Effect > 100%!\n"
     ]
    }
   ],
   "source": [
    "# OLS with log-log transformation\n",
    "ols = TransformedTargetRegressor(\n",
    "    regressor=Pipeline([\n",
    "        ('log', FunctionTransformer(np.log)),\n",
    "        ('reg', LinearRegression()),\n",
    "    ]),\n",
    "    func=np.log,\n",
    "    inverse_func=np.exp,\n",
    ")\n",
    "\n",
    "ols.fit(X_train, y_train)\n",
    "\n",
    "# Extract coefficients\n",
    "ols_b, ols_c = ols.regressor_.named_steps['reg'].coef_\n",
    "ols_t1 = np.exp(ols.regressor_.named_steps['reg'].intercept_)\n",
    "ols_lr = 2 ** ols_b  # Learning rate\n",
    "ols_re = 2 ** ols_c  # Rate effect\n",
    "\n",
    "print(\"OLS Results:\")\n",
    "print(f\"  T1 = {ols_t1:.2f}\")\n",
    "print(f\"  b = {ols_b:.4f}\")\n",
    "print(f\"  c = {ols_c:.4f}\")\n",
    "print(f\"  Learning Rate = {ols_lr:.4f} ({ols_lr*100:.1f}%)\")\n",
    "print(f\"  Rate Effect = {ols_re:.4f} ({ols_re*100:.1f}%)\")\n",
    "print(f\"\\n  Train R² = {ols.score(X_train, y_train):.4f}\")\n",
    "print(f\"  Test R² = {ols.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# Check validity\n",
    "ols_valid = (ols_lr <= 1) and (ols_re <= 1)\n",
    "if not ols_valid:\n",
    "    print(f\"\\n  ⚠️  INVALID: Learning Rate or Rate Effect > 100%!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Method 2: Ridge Regression with Cross-Validation (RidgeCV)\n",
    "\n",
    "Adds L2 penalization to reduce overfitting, but still no economic constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Ridge with CV for alpha selection\nridge = TransformedTargetRegressor(\n    regressor=Pipeline([\n        ('log', FunctionTransformer(np.log)),\n        ('reg', RidgeCV(\n            alphas=np.logspace(-5, 5, 11),\n            fit_intercept=True,\n            store_cv_results=True  # Fixed: was store_cv_values\n        ))\n    ]),\n    func=np.log,\n    inverse_func=np.exp,\n)\n\nridge.fit(X_train, y_train)\n\n# Extract coefficients\nridge_b, ridge_c = ridge.regressor_.named_steps['reg'].coef_\nridge_t1 = np.exp(ridge.regressor_.named_steps['reg'].intercept_)\nridge_lr = 2 ** ridge_b\nridge_re = 2 ** ridge_c\nridge_alpha = ridge.regressor_.named_steps['reg'].alpha_\n\nprint(\"RidgeCV Results:\")\nprint(f\"  Selected alpha = {ridge_alpha:.6f}\")\nprint(f\"  T1 = {ridge_t1:.2f}\")\nprint(f\"  b = {ridge_b:.4f}\")\nprint(f\"  c = {ridge_c:.4f}\")\nprint(f\"  Learning Rate = {ridge_lr:.4f} ({ridge_lr*100:.1f}%)\")\nprint(f\"  Rate Effect = {ridge_re:.4f} ({ridge_re*100:.1f}%)\")\nprint(f\"\\n  Train R² = {ridge.score(X_train, y_train):.4f}\")\nprint(f\"  Test R² = {ridge.score(X_test, y_test):.4f}\")\n\n# Check validity\nridge_valid = (ridge_lr <= 1) and (ridge_re <= 1)\nif not ridge_valid:\n    print(f\"\\n  ⚠️  INVALID: Learning Rate or Rate Effect > 100%!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Method 3: PCReg - Constraints Only (No Penalization)\n",
    "\n",
    "Uses the custom prediction function with bounded optimization to enforce:\n",
    "- $b \\in [-0.5, 0]$ → Learning Rate between 71% and 100%\n",
    "- $c \\in [-0.5, 0]$ → Rate Effect between 71% and 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_space_prediction_fn(X: np.ndarray, params: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Learning curve prediction function for unit-space fitting.\n",
    "    \n",
    "    Y = T1 * X1^b * X2^c\n",
    "    \"\"\"\n",
    "    T1, b, c = params[0], params[1], params[2]\n",
    "    return T1 * (X[:, 0] ** b) * (X[:, 1] ** c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCReg (Constraints Only) Results:\n",
      "  T1 = 152.33\n",
      "  b = -0.0590\n",
      "  c = -0.5000\n",
      "  Learning Rate = 0.9599 (96.0%)\n",
      "  Rate Effect = 0.7071 (70.7%)\n",
      "\n",
      "  Train R² = 0.8873\n",
      "  Test R² = -0.2138\n",
      "\n",
      "  ✓ Constraints satisfied: True\n"
     ]
    }
   ],
   "source": [
    "# PCReg with constraints only (alpha=0 means no penalization)\n",
    "pc_constrained = pcreg.PCRegression(\n",
    "    coef_names=['T1', 'b', 'c'],\n",
    "    bounds={'T1': (0, None), 'b': (-0.5, 0), 'c': (-0.5, 0)},\n",
    "    prediction_fn=unit_space_prediction_fn,\n",
    "    fit_intercept=False,\n",
    "    x0=[1, 0, 0],\n",
    "    loss='sspe',\n",
    "    alpha=0,  # No penalization\n",
    "    safe_mode=True\n",
    ")\n",
    "\n",
    "pc_constrained.fit(X_train, y_train)\n",
    "\n",
    "# Extract coefficients\n",
    "pc_con_t1 = pc_constrained.coef_[0]\n",
    "pc_con_b = pc_constrained.coef_[1]\n",
    "pc_con_c = pc_constrained.coef_[2]\n",
    "pc_con_lr = 2 ** pc_con_b\n",
    "pc_con_re = 2 ** pc_con_c\n",
    "\n",
    "print(\"PCReg (Constraints Only) Results:\")\n",
    "print(f\"  T1 = {pc_con_t1:.2f}\")\n",
    "print(f\"  b = {pc_con_b:.4f}\")\n",
    "print(f\"  c = {pc_con_c:.4f}\")\n",
    "print(f\"  Learning Rate = {pc_con_lr:.4f} ({pc_con_lr*100:.1f}%)\")\n",
    "print(f\"  Rate Effect = {pc_con_re:.4f} ({pc_con_re*100:.1f}%)\")\n",
    "print(f\"\\n  Train R² = {pc_constrained.score(X_train, y_train):.4f}\")\n",
    "print(f\"  Test R² = {pc_constrained.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# Check validity (should always be valid due to constraints)\n",
    "pc_con_valid = (pc_con_lr <= 1) and (pc_con_re <= 1)\n",
    "print(f\"\\n  ✓ Constraints satisfied: {pc_con_valid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Method 4: PCReg-GCV (Penalization + Constraints)\n",
    "\n",
    "Combines:\n",
    "- **Economic constraints** (bounds on parameters)\n",
    "- **Elastic Net penalization** (L1 + L2) with GCV-based hyperparameter selection\n",
    "- **Penalty exclusion** for T1 (we don't want to shrink the intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCReg-GCV Results:\n",
      "  Selected alpha = 1.000000\n",
      "  Selected l1_ratio = 0.00\n",
      "  T1 = 99.23\n",
      "  b = -0.1984\n",
      "  c = -0.1345\n",
      "  Learning Rate = 0.8715 (87.2%)\n",
      "  Rate Effect = 0.9110 (91.1%)\n",
      "\n",
      "  Train R² = 0.8767\n",
      "  Test R² = -0.2373\n",
      "\n",
      "  ✓ Constraints satisfied: True\n"
     ]
    }
   ],
   "source": [
    "# PCReg with GCV for hyperparameter selection\n",
    "pc_gcv = pcreg.PenalizedConstrainedCV(\n",
    "    coef_names=['T1', 'b', 'c'],\n",
    "    bounds={'T1': (0, None), 'b': (-0.5, 0), 'c': (-0.5, 0)},\n",
    "    prediction_fn=unit_space_prediction_fn,\n",
    "    fit_intercept=False,\n",
    "    x0=[1, 0, 0],\n",
    "    loss='sspe',\n",
    "    alphas=np.logspace(-5, 0, 10), # Penalty\n",
    "    l1_ratios=[0.0, 0.5, 1.0], # \n",
    "    cv=3,\n",
    "    selection='gcv',\n",
    "    penalty_exclude=['T1'],  # Don't penalize the intercept\n",
    "    n_jobs=1,\n",
    "    verbose=0,\n",
    "    safe_mode=True\n",
    ")\n",
    "\n",
    "pc_gcv.fit(X_train, y_train)\n",
    "\n",
    "# Extract coefficients\n",
    "pc_gcv_t1 = pc_gcv.coef_[0]\n",
    "pc_gcv_b = pc_gcv.coef_[1]\n",
    "pc_gcv_c = pc_gcv.coef_[2]\n",
    "pc_gcv_lr = 2 ** pc_gcv_b\n",
    "pc_gcv_re = 2 ** pc_gcv_c\n",
    "\n",
    "print(\"PCReg-GCV Results:\")\n",
    "print(f\"  Selected alpha = {pc_gcv.alpha_:.6f}\")\n",
    "print(f\"  Selected l1_ratio = {pc_gcv.l1_ratio_:.2f}\")\n",
    "print(f\"  T1 = {pc_gcv_t1:.2f}\")\n",
    "print(f\"  b = {pc_gcv_b:.4f}\")\n",
    "print(f\"  c = {pc_gcv_c:.4f}\")\n",
    "print(f\"  Learning Rate = {pc_gcv_lr:.4f} ({pc_gcv_lr*100:.1f}%)\")\n",
    "print(f\"  Rate Effect = {pc_gcv_re:.4f} ({pc_gcv_re*100:.1f}%)\")\n",
    "print(f\"\\n  Train R² = {pc_gcv.score(X_train, y_train):.4f}\")\n",
    "print(f\"  Test R² = {pc_gcv.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# Check validity (should always be valid due to constraints)\n",
    "pc_gcv_valid = (pc_gcv_lr <= 1) and (pc_gcv_re <= 1)\n",
    "print(f\"\\n  ✓ Constraints satisfied: {pc_gcv_valid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ridge_t1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      6\u001b[39m true_re = \u001b[32m2\u001b[39m ** true_c\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Create comparison dataframe\u001b[39;00m\n\u001b[32m      9\u001b[39m results = pd.DataFrame({\n\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mMethod\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33mTrue\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mOLS\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mRidgeCV\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPCReg (Const.)\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPCReg-GCV\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mT1\u001b[39m\u001b[33m'\u001b[39m: [true_t1, ols_t1, \u001b[43mridge_t1\u001b[49m, pc_con_t1, pc_gcv_t1],\n\u001b[32m     12\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m: [true_b, ols_b, ridge_b, pc_con_b, pc_gcv_b],\n\u001b[32m     13\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mc\u001b[39m\u001b[33m'\u001b[39m: [true_c, ols_c, ridge_c, pc_con_c, pc_gcv_c],\n\u001b[32m     14\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mLearning Rate\u001b[39m\u001b[33m'\u001b[39m: [true_lr, ols_lr, ridge_lr, pc_con_lr, pc_gcv_lr],\n\u001b[32m     15\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mRate Effect\u001b[39m\u001b[33m'\u001b[39m: [true_re, ols_re, ridge_re, pc_con_re, pc_gcv_re],\n\u001b[32m     16\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mTrain R²\u001b[39m\u001b[33m'\u001b[39m: [np.nan, ols.score(X_train, y_train), ridge.score(X_train, y_train), \n\u001b[32m     17\u001b[39m                  pc_constrained.score(X_train, y_train), pc_gcv.score(X_train, y_train)],\n\u001b[32m     18\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mTest R²\u001b[39m\u001b[33m'\u001b[39m: [np.nan, ols.score(X_test, y_test), ridge.score(X_test, y_test),\n\u001b[32m     19\u001b[39m                 pc_constrained.score(X_test, y_test), pc_gcv.score(X_test, y_test)],\n\u001b[32m     20\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mValid\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33mYes\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mYes\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ols_valid \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mNO\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mYes\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ridge_valid \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mNO\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     21\u001b[39m               \u001b[33m'\u001b[39m\u001b[33mYes\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pc_con_valid \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mNO\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mYes\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pc_gcv_valid \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mNO\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     22\u001b[39m })\n\u001b[32m     24\u001b[39m results.set_index(\u001b[33m'\u001b[39m\u001b[33mMethod\u001b[39m\u001b[33m'\u001b[39m, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     25\u001b[39m results.round(\u001b[32m4\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'ridge_t1' is not defined"
     ]
    }
   ],
   "source": [
    "# True values\n",
    "true_t1 = df['T1_true'].iloc[0]\n",
    "true_b = df['b_true'].iloc[0]\n",
    "true_c = df['c_true'].iloc[0]\n",
    "true_lr = 2 ** true_b\n",
    "true_re = 2 ** true_c\n",
    "\n",
    "# Create comparison dataframe\n",
    "results = pd.DataFrame({\n",
    "    'Method': ['True', 'OLS', 'RidgeCV', 'PCReg (Const.)', 'PCReg-GCV'],\n",
    "    'T1': [true_t1, ols_t1, ridge_t1, pc_con_t1, pc_gcv_t1],\n",
    "    'b': [true_b, ols_b, ridge_b, pc_con_b, pc_gcv_b],\n",
    "    'c': [true_c, ols_c, ridge_c, pc_con_c, pc_gcv_c],\n",
    "    'Learning Rate': [true_lr, ols_lr, ridge_lr, pc_con_lr, pc_gcv_lr],\n",
    "    'Rate Effect': [true_re, ols_re, ridge_re, pc_con_re, pc_gcv_re],\n",
    "    'Train R²': [np.nan, ols.score(X_train, y_train), ridge.score(X_train, y_train), \n",
    "                 pc_constrained.score(X_train, y_train), pc_gcv.score(X_train, y_train)],\n",
    "    'Test R²': [np.nan, ols.score(X_test, y_test), ridge.score(X_test, y_test),\n",
    "                pc_constrained.score(X_test, y_test), pc_gcv.score(X_test, y_test)],\n",
    "    'Valid': ['Yes', 'Yes' if ols_valid else 'NO', 'Yes' if ridge_valid else 'NO', \n",
    "              'Yes' if pc_con_valid else 'NO', 'Yes' if pc_gcv_valid else 'NO']\n",
    "})\n",
    "\n",
    "results.set_index('Method', inplace=True)\n",
    "results.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learning rates\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "methods = ['True', 'OLS', 'RidgeCV', 'PCReg\\n(Const.)', 'PCReg-GCV']\n",
    "learning_rates = [true_lr, ols_lr, ridge_lr, pc_con_lr, pc_gcv_lr]\n",
    "rate_effects = [true_re, ols_re, ridge_re, pc_con_re, pc_gcv_re]\n",
    "colors = ['green', 'red' if ols_lr > 1 else 'blue', 'red' if ridge_lr > 1 else 'blue', 'blue', 'blue']\n",
    "\n",
    "# Learning Rate\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.bar(methods, learning_rates, color=colors, edgecolor='black', alpha=0.7)\n",
    "ax1.axhline(y=1.0, color='red', linestyle='--', linewidth=2, label='Max Valid (100%)')\n",
    "ax1.set_ylabel('Learning Rate', fontsize=12)\n",
    "ax1.set_title('Learning Rate by Method', fontsize=14)\n",
    "ax1.set_ylim(0, max(learning_rates) * 1.15)\n",
    "ax1.legend()\n",
    "for bar, lr in zip(bars1, learning_rates):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "             f'{lr*100:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Rate Effect\n",
    "colors2 = ['green', 'red' if ols_re > 1 else 'blue', 'red' if ridge_re > 1 else 'blue', 'blue', 'blue']\n",
    "ax2 = axes[1]\n",
    "bars2 = ax2.bar(methods, rate_effects, color=colors2, edgecolor='black', alpha=0.7)\n",
    "ax2.axhline(y=1.0, color='red', linestyle='--', linewidth=2, label='Max Valid (100%)')\n",
    "ax2.set_ylabel('Rate Effect', fontsize=12)\n",
    "ax2.set_title('Rate Effect by Method', fontsize=14)\n",
    "ax2.set_ylim(0, max(rate_effects) * 1.15)\n",
    "ax2.legend()\n",
    "for bar, re in zip(bars2, rate_effects):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "             f'{re*100:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output_v2/motivating_example_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Diagnostics for PCReg-GCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate diagnostics report\n",
    "diag = pcreg.ModelDiagnostics(pc_gcv, X_train, y_train)\n",
    "report = diag.summary(bootstrap=True, n_bootstrap=100, include_alpha_trace=True)\n",
    "report.plot_diagnostics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export HTML report\n",
    "report.to_html(\"output_v2/motivating_example_diagnostics.html\")\n",
    "print(\"Diagnostics report saved to output_v2/motivating_example_diagnostics.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Takeaways\n",
    "\n",
    "1. **OLS can fail**: With limited/noisy data, OLS may produce economically impossible coefficients\n",
    "\n",
    "2. **Penalization alone isn't enough**: RidgeCV helps with overfitting but doesn't guarantee valid coefficients\n",
    "\n",
    "3. **Constraints ensure validity**: PCReg with bounds guarantees economically sensible results\n",
    "\n",
    "4. **Combined approach is best**: PCReg-GCV provides:\n",
    "   - Economic validity through constraints\n",
    "   - Better generalization through penalization\n",
    "   - Automatic hyperparameter tuning via GCV\n",
    "   - Interpretable, defensible estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*70)\n",
    "print(\"MOTIVATING EXAMPLE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTrue Learning Rate: {true_lr*100:.1f}%\")\n",
    "print(f\"True Rate Effect: {true_re*100:.1f}%\")\n",
    "print()\n",
    "print(f\"OLS Learning Rate: {ols_lr*100:.1f}% {'⚠️ INVALID' if ols_lr > 1 else '✓'}\")\n",
    "print(f\"OLS Rate Effect: {ols_re*100:.1f}% {'⚠️ INVALID' if ols_re > 1 else '✓'}\")\n",
    "print()\n",
    "print(f\"RidgeCV Learning Rate: {ridge_lr*100:.1f}% {'⚠️ INVALID' if ridge_lr > 1 else '✓'}\")\n",
    "print(f\"RidgeCV Rate Effect: {ridge_re*100:.1f}% {'⚠️ INVALID' if ridge_re > 1 else '✓'}\")\n",
    "print()\n",
    "print(f\"PCReg (Constraints) Learning Rate: {pc_con_lr*100:.1f}% ✓\")\n",
    "print(f\"PCReg (Constraints) Rate Effect: {pc_con_re*100:.1f}% ✓\")\n",
    "print()\n",
    "print(f\"PCReg-GCV Learning Rate: {pc_gcv_lr*100:.1f}% ✓\")\n",
    "print(f\"PCReg-GCV Rate Effect: {pc_gcv_re*100:.1f}% ✓\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}