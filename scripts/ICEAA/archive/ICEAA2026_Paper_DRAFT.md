Penalized-Constrained Regression:

Combining Regularization and Domain Constraints for Cost Estimation

*ICEAA 2026 Professional Development & Training Workshop*

Kevin Joy, Max Watstein

Herren Associates

***Small Data, Big Problems: Can Constraints and Penalties Save
Regression?***

########## Abstract

Small datasets with intercorrelation pose serious challenges to the
stability of coefficients generated by Ordinary Least Squares (OLS)
regression. A motivating example in cost estimating is Cost Improvement
Curve with Rate Effect analysis, where datasets are typically small, the
lot mid-point (Learning) is correlated to lot size (Rate) as production
ramps up, and slopes are expected to be less than or equal to 100%.
Lasso and Ridge regularization methods address multicollinearity by
penalizing coefficients. Separately, constrained optimization methods
can impose explicit restrictions on coefficient values when prior
knowledge about their behavior is known---such as bounding slopes within
a known range. This paper investigates the combined effects of penalized
regularization methods (Lasso and Ridge) and constrained optimization
methods. We explore how to assess model stability and goodness-of-fit
using likelihood-free diagnostic techniques suited to optimization-based
regressions, such as cross-validation for generalization error.

########## 1. Introduction and Motivation

#################### 1.1 The Problem: Small, Correlated Datasets in Cost Estimation

Developing hundreds of CERs for small datasets ranging from 5-30 data
points, a recurring pattern emerged: strong fit statistics (R², CV) but
nonsensical coefficients (wrong signs, implausible magnitudes and poor
p-values for critical predictors). As Department of War analysts, this
story may feel too close to home.

Unfortunately, multicollinear datasets are a frequent presence in our
analysis causing models to misbehave. The consequences of
multicollinearity in small samples are well-documented[^1]:

- **Unstable Coefficient Estimates** ---coefficient estimates that swing
  wildly with small changes in the data

- **Wrong coefficient signs** --- estimates flip positive/negative
  contrary to domain knowledge

- **Unreliable hypothesis testing** --- high F-statistic but
  statistically insignificant individual t-statistics

- **Hidden extrapolation** --- predictions fall outside the convex hull
  of training data despite appearing within variable ranges

- **Inflated variance** --- coefficient variance increases by factor
  1/(1−R²) where R² is correlation between predictors

############################## Motivating Example: Cost Improvement Curve with Rate Effect

The motivating example for this research in cost estimation is the
learning curve with rate effect, where the model takes the form:

Y = T₁ × (LotMidpoint)ᵇ × (LotQuantity)ᶜ × ε.

In this specification, lot midpoint (Learning variable) is inherently
correlated with lot size (Rate variable) as production ramps up from
Prototypes to LRIP to FRP. Correlations of ρ = (-.3, .88) have been
found in the Selected Acquisition Report comparing lot size to
cumulative quantities [(show python script)]{.mark}. Domain knowledge
establishes that learning and rate slopes should be ≤ 100% (i.e., costs
should not increase with cumulative production).

#################### 1.2 Diagnosing Multicollinearity is Well Established

There are various approaches to diagnose multicollinearity:[^2]

############################## Standard Diagnostic Measures

- **Simple correlation coefficients:** rₓᵢ,ₓⱼ \> 0.8 suggests potential
  multicollinearity

- **Mismatch between F and t statistics:** High overall F but
  individually insignificant t-statistics

- **R² among predictors:** Harmful if R² for Xᵢ \| all other X\'s \>
  0.90

- **Variance Inflation Factors (VIF):** VIF = 1/(1−R²ₓᵢ\|ₒₜₕₑᵣₓ). VIF \>
  10 indicates likely harmful multicollinearity (CEBoK threshold)

- **Condition number:** κ = λₘₐₓ/λₘᵢₙ from eigenvalue decomposition of
  X\'X. Collinearity harmful if κ \> 30

[Unfortunately, there's no unanimous view of a single best measure of
when "severity" precisely kicks in. A good practice therefore seems to
use several highly-regarded heuristics coupled with the formal
statistical tests to obtain better insights into the sample at
hand."]{.mark} [DIAGNOSTIC CONCLUSION Remedial action, therefore, seems
in order to improve the precision of leastsquares estimates.]{.mark}

#################### 1.3 Traditional Remedies

Traditional approaches are summarized in the table below. Each of these
remedies comes with trade-offs and ultimately may not meet all the
analyst\'s expectations.

  ----------------------------------------------------------------------------
  **Remedy**          **Description**            **Limitation**
  ------------------- -------------------------- -----------------------------
  Collect more data   Increases sample size,     Often infeasible in defense
                      reduces variance           cost analysis

  Drop variables      Remove collinear           May lose domain-required
                      predictors via confluence  variables
                      analysis                   

  Centering/Scaling   Reduces structural         Only helps
                      multicollinearity          polynomial/interaction terms

  Ridge Regression    L2 penalty shrinks         No sparsity; no domain
                      coefficients               constraints

  Lasso Regression    L1 penalty enables         Arbitrary selection among
                      variable selection         correlated vars; no
                                                 constraints

  Elastic Net         Combined L1+L2 penalties   Still no explicit domain
                                                 constraints

  PCA / PLS           Transforms to uncorrelated Loses coefficient
                      components                 interpretability
  ----------------------------------------------------------------------------

#################### 1.4 Theoretical Foundation: Why Some Regularization is Always Optimal

**Theobald-Farebrother Theorem (1974, 1976):** For any OLS problem,
there exists a ridge parameter λ\* \> 0 such that the ridge estimator
has strictly lower Mean Squared Error (MSE) than OLS. This result holds
for the population MSE (true prediction risk), not merely training
error.

**Why This Matters:** OLS minimizes training error but may
overfit---especially with correlated predictors. Ridge introduces bias
but reduces variance. The theorem guarantees that for some λ \> 0, the
variance reduction exceeds the bias increase, yielding lower total
error.

**The Practical Challenge:** The optimal λ\* depends on unknown
population parameters (β, σ²). Cross-validation provides an empirical
estimate. When CV selects λ ≈ 0, OLS was already near-optimal. The
framework adapts automatically.

#################### 1.5 Theoretical Foundation: Constrained Methods Superior to Unconstrained

The Penalized and Constrained (PAC) optimization method developed by
James et al. found:

> *\"The results suggest that PAC and relaxed PAC are surprisingly
> robust to random violations in the constraints. While both methods
> deteriorated slightly as \[constraint error\] increased, they were
> still both superior to their unconstrained counterparts for all values
> of \[error\] and all settings.\"* --- James et al. (2020)

#################### 1.6 Research Contribution: Penalized-Constrained Optimization for Cost Estimation

This paper provides a practical guide and framework that combines
penalized regularization (Lasso, Ridge, Elastic Net) with constrained
optimization in the context of cost estimation. Key contributions
include:

- [Majority of research done on large dataset does it apply to small
  datasets]{.mark}

- Python package combining Elastic Net penalties with lower and upper
  bound constraints on coefficients

- Proper diagnostic adjustments via Generalized Degrees of Freedom (GDF)
  for constraint-driven regression

- Cross-validation framework for likelihood-free hyperparameter
  selection

- sklearn-compatible Python implementation for practical application

- Benchmarks on simulation data comparing proposed method against OLS,
  Ridge, Lasso, and constrained-only approaches across varying sample
  sizes, correlations, and error variances

- Guidance on when and how to implement algorithms

**Future Work (Beyond Scope of This Paper):** Validation on actual
program data using publicly available Selected Acquisition Reports
(SARs), which are not subject to CUI restrictions.

########## 2. Mathematical Framework

#################### 2.1 Problem Formulation

[This is minimization optimization. It was selected because it's super
flexible]{.mark}

- [It can be used for OLS, Ridge, Lasso, Elastic Net, Contrained]{.mark}

[Add discussion on lasso and ridge parameters]{.mark}

The Penalized-Constrained Regression problem adopts the sklearn Elastic
Net parameterization (α, l1_ratio):

minimize L(β) + α · l1_ratio · ‖β‖₁ + ½ · α · (1 − l1_ratio) · ‖β‖₂²

subject to Aβ ≤ b and/or Cβ = d

############################## Parameter Definitions

- **L(β):** Loss function (SSE, SSPE, or user-defined; default is SSPE)

- **α ≥ 0:** Overall penalty strength; α = 0 recovers constrained-only
  optimization

- **l1_ratio ∈ \[0,1\]:** Mix between L1 and L2; l1_ratio = 1 is Lasso,
  l1_ratio = 0 is Ridge

- **Aβ ≤ b:** Linear inequality constraints (e.g., β₁ ≥ 0, slope ≤ 1.0)

- **Cβ = d:** Linear equality constraints (optional)

*Note:* The PAC formulation only applies L1 penalty and has its own
algorithm to solve optimal path.

############################## Why sklearn Elastic Net Parameterization?

We adopt sklearn\'s (α, l1_ratio) parameterization rather than the
standard statistical (λ₁, λ₂) formulation for several practical reasons.
First, sklearn is the dominant machine learning library in Python, and
using consistent parameterization enables direct comparison with
sklearn\'s ElasticNetCV results. Second, the l1_ratio parameter provides
intuitive interpretation: 0 = pure Ridge, 1 = pure Lasso, 0.5 =
balanced. Third, this choice facilitates reproducibility---practitioners
can directly use sklearn\'s cross-validation infrastructure for
hyperparameter tuning before applying constraints. The conversion is
straightforward: λ₁ = α · l1_ratio and λ₂ = α · (1 − l1_ratio).

#################### 2.2 Error Term Options

The framework supports multiple loss functions L(β), each with distinct
properties:

  -------------------------------------------------------------------------
  **Loss**   **Formula**            **Properties**     **Use Case**
  ---------- ---------------------- ------------------ --------------------
  SSE        Σ(yᵢ − ŷᵢ)²            Convex;            Standard OLS setting
                                    closed-form for    
                                    linear             

  SSPE       Σ\[(yᵢ − ŷᵢ)/yᵢ\]²     Unit space;        Default for CERs
                                    MUPE-consistent    

  LAD        Σ\|yᵢ − ŷᵢ\|           Robust to outliers Heavy-tailed errors
  -------------------------------------------------------------------------

############################## Why Unit Space (SSPE)

The default loss function is Sum of Squared Percentage Errors:

L(β) = Σᵢ \[(yᵢ − f(Xᵢ, β)) / yᵢ\]²

This produces directly interpretable percentage errors, while penalizing
outliers and is comparable to MUPE formulation.

############################## Future Work MUPE Formulation (IRLS Variant)

The MUPE method employs Iteratively Reweighted Least Squares (IRLS),
fixing the denominator from the previous iteration. This decoupling
eliminates the positive bias inherent in direct MPE minimization. MUPE
is the Best Linear Unbiased Estimator (BLUE) for linear models with
multiplicative error (Hu & Sjovold, 1994).

############################## Future Work Robustness Alternative: pcLAD

For data with heavy-tailed errors or outliers, Wu et al. (2022)
demonstrate that Penalized-Constrained LAD (pcLAD) offers superior
performance:

> *\"pcLAD enjoys the Oracle property even with Cauchy-distributed
> errors\... particularly effective for monotone curve fitting and
> non-negative constraints.\"* --- Wu et al. (2022)

############################## Key Definitions

**Oracle Property:** An estimator has the Oracle property if it (1)
correctly identifies which coefficients are truly zero (variable
selection consistency), and (2) estimates non-zero coefficients as
efficiently as if an \"oracle\" revealed the true model in advance. This
is the gold standard for high-dimensional estimators---the Lasso
achieves it under certain conditions.

**Cauchy-Distributed Errors:** The Cauchy distribution has extremely
heavy tails---so heavy that its mean and variance are mathematically
undefined (infinite). Outliers occur far more frequently than with
normal distributions. Squared-error methods (OLS, Ridge, Lasso) perform
poorly with Cauchy errors because outliers dominate the objective. LAD
methods minimize absolute errors, so outliers have linear rather than
quadratic influence---making them robust to such extremes.

*\[Scope note: This paper focuses on SSPE for MUPE-consistency.
LAD-based methods are recommended for future research when cost data
exhibits extreme outliers.\]*

#################### 2.3 Convexity and Global Optimality Conditions

Global optimality is guaranteed when the following conditions are
satisfied:

\(1\) The loss function L(β) is convex (e.g., SSE with linear model);

\(2\) All constraints define a convex feasible region (linear
inequalities/equalities);

\(3\) Regularization terms such as L1 and L2 penalties are convex by
definition. Therefore, when added to a convex loss function, the overall
objective remains convex.

**See Note on the L1 Penalty in section 3.3:** Although the L1 penalty
is convex, it is not differentiable at zero, which introduces challenges
for gradient-based optimization algorithms. This non-smoothness can slow
convergence and necessitate specialized methods such as coordinate
descent, subgradient methods, or proximal algorithms to efficiently
handle the non-differentiable points and promote sparsity in the
solution. Implementing efficient algorithms to solve both L1/L2
penalties and constraints is in future work.

**Non-Convex Cases:** For nonlinear models (e.g., power forms Y = aXᵇZᶜ)
with SSPE objective, the problem is non-convex. Local minima are
possible. ZMPE is documented to be sensitive to starting points (Hu &
Smith, 2007). Recommendation: test multiple starting points; COBYLA
optimizer recommended for ZMPE-type problems (Schiavoni et al., 2021).

#################### 2.4 Note on BLUE (Best Linear Unbiased Estimator)

By the Gauss-Markov theorem, OLS is BLUE under classical assumptions.
Introducing penalties and/or constraints means the resulting estimator
is no longer BLUE. This is an intentional tradeoff: we accept bias in
exchange for reduced variance (bias-variance tradeoff), improved
interpretability (domain-consistent coefficients), and enhanced
predictive accuracy (regularization benefits). Per the
Theobald-Farebrother theorem, this tradeoff yields lower MSE for some λ
\> 0. Cross-validation identifies when that λ is near zero (OLS
suffices) versus when substantial regularization helps.

########## 3. Algorithm and Implementation

#################### 3.1 High-Level Algorithm

1.  **Input:** Data (X, y), functional form f(X, β), penalty parameters
    (α, l1_ratio), bounds/constraints, error function, optimizer choice

2.  **Scale (optional):** Standardize X (mean=0, std=1) if scale=True

3.  **Initialize:** Starting coefficients from OLS (default when
    possible), trimmed to satisfy bounds; alternatively zeros or
    user-specified

4.  **Optimize:** Solve constrained penalized minimization via selected
    optimizer

5.  **Unscale:** Transform coefficients back to original units:
    β_original = β_scaled / σ

6.  **Output:** Coefficient estimates β̂, fit statistics (GDF-adjusted),
    active_constraints\_ flag

############################## Initialization Strategy

The default initialization uses OLS coefficients when the problem is
well-conditioned (X\'X invertible). These starting values are then
trimmed to satisfy bounds---any coefficient outside the specified bounds
is clamped to the nearest boundary. This approach provides a warm start
that respects domain constraints from the first iteration, improving
convergence speed and reducing the risk of local minima.

**Why Scaling Matters:** Scaling ensures that the penalty
terms---especially L1 and L2 ---are applied fairly across features.
Without scaling, features with larger magnitudes may dominate the
optimization, leading to biased shrinkage and poor variable selection.

Scaling improves the conditioning of the optimization problem, which can
significantly enhance convergence speed and numerical stability. After
optimization, the estimated coefficients can be transformed back to the
original feature space for interpretation.

\[Add research on why scaling matters and it's benefits\]

[For **linear models**, standardization (zero mean, unit variance) is
typically sufficient.]{.mark}

[For **nonlinear models** like *AX\^b,* you may need:]{.mark}

- [**Log-transformations** (common in power-law modeling)]{.mark}

- [**Min-max scaling** or **domain-specific normalization**]{.mark}

- [Careful **unscaling of both *X* and *B*** to preserve
  interpretability]{.mark}

[In nonlinear models such as Y=AX\^b, scaling the predictor X alters the
curvature and interpretation of the exponent b. When applying penalized
regression with L1 and L2 penalties to such models, scaling affects both
the error function and the regularization terms. Therefore, scaling must
be applied with caution, and unscaling procedures should be explicitly
defined to preserve interpretability in the original data space. In
future work, we plan to explore scaling strategies tailored to nonlinear
functional forms, including log-transforms and domain-aware
normalization.]{.mark}

#################### 3.2 Optimization Methods

**General Constrained Solvers (scipy.optimize.minimize):**

- **SLSQP (Sequential Least-Squares Quadratic Programming):** Current
  default. Handles bounds and linear constraints efficiently.

- **COBYLA (Constrained Optimization BY Linear Approximation):**
  Derivative-free; recommended for ZMPE-type problems (Schiavoni et al.,
  2021).

- **trust-constr:** Interior point method; suitable for larger-scale
  problems with many constraints.

***Note on L1 Penalty (Future Work)***

Evaluating alternative solvers is an area for improvement. The L1
penalty is not differentiable at 0, meaning solvers must handle this
case using non-smoothness techniques. The most popular approach is
coordinate descent; for problems that are also constrained, projected
coordinate descent or constrained coordinate descent is required.
Options include using cvxpy or creating a custom implementation using
numpy. Additionally, the PAC algorithm from James et al. (2020) provides
a specialized solver for penalized-constrained problems.

**Practical Implications:** In this study, we rely on general-purpose
optimization routines (e.g., those available in SciPy) to solve the
penalized-constrained optimization problem. Despite the theoretical
limitations of these solvers in handling non-differentiable terms, they
often yield reasonable and stable solutions in practice, especially when
the problem size is moderate and constraints are well-posed.

########## 4. Degrees of Freedom for Constrained Models

**Critical Issue:** When constraints are imposed on regression
coefficients, the effective degrees of freedom (DF) must be adjusted.
Without this adjustment, fit statistics (SEE, SPE, Adjusted R²) are
incorrect and misleading.

#################### 4.1 Generalized Degrees of Freedom (GDF)

> *\"ZMPE users do not adjust the degrees of freedom (DF) to account for
> constraints included in the regression process. As a result, fit
> statistics for the ZMPE equations, e.g., the standard percent error
> (SPE) and generalized R² (GRSQ), can be incorrect and misleading.\"*
> --- Hu (GDF Paper)

Two primary approaches exist for computing GDF in constrained regression:

############################## Hu\'s GDF Formula (Conservative Approach)

GDF = n − p − (# Constraints) + (# Redundancies)

Where n is the sample size and p denotes the total number of estimated parameters (coefficients). Under Hu's approach, **all specified constraints** reduce degrees of freedom, regardless of whether they are binding at the solution. One restriction is equivalent to a loss of one DF.

**Redundancies Definition (Hu):** If two constraints are specified in a
regression model but one constraint can be derived from the other, then
count only a loss of one DF rather than two. Additionally, if a
parameter is known (e.g., the startup cost is known or the rate slope is
given), then this amounts to a gain of one DF. For ZMPE CERs (except
simple factor CERs), DF should be subtracted by one because the solution
is achieved using the constraint alone.

**When to Use Hu's Method:** This conservative approach is appropriate when:
- All specified constraints meaningfully affect the estimation process
- You want a consistent DF calculation regardless of solution location
- The analyst has limited visibility into which constraints are binding

############################## Gaines et al. (2018) Constrained Lasso DF Formula

GDF = n − p − (# equality constraints) − (# binding inequality constraints)

Where n is the sample size and p is the number of active predictors (non-zero coefficients). Under Gaines' approach, **only binding constraints** (those where the coefficient is at the bound) reduce degrees of freedom. Non-binding constraints do not affect the DF calculation.

**When to Use Gaines' Method:** This approach is appropriate when:
- You want DF to reflect the actual constraints active at the solution
- Loose bounds are specified primarily as "guardrails" rather than informative priors
- The solution is typically in the interior of the feasible region

############################## Comparing the Two Formulations

| Aspect | Hu's Method | Gaines' Method |
|--------|-------------|----------------|
| Formula | n − p − (all constraints) | n − p − (binding constraints) |
| Philosophy | Conservative; all constraints matter | Adaptive; only active constraints matter |
| Loose bounds (β ≤ 0) | Counts against DF | No DF loss if not binding |
| Tight bounds (0.85 < β < 0.95) | Counts against DF | Counts only if at bound |
| Result | Lower GDF (larger standard errors) | Higher GDF (smaller standard errors) |

**Practical Guidance:** Loose bounds (e.g., β ≤ 0) may not bind---resulting in no DF loss under Gaines' formulation. Tight bounds (e.g., 0.85 < β < 0.95) are more likely to bind, making the two methods converge. When regularization pushes coefficients away from bounds that would otherwise bind under OLS, Gaines' method provides a more accurate reflection of the estimation uncertainty.

**Implementation Note:** The `penalized-constrained` package supports both methods via the `gdf_method` parameter ('hu' or 'gaines'). The default is 'gaines' as it reflects the effective degrees of freedom at the solution.

Future Work: *will empirically evaluate which approach produces more
accurate uncertainty quantification through simulation, but acknowledges
that neither approach may be definitively superior in all cases.*

#################### 4.2 Fit Statistics Using GDF

SEE = √\[Σ(yᵢ − ŷᵢ)² / GDF\] SPE = √\[Σ((yᵢ − ŷᵢ)/ŷᵢ)² / GDF\]

> *\"Using ZMPE CERs in cost uncertainty analysis may unduly tighten the
> S-curve because their SPEs underestimate the CER error
> distribution.\"* --- Hu (GDF Paper)

########## 5. Model Diagnostics and Validation

#################### 5.1 Model Uncertainty Estimation: Cross-Validation as Primary Model Selection

Since penalized-constrained estimators do not follow standard likelihood
theory, cross-validation serves as the primary arbiter of model quality.
Cross-validation provides several advantages for optimization-based
methods: it directly estimates out-of-sample prediction error without
distributional assumptions, enables comparison across fundamentally
different model classes (OLS vs. penalized vs. constrained), and
provides a principled basis for hyperparameter selection (α, l1_ratio).
For cost estimation with small samples, Leave-One-Out Cross-Validation
(LOOCV) is recommended to maximize training data usage. K-fold CV (k=5
or k=10) provides a computationally cheaper alternative when sample size
increases.

[\[ADD discussion on Conformal Prediction intervals\]]{.mark}

**[What is Conformal Prediction?]{.mark}**

[Conformal prediction is a **model-agnostic framework** for creating
statistically valid prediction intervals (or sets) around point
predictions. It works under the assumption that data points are
*exchangeable* and provides **finite-sample coverage guarantees**. In
other words, if you specify a 95% coverage level, the intervals will
contain the true value about 95% of the time.]{.mark}

[Key properties:]{.mark}

- [**Model-agnostic**: Works with any ML model (linear, tree-based,
  neural nets).]{.mark}

- [**Distribution-free**: No assumptions about the underlying data
  distribution.]{.mark}

- [**Coverage guarantee**: You choose a significance level (e.g., 0.05
  for 95% coverage), and the intervals are calibrated to meet
  that.]{.mark}

**[How It Works]{.mark}**

1.  [Train your model normally.]{.mark}

2.  [Use a **calibration set** (not the training set) to compute
    residuals.]{.mark}

3.  [Calculate a quantile of these residuals based on your desired
    coverage (e.g., 95%).]{.mark}

4.  [For new predictions:]{.mark}

    a.  [Lower bound = prediction − quantile]{.mark}

    b.  [Upper bound = prediction + quantile]{.mark}

[Variants:]{.mark}

- [**Absolute Conformal**: Adds a fixed width based on residual
  quantile.]{.mark}

- [**Locally Weighted Conformal**: Interval width varies by local
  uncertainty.]{.mark}

**[How It Works in Extrapolation]{.mark}**

- [The interval width is based on the **distribution of past residuals**
  (errors between predictions and actuals in the calibration
  set).]{.mark}

- [When you predict outside the training/calibration range, the method
  **does not inherently know the new uncertainty**---it assumes the same
  error distribution applies.]{.mark}

- [So, the intervals will still be generated, but they may
  **underestimate uncertainty** because the model might perform worse in
  unseen regions.]{.mark}

**[⚠️ Limitations]{.mark}**

- [**Coverage guarantee only holds under exchangeability** (future data
  behaves like past data).]{.mark}

- [If the new region has different dynamics (e.g., trend shifts,
  seasonality changes), intervals can be misleading.]{.mark}

- [For long-horizon forecasting, residuals often grow with horizon, so
  naive conformal intervals can be too narrow.]{.mark}

**[✅ Practical Solutions]{.mark}**

- [Use **adaptive conformal prediction**: adjust interval width based on
  forecast horizon or local uncertainty.]{.mark}

- [Combine with **quantile regression** or **bootstrapping** for better
  extrapolation handling.]{.mark}

- [Monitor coverage on rolling windows and recalibrate
  periodically.]{.mark}

#################### 5.2 Coefficient Uncertainty Estimation

Four alternative approaches are available to estimate coefficient
uncertainty for optimization-based algorithms, each suitable for
likelihood-free contexts:

(1) **Hessian-based covariance:** Uses the inverse Hessian of the
    objective function at the solution to approximate the covariance
    matrix. Available directly from most optimizers. Assumes local
    quadratic approximation is valid.

(2) **Jacobian-based (like curve_fit):** Estimates covariance from the
    Jacobian of residuals, following scipy.optimize.curve_fit
    methodology. Appropriate for nonlinear least squares problems.

(3) **Bootstrap:** Resamples the data and re-estimates coefficients to
    build empirical confidence intervals. Does not require
    distributional assumptions. *Caveat:* For penalized models,
    bootstrap CIs may be artificially narrow because penalties constrain
    coefficient variability across resamples (penalized R package
    documentation). In such cases it is prudent compare the
    unconstrained bootstrap to the constrained bootstrap.

(4) **Profile likelihood:** Varies each coefficient individually while
    re-optimizing others to trace out confidence regions. Most
    computationally expensive but most robust for non-quadratic
    objective surfaces.

[MATLAB's fitnlm estimates standard errors and p-values using a linear
approximation of the nonlinear model, leveraging the Jacobian of
residuals and assuming normally distributed errors. In contrast, COSTAT
uses a derivative-free optimization method (Nelder-Mead simplex) focused
on parameter estimation, without built-in statistical inference. For
COSTAT-based models, uncertainty must be assessed via external methods
such as bootstrapping or sensitivity analysis.]{.mark}

This paper implements Bootstrap and Hessian-based covariance metrics to
approximate coefficient uncertainty.

########## 6. Simulation Study Design

#################### 6.1 Data Generation: Learning Curve with Rate Effect

**True Model:** Y = T₁ × (LotMidpoint)ᵇ × (LotQuantity)ᶜ × ε

- T₁ = 100 (first unit cost) --- *Note: T₁ is estimated in all examples,
  not fixed*

- Error: Multiplicative lognormal with specified CV

Correlated predictor datasets are generated using a custom function that
creates realistic production ramp-up schedules where lot midpoint and
quantity naturally correlate at the specified ρ levels.

[\[Idea sample size can be a function of which datapoints are fed into
the regression model. That way I don't have to generate n data sets to
represent number of samples\]]{.mark}

#################### 6.2 Experimental Factors

- **Sample sizes:** n ∈ {5, 10, 30} lots

- **Correlation levels:** ρ(ln(Midpoint), ln(Quantity)) ∈ {0, 0.5, 0.9}

- **Error CV:** {0.01, 0.1, 0.2}

- **Learning slopes:** b ∈ {ln(0.85)/ln(2), ln(0.90)/ln(2),
  ln(0.95)/ln(2)} corresponding to 85%, 90%, 95% learning

- **Rate slopes:** c ∈ {ln(0.80)/ln(2), ln(0.85)/ln(2), ln(0.90)/ln(2)}
  corresponding to 80%, 85%, 90% rate effect

- **Replications:** 50 simulations per scenario \[placeholder --- will
  increase prior to publication\]

#################### 6.3 Methods Compared

- **OLS:** Base case, unconstrained, unpenalized

- **OLS_LearnOnly:** Drop rate variable (confluence analysis remedy)

- **RidgeCV:** Ridge with CV-selected λ, no constraints

- **LassoCV:** Lasso with CV-selected λ, no constraints

- **PLS / PCA_Linear:** Both produce identical results for two
  predictors

- **BayesianRidge:** sklearn implementation using spherical Gaussian
  prior p(w\|λ) = N(w\|0, λ⁻¹I) centered at zero. Iteratively maximizes
  marginal log-likelihood to optimize regularization parameters (MacKay,
  1992; Tipping, 2001). Note: This uses uninformative priors and does
  not incorporate domain-specific constraints, so serves as comparison
  to automatic regularization without domain knowledge.

- **ConstrainedOnly:** Constraints without penalty (α=0)

- **PenalizedConstrainedCV:** Proposed method with loose bounds (β ∈
  \[−1, 0\])

- **PenalizedConstrainedCV_Tight:** Proposed method with tight bounds
  near true values. This serves as an approximate upper bound on
  performance---analogous to an \"oracle\" benchmark that knows the true
  parameter region.

- **PenalizedConstrainedCV_Wrong:** Proposed method with incorrect
  constraints. Following James et al. (2020) PAC methodology, \"wrong\"
  constraints are generated by adding random noise to the true bounds:
  constraint_wrong = constraint_true + a × U(−1,1), where a controls the
  magnitude of constraint error. This tests robustness to constraint
  misspecification.

#################### 6.4 Evaluation Metrics

- **Coefficient bias:** \|β̂ − β_true\| for b (learning) and c (rate)

- **Coefficient variance:** Stability across replications

- **[\[Should this match error term SSPE used to fit model? I guess we
  could compare all the major error metrics\]]{.mark} MAPE on next 5
  lots:** Out-of-sample prediction accuracy

- **Domain consistency rate:** Percentage of replications with correct
  coefficient signs

########## 7. Results

*\[PLACEHOLDER: To be completed after simulation runs\]*

########## 8. Discussion

#################### 8.1 Practical Guidance for Cost Estimators

- **Document everything:** constraints, penalties, active bounds at
  solution. This is NOT OLS.

- **Start with loose constraints based on domain knowledge:** (e.g.,
  learning slope ≤ 100%)

- **Constraints need not be perfect:** even approximately correct bounds
  improve estimation

- **Derive constraints from domain benchmarks:** Published learning
  curve studies, historical program data, and subject matter expert
  knowledge can inform reasonable bounds. Reference benchmarks specific
  to your domain (e.g., aerospace learning curves typically range
  75-95%).

- **Use cross-validation for (α, l1_ratio) selection:** do not impose
  arbitrary penalty values

- **Report GDF-adjusted statistics:** for transparency

- **Try multiple starting points:** For nonlinear models, testing
  multiple starting points can avoid local optima

- **Don\'t worry (too much) about global optimum:** Even if not
  confirmed to be globally optimal, it can still be the best reasonable
  model

- **Regularization is recommended (even if minimal):** Per
  Theobald-Farebrother, some L2 regularization is always optimal, even
  when correlation is not high

#################### 8.2 Limitations and Cautions

- **Abuse potential:** Constraints could be used to force desired
  results. Transparency in documentation is essential.

- **Not BLUE:** Always disclose that the method intentionally introduces
  bias in exchange for reduced variance.

- **Local optima:** For nonlinear models, test multiple starting points.

- **Bootstrap CIs:** May be artificially narrow for penalized models.

- **Speed:** Optimization routines take longer to converge than
  closed-form OLS (trivial concern for small datasets and few runs).

- **Heteroscedasticity:** This implementation does not include weighted
  approaches. SSPE partially addresses heteroscedasticity through
  unit-space operation, but formal weighted least squares integration is
  future work.

#################### 8.3 Future Research

- Validation on actual program data using publicly available Selected
  Acquisition Reports (SARs)

- pcLAD implementation for robust estimation with outliers

- Additional algorithms: coordinate descent, projected gradient methods,
  and the PAC algorithm from James et al.

- Alternative optimizers: Systematic comparison of SLSQP, COBYLA,
  trust-constr, and cvxpy performance

- Weighted approaches for explicit heteroscedasticity modeling

- PenalizedConstrainedMUPE: Proposed method using MUPE/IRLS loss
  function with penalties and constraints

########## 9. Conclusion

*\[PLACEHOLDER: To be written after results\]*

########## References

#################### Cost Estimation / ICEAA Community

Flynn, B. & James, A. (2016). \"Multicollinearity in CER Development.\"
ICEAA Professional Development & Training Workshop.

Hu, S. (2010+). \"Generalized Degrees of Freedom for Constrained CERs.\"
Tecolote Research, PRT-191.

Hu, S. & Smith, A. (2007). \"Why ZMPE When You Can MUPE?\" ISPA/SCEA
Joint Conference.

Schiavoni, B. et al. (2021). \"Assessing Regression Methods.\" ICEAA
Online Workshop.

#################### Machine Learning / Statistics

Gaines, B.R., Kim, J., & Zhou, H. (2018). \"Algorithms for Fitting the
Constrained Lasso.\" J. Computational and Graphical Statistics, 27(4),
861-871.

James, G.M., Paulson, C., & Rusmevichientong, P. (2020). \"Penalized and
Constrained Optimization.\" JASA.

MacKay, D.J.C. (1992). \"Bayesian Interpolation.\" Computation and
Neural Systems, Vol. 4, No. 3.

Tipping, M.E. (2001). \"Sparse Bayesian Learning and the Relevance
Vector Machine.\" Journal of Machine Learning Research, Vol. 1.

Wu, X., Liang, R., & Yang, H. (2022). \"Penalized and Constrained LAD
Estimation.\" Statistical Papers, 63, 53-95.

#################### Foundational Theory

Theobald, C.M. (1974). \"Generalizations of mean square error applied to
ridge regression.\" JRSS-B, 36, 103-106.

Farebrother, R.W. (1976). \"Further results on the mean square error of
ridge regression.\" JRSS-B, 38, 248-250.

########## Appendix: Key Quotes for Potential Use

*\[Reference compilation for author\'s convenience\]*

############################## On Multicollinearity Severity

> *\"The statistical literature regards multicollinearity as one of the
> most vexing and intractable problems in all of regression analysis.\"*
> --- Flynn & James (2016)

############################## On Coefficient Instability

> *\"Generate unstable models where small changes in the data produce
> big changes in parameter estimates \[bouncing β\'s\].\"* --- Flynn &
> James (2016)

############################## On GDF Importance

> *\"Using ZMPE CERs in cost uncertainty analysis may unduly tighten the
> S-curve because their SPEs underestimate the CER error
> distribution.\"* --- Hu (GDF Paper)

############################## On Robustness to Imperfect Constraints

> *\"PAC and relaxed PAC were surprisingly robust to random violations
> in the constraints. Both methods deteriorated slightly as \[constraint
> error\] increased, but remained superior to unconstrained methods for
> all values and all settings.\"* --- James et al. (2020)

############################## On Constrained Lasso Flexibility

> *\"The constrained lasso is a very flexible framework for imposing
> additional knowledge and structure onto the lasso coefficient
> estimates.\"* --- Gaines et al. (2018)

########## BACKUP SECTION: Content Moved from Main Text

*This section contains content that was removed from the main text
during revision but is preserved for reference.*

#################### Original Section 2.3 Implementation Notes (Moved to Section 3)

The L1 penalty is not differentiable at 0 meaning that solvers must
handle this case using non-smoothness techniques. The most popular
approach is coordinate descent; for problems that are also constrained
you need to use projected coordinate descent or constrained coordinate
descent. What does pcLAD use? Options are to use cvxpy or create custom
implementation using numpy. This is future work---we are sticking with
default solvers and acknowledge the issue for now.

#################### Original BayesianRidge Description

BayesianRidge: sklearn implementation where prior knowledge is 0 (seems
promising and can likely include prior information for variables instead
of using constraints by transforming Y. There\'s an ICEAA paper on doing
Bayesian Regression that I can cite for its use in Cost Estimation.

#################### Original Section 5.1 Incomplete Thought

\"Should I discuss the other forms? And pros for generalizing beyond
dataset?\" --- This has been addressed by expanding Section 5.1 to
discuss cross-validation advantages and Section 5.2 to cover four
likelihood-free coefficient uncertainty estimation approaches.

#################### Future Work Items Not Yet Incorporated

- PenalizedConstrainedMUPE (Future Study?): Proposed method using
  MUPE/IRLS loss function with penalties and constraints

- Bayesian regression as alternative to constraints: Prior information
  can be incorporated via transformed Y instead of explicit constraints

- Oracle benchmark implementation: Could implement true oracle (known
  optimal λ) separately from Tight constraints approach

[^1]: PA04-paper-Collinearity-Kill-Chain.pdf

[^2]: PA04-paper-Collinearity-Kill-Chain.pdf
