---
title: "Penalized-Constrained Regression"
subtitle: "Combining Regularization and Domain Constraints for Cost Estimation"
author:
  - name: Kevin Joy
    affiliation: Herren Associates
  - name: Max Watstein
    affiliation: Herren Associates
date: today
format:
  pdf:
    toc: true
    toc-depth: 3
    number-sections: true
    number-offset: 1
    colorlinks: true
    documentclass: article
    geometry:
      - margin=1in
    fontsize: 11pt
    include-in-header:
      text: |
        \usepackage{booktabs}
        \usepackage{float}
        \floatplacement{figure}{H}
        \floatplacement{table}{H}
bibliography: references.bib
execute:
  echo: false
  warning: false
  message: false
---

```{python}
#| label: setup

import pandas as pd
import numpy as np
from pathlib import Path
import subprocess
import sys
import matplotlib.pyplot as plt
import seaborn as sns
import tabulate

# Set plotting style
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("colorblind")
plt.rcParams['figure.dpi'] = 150
plt.rcParams['savefig.dpi'] = 150
plt.rcParams['font.size'] = 10

# Set paths
OUTPUT_DIR = Path('output_v2')

# Run simulation if data doesn't exist
if not (OUTPUT_DIR / 'simulation_results.parquet').exists():
    print("Simulation data not found. Running simulation...")
    subprocess.run([sys.executable, 'run_simulation.py'], check=True)
    subprocess.run([sys.executable, 'simulation_analysis.py'], check=True)

# Load data
df_example = pd.read_csv(OUTPUT_DIR / 'motivational_example_data.csv')
results_all = pd.read_parquet(OUTPUT_DIR / 'simulation_results.parquet')

# Filter to OLS, PCReg_GCV, and OLS_LearnOnly for paper analysis
MODELS_FOR_PAPER = ['OLS', 'PCReg_GCV', 'OLS_LearnOnly']
results = results_all[results_all['model_name'].isin(MODELS_FOR_PAPER)].copy()

# Calculate derived columns for analysis
results['LC_est'] = 2 ** results['b']
results['RC_est'] = 2 ** results['c']
results['LC_true'] = 2 ** results['b_true']
results['RC_true'] = 2 ** results['c_true']

# Define OLS reasonableness (based on OLS results for each scenario)
ols_reasonableness = results[results['model_name'] == 'OLS'][['seed', 'LC_est', 'RC_est']].copy()
ols_reasonableness['ols_reasonable'] = (
    (ols_reasonableness['LC_est'] >= 0.70) & (ols_reasonableness['LC_est'] <= 1.0) &
    (ols_reasonableness['RC_est'] >= 0.70) & (ols_reasonableness['RC_est'] <= 1.0)
)
ols_reasonableness = ols_reasonableness[['seed', 'ols_reasonable']]

# Merge back to all results
results = results.merge(ols_reasonableness, on='seed', how='left')

# Calculate bias for each coefficient
results['T1_bias'] = results['T1_est'] - results['T1_true']
results['b_bias'] = results['b'] - results['b_true']
results['c_bias'] = results['c'] - results['c_true']
```

\newpage

# Abstract {.unnumbered}

Small datasets with intercorrelation pose serious challenges to the stability of coefficients generated by Ordinary Least Squares (OLS) regression. A motivating example in cost estimating is Learning Curve with Rate Effect analysis, where datasets are typically small, the lot midpoint (Learning) is correlated to lot quantity (Rate) as production ramps up, and slopes are expected to be $\leq 100\%$. Lasso, Ridge, and Elastic Net regularization methods address multicollinearity by penalizing coefficients. Separately, constrained optimization methods can impose explicit restrictions on coefficient values when prior knowledge about their behavior is known—such as bounding slopes within a known range. This paper investigates the combined effects of penalized regularization methods and constrained optimization.  We explore how to assess model stability and goodness-of-fit using likelihood-free diagnostic techniques suited to optimization-based regressions, such as cross-validation for generalization error.

\newpage

# Introduction

While developing Cost Estimating Relationships (CERs) for small datasets (5-30 data points), a recurring pattern emerged. The regression models often showed strong fit statistics with high R² and low standard error, but nonsensical coefficients. Coefficients often had wrong signs, implausible magnitudes, and poor p-values. As Department of Defense (DoD) analysts, this story may feel all too familiar.

Intercorrelated datasets are a frequent presence in cost analysis, causing regression models to misbehave. This is especially true for Learning Curve with Rate Effect analysis, where datasets are typically small and the lot midpoint is usually correlated to lot quantity. Traditional remedies for multicollinearity can be insufficient for learning curve datasets. Increasing sample size is infeasible as it requires waiting years for additional production lots and cost data to become available. Dropping explanatory variables, such as rate effect, leads to model misspecification that ignores a fundamental cost driver. This issue is compounded when predicting outside the relevant range which is very common. Even when these remedies are employed, OLS can still produce implausible coefficients with the wrong signs, such as learning curve slopes \>100%. Regularization techniques like Ridge, Lasso, and Elastic Net can help stabilize estimates, but they don't guarantee plausible coefficients. Constrained optimization can enforce domain knowledge, but without regularization, it may not solve the underlying multicollinearity problem. This paper explores the combination of penalized regularization and constrained optimization to address both challenges.

For our research, we reviewed existing literature on multicollinearity, regularization, constrained optimization, and learning theory. Drawing on these foundations, we developed the Penalized-Constrained Regression (PCReg) framework, which integrates Elastic Net penalties with domain-knowledge coefficient constraints. We then used Monte Carlo simulation to evaluate PCReg's accuracy estimating the true parameters and compared it's performance against OLS across a range of sample sizes and collinearity conditions, measuring both coefficient recovery and out-of-sample prediction accuracy.

This paper develops and validates Penalized-Constrained Regression for cost estimation, providing:

1.  **PCReg Python Package** PCReg application in Python
2.  **PCReg Excel Template** PCReg application in Excel
3.  **Cross Validation framework** for hyperparameter selection without data splitting
4.  **Practical Guidance** for when to use PCReg and how to assess model fit

\newpage

# Motivating Example

## The Learning Curve Problem

The motivating example for this research is the learning curve with rate effect:

$$
\begin{aligned}
\text{Average Unit Cost} &= T_1 \cdot (\text{Lot Midpoint})^b \cdot (\text{Lot Quantity})^c \cdot \varepsilon \\
\\
\text{where: } T_1 &= \text{theoretical first unit cost} \\
b &= \text{learning slope in log space} \\
c &= \text{rate slope in log space} \\
\varepsilon &= \text{multiplicative error}
\end{aligned}
$$

$b$ and $c$ are both slopes in log space. When transformed to unit space, they become the learning curve slope (LCS = $2^b$) and the rate effect ($2^c$), both typically ranging from 70-100%. As DoD programs ramp up production from Engineering Manufacturing Development (EMD) to Full Rate Production (FRP), lot midpoint (the learning predictor) becomes highly correlated with lot quantity (the rate predictor). By definition LCS and Rate Effect are $\leq 100\%$, meaning costs strictly decrease with cumulative production or lot quantity.

If estimated LCS or Rate Effect are \>100% this is often an indicator of multicollinearity or that other explanatory variables such as supply chain disruptions or diseconomies of scale are missing and need to be modeled, not a violation of the definition. Once these factors are properly modeled as separate explanatory variables, estimated LCS and Rate Effect should be $\leq 100\%$, consistent with their definition. It is the analyst's responsibility to ensure the model is properly specified and includes all relevant cost drivers. 

## Example Scenario

We demonstrate the "Learning Curve Problem" using the following dataset which was generated by our Simulation Study \***insert cross reference**.

```{python}
#| label: scenario-params

df_train = df_example[df_example['lot_type'] == 'train'].copy()
df_test = df_example[df_example['lot_type'] == 'test'].copy()

true_lr = 2 ** df_example['b_true'].iloc[0]
true_re = 2 ** df_example['c_true'].iloc[0]
true_T1 = df_example['T1_true'].iloc[0]
true_b = df_example['b_true'].iloc[0]
true_c = df_example['c_true'].iloc[0]
n_train = len(df_train)
n_test = len(df_test)

# Calculate correlation between log predictors
log_mp = np.log(df_train['lot_midpoint'])
log_qty = np.log(df_train['lot_quantity'])
corr = np.corrcoef(log_mp, log_qty)[0, 1]

# Get CV from data
cv_error = df_example['cv_error'].iloc[0]

# Calculate true cost curve for plotting
lot_mp_range = np.linspace(df_example['lot_midpoint'].min() * 0.8,
                            df_example['lot_midpoint'].max() * 1.2, 100)
# Use median lot quantity for true curve
median_qty = df_example['lot_quantity'].median()
true_curve = true_T1 * (lot_mp_range ** true_b) * (median_qty ** true_c)
```

```{python}
#| label: tbl-scenario-example
#| tbl-cap: "Motivating example training dataset."
#| echo: false
#| output: asis

from tabulate import tabulate

# Choose and format the columns you want to show
cols = ["lot_midpoint", "lot_quantity", "observed_cost"]
df_show = df_train[cols].copy()

# Format floats to a consistent precision
md = tabulate(
    df_show,
    headers="keys",
    tablefmt="pipe",   # Pandoc-friendly pipe table
    showindex=False,
    floatfmt=".3f"
)

print(md)
```

Using OLS, we get the following results:

```{python}
#| label: model-fits

from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer
from sklearn.compose import TransformedTargetRegressor
from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_squared_error
import penalized_constrained as pcreg



X_train = df_train[['lot_midpoint', 'lot_quantity']].values
y_train = df_train['observed_cost'].values
X_test = df_test[['lot_midpoint', 'lot_quantity']].values
y_test_true = df_test['true_cost'].values  # True underlying values
y_test_obs = df_test['observed_cost'].values  # Noisy observations

# ==== OLS Stats Model ===
import statsmodels.formula.api as smf
ols_sm = smf.ols(formula="np.log(observed_cost) ~ np.log(lot_midpoint) + np.log(lot_quantity)", data=df_train).fit()
    
# === OLS ===
ols = TransformedTargetRegressor(
    regressor=Pipeline([
        ('log', FunctionTransformer(np.log)),
        ('reg', LinearRegression()),
    ]),
    func=np.log,
    inverse_func=np.exp,
)
ols.fit(X_train, y_train)

ols_b, ols_c = ols.regressor_.named_steps['reg'].coef_
ols_lr = 2 ** ols_b
ols_re = 2 ** ols_c
ols_T1 = np.exp(ols.regressor_.named_steps['reg'].intercept_)
ols_pred_train = ols.predict(X_train)
ols_pred_test = ols.predict(X_test)
ols_r2_train = r2_score(y_train, ols_pred_train)
ols_mape_true = mean_absolute_percentage_error(y_test_true, ols_pred_test)
ols_resid_train = y_train - ols_pred_train
ols_resid_true = y_test_true - ols_pred_test

# === OLS Learn Only ===
X_train_learn = df_train[['lot_midpoint']].values
X_test_learn = df_test[['lot_midpoint']].values

ols_learn = TransformedTargetRegressor(
    regressor=Pipeline([
        ('log', FunctionTransformer(np.log)),
        ('reg', LinearRegression()),
    ]),
    func=np.log,
    inverse_func=np.exp,
)
ols_learn.fit(X_train_learn, y_train)

ols_learn_b = ols_learn.regressor_.named_steps['reg'].coef_[0]
ols_learn_lr = 2 ** ols_learn_b
ols_learn_T1 = np.exp(ols_learn.regressor_.named_steps['reg'].intercept_)
ols_learn_pred_train = ols_learn.predict(X_train_learn)
ols_learn_pred_test = ols_learn.predict(X_test_learn)
ols_learn_r2_train = r2_score(y_train, ols_learn_pred_train)
ols_learn_mape_true = mean_absolute_percentage_error(y_test_true, ols_learn_pred_test)
ols_learn_resid_train = y_train - ols_learn_pred_train
ols_learn_resid_true = y_test_true - ols_learn_pred_test

# === PCReg ===
def prediction_fn(X, params):
    T1, b, c = params
    return T1 * (X[:, 0] ** b) * (X[:, 1] ** c)

pc_gcv = pcreg.PenalizedConstrainedCV(
    coef_names=['T1', 'b', 'c'],
    bounds={'T1': (0, None), 'b': (-0.5, 0), 'c': (-0.5, 0)},
    prediction_fn=prediction_fn,
    fit_intercept=False,
    x0=[100, -0.1, -0.1],
    alphas=np.linspace(0, 5, 10),
    l1_ratios= [0.0, 0.5, 1.0],
    selection='gcv',
    loss='sspe',
    penalty_exclude=['T1'],
    n_jobs=1
)
pc_gcv.fit(X_train, y_train)

pc_T1, pc_b, pc_c = pc_gcv.coef_
pc_lr = 2 ** pc_b
pc_re = 2 ** pc_c
pc_pred_train = pc_gcv.predict(X_train)
pc_pred_test = pc_gcv.predict(X_test)
pc_r2_train = r2_score(y_train, pc_pred_train)
pc_mape_true = mean_absolute_percentage_error(y_test_true, pc_pred_test)
pc_resid_train = y_train - pc_pred_train
pc_resid_true = y_test_true - pc_pred_test

# Compute GDF early for inline references
_report_early = pcreg.generate_summary_report(
    pc_gcv, X_train, y_train,
    full=False,
    gdf_method='gaines',
    bootstrap=False
)
gdf = _report_early.fit_stats.gdf
n_active = _report_early.constraints.n_active
```

| Metric        |                             OLS |
|:--------------|--------------------------------:|
| $T_1$         |      `{python} f'{ols_T1:.0f}'` |
| Learning Rate | `{python} f'{ols_lr*100:.1f}'`% |
| Rate Effect   | `{python} f'{ols_re*100:.1f}'`% |

```{python}
#| label: ols-stats model summary fit
#| echo: false
#| output: asis

ols_sm_summary = ols_sm.summary2()
print(ols_sm.summary().as_latex())
```

We see find that by using OLS this example dataset yields invalid coefficents, specifcally a LCS of `{python} f'{ols_lr*100:.1f}'`% which violates our definiton of LCS being $\leq 100\%$. But otherwise this is a good fit with strong R² (`{python} f'{ols_r2_train:.2f}'`) and significant F-statistic. This is a common pattern in cost estimating where OLS produces good fit statistics but implausible coefficients. The next section introduces the Penalized-Constrained Regression (PCReg) framework which addresses this issue by combining regularization penalties with domain-knowledge constraints to stabilize coefficient estimates while ensuring they remain within plausible ranges.
\newpage

# Theoretical Foundation

## Regularization: Optimality of Ridge Penalty
**Theobald-Farebrother Theorem [@theobald1974ridge; @farebrother1976further]:** For any OLS problem, there exists a ridge parameter $\lambda^* > 0$ such that the ridge estimator has strictly lower mean squared error (MSE) than OLS. This result holds for population MSE (true prediction risk), not merely training error. The optimal $\lambda^*$ depends on unknown population parameters, which cross-validation estimates empirically.

## Constrained Methods: Superior Performance Under Misspecification
@james2020pac demonstrated that penalized and constrained (PAC) methods remain superior to unconstrained alternatives even when constraints are imperfectly specified within the Website Advertising domain:

> *"PAC and relaxed PAC are surprisingly robust to random violations in the constraints... they were still both superior to their unconstrained counterparts for all values [of constraint error] and all settings."*

# Framework
The Penalized-Constrained Regression (PCReg) framework applies the theoretical and experimental foundations to the cost estimation domain. PCReg consists of two major components:
- the penalized-constrained regression model and
- the model diagnostics

## Penalized-Constrained Regression (PCReg) Model

The Penalized-Constrained Regression (PCReg) Model integrates regularization penalties (L1 and L2) with domain-knowledge box-constraints ($\theta_{\text{lower}} \leq \theta \leq \theta_{\text{upper}}$) to stabilize coefficient estimates, while ensuring they remain within plausible ranges. 

### Objective Function

The objective function minimized by PCReg combines a loss function with regularization penalties subject to coefficient bounds:

$$
\begin{aligned}
\text{Objective:} \quad & \underset{\theta}{\arg\min} \; \underbrace{\sum_{i=1}^{n} L\!\left(y_i, \hat{y}_i\right)}_{\text{Loss}} \;+\; \underbrace{\lambda \Big[\, \alpha \, \|\theta\|_1 \;+\; \frac{1-\alpha}{2} \, \|\theta\|_2^2 \,\Big]}_{\text{Elastic Net Penalty}} \\[10pt]
\text{Subject to:} \quad & \theta_{\text{lower}} \le \theta \le \theta_{\text{upper}} \quad \text{(componentwise bounds)} \\[12pt]
\text{where:} \quad & L(y_i, \hat{y}_i) = \text{loss function (e.g., SSE, SSPE)} \\
& \lambda \ge 0 = \text{regularization strength} \\
& \alpha \in [0,1] = \text{L1/L2 mixing parameter} \\
& \|\theta\|_1 = \sum_j |\theta_j| \quad \text{(Lasso penalty)} \\
& \|\theta\|_2^2 = \sum_j \theta_j^2 \quad \text{(Ridge penalty)}
\end{aligned}
$$

The lower and upper bounds ($\theta_{lower}$ and $\theta_{upper}$) can be set to achieve loose bounds or tight bounds for the coefficient:

-   **Loose bounds** (e.g., 70-100%): Wide range which allows flexibility while preventing egregious coefficient violations
-   **Tight bounds** (e.g., 85-95%): Narrower range which incorporates strong prior knowledge but risks over-constraining coefficients

### Model Specification
The users can specify the following components of the PCReg model:
- **Prediction function**: Defaults to linear (`X @ coef + intercept`), but supports custom non-linear functions
- **Loss function**: Sum of Squared Percentage Errors (SSPE, default), Sum of Squared Errors (SSE), Mean Squared Error (MSE), or custom loss functions
- **Coefficient bounds**: Componentwise constraints that encode domain knowledge (e.g., learning curve slopes between -1 and 0)
- **Penalty exclusion**: Option to exclude specific coefficients from regularization while still applying bounds
- **Optimizer**: Defaults to SLSQP for constrained optimization


The framework maximizes flexibility and collapses to known regression techniques in special cases:
- OLS when prediction function is linear, `alpha=0`, and bounds are not specified
- Ridge when prediction function is linear, `alpha>0` and l1_ratio=0
- Lasso when prediction function is linear, `alpha>1` and l1_ratio=1
- Elastic Net when prediction function is linear, `alpha>0` and `0 < l1_ratio < 1`
- Constrained regression when bounds are specified but `alpha=0`

### Cross Validation (CV) for Hyperparameter Selection

Finding the optimal penalty parameters for penalized-constrained regression requires testing different values rather than using a formula. Cross Validation (CV) is a method that evaluates how well a model predicts new data by testing it on observations not used during training. The CV process evaluates all combinations of user-specified penalties ($\lambda$) and L1 ratio values ($\alpha$) to find the hyperparameters that produce the best out-of-sample predictions.

Traditional K-Fold CV splits the training data into K subsets, repeatedly training on K-1 subsets and validating on the remaining one. For small datasets, this approach further reduces the already limited training sample size.

We implement both K-Fold CV and Generalized Cross Validation (GCV), with GCV as the default for small datasets. GCV approximates leave-one-out cross validation using a mathematical formula rather than actually splitting data [@golub1979generalized]. It estimates prediction error based on training residuals adjusted by model complexity (degrees of freedom), making it efficient for small samples. For each combination of penalty parameters, the GCV score is calculated as:

$$
\begin{aligned}
\text{GCV} &= \frac{n \cdot \text{RSS}}{(n - \text{df})^2} \\[6pt]
\text{where:} \quad & \text{RSS} = \text{residual sum of squared errors} \\
& n = \text{number of observations} \\
& \text{df} = \text{effective degrees of freedom}
\end{aligned}
$$

The optimal penalty parameters are those that minimize the GCV score.

## Model Diagnostics

The flexibility of penalized-constrained regression means goodness of fit statitics cannot gurantee classical inference (p-values and F-statistic) apply due to supporting coefficient bounds, regularization penalties, custom loss functions and custom prediction functions. No single inference framework covers all scenarios. When coefficients are at bounds (from constraints or L1 penalties), classical theory breaks down. Additionally, introducing custom loss and prediction functions require specifiying their corresponding distributional assumptions as in Generalized Linear Models.  We therefore focus on fit statistics that are robust and informative across model specifications:

### In-Sample Fit Statistics
Standard in-sample fit measures remain valid across all model configurations, including R² (coefficient of determination), SSE (sum of squared errors), RMSE (root mean squared error), MAE (mean absolute error), MAPE (mean absolute percentage error), and residual diagnostics.


### Generalized Degrees of Freedom (GDF)-adjusted fit statistics
Classical degrees of freedom ($df=n - p$) assumes unconstrained estimation. GDF suggests that optimization based models ought to lose degrees of freedom to take into account the constraints. We implement both @gaines2018constrained and @hu2010gdf GDF formulas to properly adjust fit statistics (SEE, SPE, Adjusted R²) and account for the effective model complexity.

**Gaines et al. (2018) Formula (Default):** @gaines2018constrained derive degrees of freedom for constrained Lasso by counting only constraints that are actually binding at the solution:

$$
\text{GDF}_{\text{Gaines}} = n - |\text{Active predictors}| - |\text{Equality constraints}| - |\text{Binding inequality constraints}|
$$

where $n$ is the sample size and active predictors are non-zero coefficients. This reflects the intuition that non-binding constraints don't reduce effective degrees of freedom.

**Hu (2010) Alternative Formula:** @hu2010gdf proposes a more conservative approach where *all specified constraints* count against degrees of freedom, regardless of whether they bind:

$$
\text{GDF}_{\text{Hu}} = n - p - (\text{\# Constraints}) + (\text{\# Redundancies})
$$

where $p$ is the total number of estimated parameters (including intercept if fitted). Hu's method always reduces GDF when constraints are specified, making it appropriate for ZMPE-type CERs where constraints fundamentally shape the solution.

### Bootstrap Confidence Intervals
Standard bootstrap confidence intervals can be misleading for penalized regression [@goeman2025penalized], as they assume interior solutions and no regularization bias. Our framework addresses this by computing **both** constrained and unconstrained bootstrap resampling:

- **Constrained bootstrap**: Resamples data and refits the model *with* the specified bounds and regularization (penalized-constrained estimation), quantifying coefficient stability under the imposed modeling constraints

- **Unconstrained bootstrap**: Resamples data and fits OLS *without* bounds or penalization (pure unconstrained estimation), showing coefficient variability in the absence of constraints and regularization. This serves as a baseline for comparison.


### Software Implementation

#### Python Package
We developed a full feature complete Python package, `penalized_constrained`, following scikit-learn conventions and API design principles. The package provides a comprehensive implementation of the PCReg framework with support for model fitting, hyperparameter selection via cross-validation, diagnostic reporting with GDF-adjusted fit statistics, and bootstrap confidence intervals. See the Appendix for detailed documentation and usage examples.

#### Excel Template
To make the methodology accessible to practitioners without programming expertise, we developed an Excel template that implements a simplified version of the PCReg algorithm. This template provides an intuitive interface for applying penalized-constrained regression to cost estimation problems.

# Example Scenario Revisited with PCReg

Given the framework for PCReg we can now model the same motivational example but now using **Penalized-Constrained Regression with Generalized Cross-Validation** (PCReg_GCV). This specification combines the theoretical foundations with practical hyperparameter selection.

## PCReg_GCV Model Specification

The PCReg_GCV specification applies the framework to our learning curve problem with the following configuration:

```{python}
#| label: pcreg-gcv-spec-revisited
#| echo: true
#| eval: false

# Define the nonlinear prediction function
def prediction_fn(X, params):
    """Learning curve with rate effect: T1 * (midpoint^b) * (quantity^c)"""
    T1, b, c = params
    return T1 * (X[:, 0] ** b) * (X[:, 1] ** c)

# Configure PCReg_GCV model
pc_gcv = pcreg.PenalizedConstrainedCV(
    coef_names=['T1', 'b', 'c'],
    bounds={'T1': (0, None), 'b': (-0.5, 0), 'c': (-0.5, 0)},
    prediction_fn=prediction_fn,
    fit_intercept=False,
    x0=[100, -0.1, -0.1],
    alphas=np.linspace(0, 5, 10),  # Range of lambda values to test
    l1_ratios= [0.0, 0.5, 1.0],  # Test Ridge, Elastic Net, and Lasso penalties
    selection='gcv',           # Generalized Cross-Validation
    loss='sspe',               # Sum of Squared Percentage Errors
    penalty_exclude=['T1'],    # Don't penalize the intercept
    n_jobs=1
)

# Fit the model
pc_gcv.fit(X_train, y_train)
```

**Key Specification Details:**
***note*** Unfortunately, due to scikit-learn API design, we relabel $\lambda$ as `alpha` and $\alpha$ as `l1_ratio` in the code to fit within the Elastic Net framework, but they correspond to the regularization strength and L1 ratio respectively in the theoretical formulation.:

- **Coefficients**: $T_1$ (first unit cost), $b$ (learning slope), $c$ (rate slope)
- **Bounds**: $T_1 > 0$, $-0.5 \leq b \leq 0$, $-0.5 \leq c \leq 0$
- **alphas (penalty)**: Range between 0 and 5 using linear spacing to explore Lasso, Ridge, and Elastic Net penalties
- **l1_ratios**: Range between 0 and 1 to explore different mixes of L1 and L2 regularization
- **Penalty Selection**: GCV automatically selects optimal $\lambda$ without data splitting
- **Loss Function**: SSPE minimizes percentage errors (appropriate for cost data)
- **Penalty Exclusion**: $T_1$ is not penalized (only slopes are regularized)


| Metric        |                             OLS |                       PCReg_GCV |
|:--------------|--------------------------------:|--------------------------------:|
| $T_1$         |      `{python} f'{ols_T1:.0f}'` |      `{python} f'{pc_T1:.0f}'` |
| Learning Rate | `{python} f'{ols_lr*100:.1f}'`% | `{python} f'{pc_lr*100:.1f}'`% |
| Rate Effect   | `{python} f'{ols_re*100:.1f}'`% | `{python} f'{pc_re*100:.1f}'`% |
| R² (Train)     |      `{python} f'{ols_r2_train:.2f}'` |      `{python} f'{pc_r2_train:.2f}'` |



: Comparison of OLS and PCReg_GCV coefficient estimates {#tbl-ols-pcreg-comparison}

| Specification        |                             OLS |                       PCReg_GCV |
|:--------------|--------------------------------:|--------------------------------:|
| loss function   |                                 SSE | SSPE |
| prediction function   |                                 Linear | Nonlinear (Learning Curve) |
| lambda (penalty) |                                 N/A | `{python} f'{pc_gcv.alpha_:.4f}'` |
| alpha (L1 ratio) |                                 N/A | `{python} f'{pc_gcv.l1_ratio_:.2f}'` |
: Comparison of OLS and PCReg_GCV coefficient Specification {#tbl-ols-pcreg-comparison}


## Model Diagnostics
We were able to find a reasonable solution with PCReg_GCV that satisfies our domain knowledge constraints (LCS and Rate Effect $\leq 100\%$) and we still have a reasonble (albeit worse) R². But how stable are the coefficients. The PCReg_GCV model includes bootstrap diagnostics to assess coefficient stability and the impact of constraints. Comparing constrained and unconstrained bootstrap distributions reveals the impact of constraints on coefficient stability. Large divergence indicates constraints are actively shaping the solution, while similar distributions suggest the data naturally satisfies the constraints. 


```{python}
#| label: model-diagnostics

# Generate diagnostic report with bootstrap
report = pcreg.generate_summary_report(
    pc_gcv, X_train, y_train,
    full=True,
    gdf_method='gaines',
    bootstrap=True,
    n_bootstrap=500,
    random_state=42
)

# Extract key diagnostic values
gdf = report.fit_stats.gdf
gdf_method = report.fit_stats.gdf_method
n_active = report.constraints.n_active
active_constraints = report.constraints.active_constraints

# Bootstrap results
boot = report.bootstrap_results
boot_const = boot.constrained if boot else None
boot_unconst = boot.unconstrained if boot else None
```

**Bootstrap Results Summary**

```{python}
#| label: bootstrap-summary

if boot_const is not None and boot_unconst is not None:
    coef_names = ['T1', 'b', 'c']
    # Constrained bootstrap stats
    const_means = boot_const.coef_mean
    const_stds = boot_const.coef_std
    const_ranges = [(boot_const.coef_ci_lower[i], boot_const.coef_ci_upper[i]) for i in range(3)]
    # Unconstrained bootstrap stats
    unconst_means = boot_unconst.coef_mean
    unconst_stds = boot_unconst.coef_std
    unconst_ranges = [(boot_unconst.coef_ci_lower[i], boot_unconst.coef_ci_upper[i]) for i in range(3)]
```

**Constrained Bootstrap** (with bounds and regularization):

-   **T1**: Mean = `{python} f'{const_means[0]:.2f}'`, Std = `{python} f'{const_stds[0]:.2f}'`, 95% CI = \[`{python} f'{const_ranges[0][0]:.2f}'`, `{python} f'{const_ranges[0][1]:.2f}'`\]
-   **b**: Mean = `{python} f'{const_means[1]:.4f}'`, Std = `{python} f'{const_stds[1]:.4f}'`, 95% CI = \[`{python} f'{const_ranges[1][0]:.4f}'`, `{python} f'{const_ranges[1][1]:.4f}'`\]
-   **c**: Mean = `{python} f'{const_means[2]:.4f}'`, Std = `{python} f'{const_stds[2]:.4f}'`, 95% CI = \[`{python} f'{const_ranges[2][0]:.4f}'`, `{python} f'{const_ranges[2][1]:.4f}'`\]

**Unconstrained Bootstrap** (no bounds, alpha=0):

-   **T1**: Mean = `{python} f'{unconst_means[0]:.2f}'`, Std = `{python} f'{unconst_stds[0]:.2f}'`, 95% CI = \[`{python} f'{unconst_ranges[0][0]:.2f}'`, `{python} f'{unconst_ranges[0][1]:.2f}'`\]
-   **b**: Mean = `{python} f'{unconst_means[1]:.4f}'`, Std = `{python} f'{unconst_stds[1]:.4f}'`, 95% CI = \[`{python} f'{unconst_ranges[1][0]:.4f}'`, `{python} f'{unconst_ranges[1][1]:.4f}'`\]
-   **c**: Mean = `{python} f'{unconst_means[2]:.4f}'`, Std = `{python} f'{unconst_stds[2]:.4f}'`, 95% CI = \[`{python} f'{unconst_ranges[2][0]:.4f}'`, `{python} f'{unconst_ranges[2][1]:.4f}'`\]

```{python}
#| label: fig-bootstrap-kde
#| fig-cap: "Bootstrap coefficient distributions comparing constrained (blue) vs unconstrained (red) estimation. Vertical lines show fitted values. Constraints reduce variance and keep estimates within economically plausible ranges."

if boot_const is not None and boot_unconst is not None:
    fig, axes = plt.subplots(1, 3, figsize=(14, 4))
    coef_names = ['T1', 'b', 'c']
    fitted_vals = pc_gcv.coef_

    for i, (ax, name) in enumerate(zip(axes, coef_names)):
        const_samples = boot_const.bootstrap_coefs[:, i]
        unconst_samples = boot_unconst.bootstrap_coefs[:, i]

        # Plot KDEs
        sns.kdeplot(const_samples, ax=ax, label='Constrained', color='#3498db', fill=True, alpha=0.4)
        sns.kdeplot(unconst_samples, ax=ax, label='Unconstrained', color='#e74c3c', fill=True, alpha=0.3)

        # Add fitted value line
        ax.axvline(fitted_vals[i], color='black', linestyle='--', linewidth=2, label=f'Fitted={fitted_vals[i]:.4f}')

        ax.set_xlabel(name)
        ax.set_ylabel('Density')
        ax.set_title(f'{name} Bootstrap Distribution')
        ax.legend(fontsize=8)

    plt.tight_layout()
    plt.show()
```

**Key Diagnostic Insights:**

1.  **Constraints at bounds**: When bootstrap samples frequently hit constraint boundaries, the data is "pulling" towards implausible values---exactly when constraints help most.

2.  **Variance reduction**: Constrained bootstrap typically shows tighter distributions, reducing coefficient uncertainty at the cost of some bias.

3.  **Divergence indicates constraint impact**: Large differences between constrained and unconstrained means show the constraints are actively shaping the solution.

```{python}
#| label: save-diagnostic-report
#| output: false

# Save HTML diagnostic report for interactive exploration
_ = report.to_html('pcreg_gcv_diagnostic_report.html', X=X_train, y=y_train)
```

A comprehensive diagnostic report with full model specifications and bootstrap distributions is provided in Appendix A. An interactive HTML version has also been saved to `pcreg_gcv_diagnostic_report.html` for detailed exploration.

\newpage

# Simulation Study

```{python}
#| label: simulation-design-params

# Extract unique factor levels from results
n_lots_levels = sorted(results['n_lots'].unique())
cv_error_levels = sorted(results['cv_error'].unique())
learning_rate_levels = sorted(results['learning_rate'].unique())
rate_effect_levels = sorted(results['rate_effect'].unique())

# Calculate simulation size
n_combinations = len(n_lots_levels) * len(cv_error_levels) * len(learning_rate_levels) * len(rate_effect_levels)
n_replications = len(results[results['model_name'] == 'OLS']) // n_combinations
n_scenarios = n_combinations * n_replications

# Format levels for display
n_lots_str = ', '.join(map(str, n_lots_levels))
cv_error_str = ', '.join(map(str, cv_error_levels))
learning_rate_str = ', '.join([f'{lr*100:.0f}%' for lr in learning_rate_levels])
rate_effect_str = ', '.join([f'{re*100:.0f}%' for re in rate_effect_levels])
```

We performed a Monte Carlo simulation study evaluate PCReg and compare with traditional OLS.

## Siumulation Design

| Factor               | Levels                       |
|:---------------------|:-----------------------------|
| Sample size (n_lots) | `{python} n_lots_str`        |
| CV error             | `{python} cv_error_str`      |
| Learning rate        | `{python} learning_rate_str` |
| Rate effect          | `{python} rate_effect_str`   |

: Simulation study factorial design. `{python} n_combinations` combinations × `{python} n_replications` replications = `{python} f'{n_scenarios:,}'` scenarios. {#tbl-design}

For each scenario, we:

1.  **Randomly selected a quantity profile** from the SAR database (actual defense program procurement histories)
2.  **Generated simulated average unit costs** using the learning curve model (@eq-learning-curve) with the scenario's true parameters
3.  **Added multiplicative lognormal noise** with the specified coefficient of variation (CV)
4.  **Split data**: First $n$ lots for training, remaining lots for test

This approach ensures realistic lot structures (varying quantities, realistic ramp-up patterns) while controlling the true underlying parameters. Note that the number of test lots varies by scenario depending on the program's total lot history. Some programs have many available lots beyond training, others have few or none. This variability is acceptable as it reflects real-world conditions.

## Example Scenario

We return to the example scenario which was introduce in [#tbl-design]. Below we see the scenario specifications which generated our example scenario dataset.

| Parameter             |                            Value |
|:----------------------|---------------------------------:|
| Training lots         |               `{python} n_train` |
| Test lots             |                `{python} n_test` |
| True $T_1$            |      `{python} f'{true_T1:.0f}'` |
| True Learning Rate    | `{python} f'{true_lr*100:.1f}'`% |
| True Rate Effect      | `{python} f'{true_re*100:.1f}'`% |
| Predictor Correlation |         `{python} f'{corr:.2f}'` |
| CV Error              |         `{python} f'{cv_error}'` |

: Example scenario parameters {#tbl-scenario}

Here is the example dataset with training and test lots (training data matches the dataset shown earlier in [#tbl-design], but now we also see the test lots with their true underlying costs):

```{python}
#| label: tbl-scenario-example-simulation
#| tbl-cap: "Motivating example dataset with training and test lots."
#| echo: false
#| output: asis

from tabulate import tabulate

# Choose and format the columns you want to show
cols = ["lot_type", "lot_midpoint", "lot_quantity", "observed_cost", "true_cost"]
df_show = df_example[cols].copy()

# Format values
train_mask = df_show['lot_type'] == 'train'

# Convert observed_cost to object type to allow mixed string/numeric values
df_show['observed_cost'] = df_show['observed_cost'].astype(object)

# Hide observed costs for test rows
df_show.loc[~train_mask, 'observed_cost'] = '—'

# Format numeric columns
for col in ['lot_midpoint', 'lot_quantity', 'true_cost']:
    df_show[col] = df_show[col].apply(lambda x: f"{x:.3f}" if isinstance(x, float) else str(x))

# Format observed_cost only for training rows (test rows already have '—')
df_show.loc[train_mask, 'observed_cost'] = df_show.loc[train_mask, 'observed_cost'].apply(
    lambda x: f"{x:.3f}" if isinstance(x, float) else str(x)
)

md = tabulate(
    df_show,
    headers="keys",
    tablefmt="pipe",
    showindex=False,
)

print(md)
```

```{python}
#| label: fig-data
#| fig-cap: "Learning curve data with `python n_train` training lots (blue) and the true underlying cost distribution (dashed line). Point sizes reflect lot quantities. The goal is to predict the true relationship, not just fit the noisy observations."

fig, ax = plt.subplots(figsize=(8, 5))

# Plot true underlying curve
ax.plot(lot_mp_range, true_curve, 'k--', linewidth=2, label='True Cost Function', zorder=1)

# Plot training data with size proportional to lot quantity
train_sizes = (df_train['lot_quantity'] / df_train['lot_quantity'].max()) * 200 + 50
ax.scatter(df_train['lot_midpoint'], df_train['observed_cost'],
           s=train_sizes, c='#1f77b4', alpha=0.8, edgecolors='black',
           label=f'Training (n={n_train})', zorder=3)

# Plot test data with size proportional to lot quantity
test_sizes = (df_test['lot_quantity'] / df_test['lot_quantity'].max()) * 200 + 50
ax.scatter(df_test['lot_midpoint'], df_test['true_cost'],
           s=test_sizes, c='#ff7f0e', alpha=0.6, edgecolors='black',
           label=f'True Test Values (n={n_test})', zorder=2)

ax.set_xlabel('Lot Midpoint (Cumulative Units)')
ax.set_ylabel('Cost ($)')
ax.set_title('Learning Curve: Training Data vs True Distribution')
ax.legend()

# Standard units, starting near 0
ax.set_xlim(0, df_example['lot_midpoint'].max() * 1.1)
ax.set_ylim(0, df_example['observed_cost'].max() * 1.2)

plt.tight_layout()
plt.show()
```

```{python}
#| label: model-fits-example

from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer
from sklearn.compose import TransformedTargetRegressor
from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_squared_error
import penalized_constrained as pcreg

X_train = df_train[['lot_midpoint', 'lot_quantity']].values
y_train = df_train['observed_cost'].values
X_test = df_test[['lot_midpoint', 'lot_quantity']].values
y_test_true = df_test['true_cost'].values  # True underlying values
y_test_obs = df_test['observed_cost'].values  # Noisy observations

# === OLS ===
ols = TransformedTargetRegressor(
    regressor=Pipeline([
        ('log', FunctionTransformer(np.log)),
        ('reg', LinearRegression()),
    ]),
    func=np.log,
    inverse_func=np.exp,
)
ols.fit(X_train, y_train)

ols_b, ols_c = ols.regressor_.named_steps['reg'].coef_
ols_lr = 2 ** ols_b
ols_re = 2 ** ols_c
ols_T1 = np.exp(ols.regressor_.named_steps['reg'].intercept_)
ols_pred_train = ols.predict(X_train)
ols_pred_test = ols.predict(X_test)
ols_r2_train = r2_score(y_train, ols_pred_train)
ols_mape_true = mean_absolute_percentage_error(y_test_true, ols_pred_test)
ols_resid_train = y_train - ols_pred_train
ols_resid_true = y_test_true - ols_pred_test

# === OLS Learn Only ===
X_train_learn = df_train[['lot_midpoint']].values
X_test_learn = df_test[['lot_midpoint']].values

ols_learn = TransformedTargetRegressor(
    regressor=Pipeline([
        ('log', FunctionTransformer(np.log)),
        ('reg', LinearRegression()),
    ]),
    func=np.log,
    inverse_func=np.exp,
)
ols_learn.fit(X_train_learn, y_train)

ols_learn_b = ols_learn.regressor_.named_steps['reg'].coef_[0]
ols_learn_lr = 2 ** ols_learn_b
ols_learn_T1 = np.exp(ols_learn.regressor_.named_steps['reg'].intercept_)
ols_learn_pred_train = ols_learn.predict(X_train_learn)
ols_learn_pred_test = ols_learn.predict(X_test_learn)
ols_learn_r2_train = r2_score(y_train, ols_learn_pred_train)
ols_learn_mape_true = mean_absolute_percentage_error(y_test_true, ols_learn_pred_test)
ols_learn_resid_train = y_train - ols_learn_pred_train
ols_learn_resid_true = y_test_true - ols_learn_pred_test

# === PCReg ===
def prediction_fn(X, params):
    T1, b, c = params
    return T1 * (X[:, 0] ** b) * (X[:, 1] ** c)

pc_gcv = pcreg.PenalizedConstrainedCV(
    coef_names=['T1', 'b', 'c'],
    bounds={'T1': (0, None), 'b': (-0.5, 0), 'c': (-0.5, 0)},
    prediction_fn=prediction_fn,
    fit_intercept=False,
    x0=[100, -0.1, -0.1],
    selection='gcv',
    loss='sspe',
    penalty_exclude=['T1'],
    n_jobs=1
)
pc_gcv.fit(X_train, y_train)

pc_T1, pc_b, pc_c = pc_gcv.coef_
pc_lr = 2 ** pc_b
pc_re = 2 ** pc_c
pc_pred_train = pc_gcv.predict(X_train)
pc_pred_test = pc_gcv.predict(X_test)
pc_r2_train = r2_score(y_train, pc_pred_train)
pc_mape_true = mean_absolute_percentage_error(y_test_true, pc_pred_test)
pc_resid_train = y_train - pc_pred_train
pc_resid_true = y_test_true - pc_pred_test
```

| Metric | True | OLS | OLS-LearnOnly | PCReg |
|:--------------|--------------:|--------------:|--------------:|--------------:|
| $T_1$ | `{python} f'{true_T1:.0f}'` | `{python} f'{ols_T1:.0f}'` | `{python} f'{ols_learn_T1:.0f}'` | `{python} f'{pc_T1:.0f}'` |
| Learning Rate | `{python} f'{true_lr*100:.1f}'`% | `{python} f'{ols_lr*100:.1f}'`% | `{python} f'{ols_learn_lr*100:.1f}'`% | `{python} f'{pc_lr*100:.1f}'`% |
| Rate Effect | `{python} f'{true_re*100:.1f}'`% | `{python} f'{ols_re*100:.1f}'`% | -- | `{python} f'{pc_re*100:.1f}'`% |
| Valid Coefficients | Yes | `{python} 'NO' if ols_lr > 1 or ols_re < 0.7 or ols_re > 1 else 'Yes'` | `{python} 'NO' if ols_learn_lr > 1 else 'Yes'` | Yes |
| Train R² | -- | `{python} f'{ols_r2_train:.3f}'` | `{python} f'{ols_learn_r2_train:.3f}'` | `{python} f'{pc_r2_train:.3f}'` |
| Test MAPE (vs True) | -- | `{python} f'{ols_mape_true*100:.1f}'`% | `{python} f'{ols_learn_mape_true*100:.1f}'`% | `{python} f'{pc_mape_true*100:.1f}'`% |

: Comparison of estimation methods. PCReg has lower Train R² due to added bias from constraints, but better Test MAPE when predicting the true underlying relationship. {#tbl-comparison}

**Key Insight**: OLS achieves a *lower* Train R² OLS (`{python} f'{ols_r2_train:.3f}'`) than PCReg (`{python} f'{pc_r2_train:.3f}'`). This is expected, penalties and constraints add bias to the training fit. However, this bias *improves* out-of-sample prediction, as shown by the lower Test MAPE against the true distribution.

```{python}
#| label: fig-residuals
#| fig-cap: "Residuals for each model on training data (left) and against true test values (right). PCReg shows larger training residuals but smaller errors when predicting the true underlying relationship."

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

models = ['OLS', 'OLS-LearnOnly', 'PCReg']
x_pos = np.arange(len(models))
colors = ['#e74c3c', '#9b59b6', '#3498db']

# Training residuals scatter
ax1 = axes[0]
train_residuals = [ols_resid_train, ols_learn_resid_train, pc_resid_train]
for i, (resid, color) in enumerate(zip(train_residuals, colors)):
    jitter = np.random.normal(0, 0.05, size=len(resid))
    ax1.scatter(np.full_like(resid, x_pos[i]) + jitter, resid, color=color, alpha=0.7, edgecolors='black', s=40)
ax1.axhline(0, color='black', linestyle='--', linewidth=1)
ax1.set_xticks(x_pos)
ax1.set_xticklabels(models)
ax1.set_ylabel('Residual ($)')
ax1.set_title('Training Residuals')

# Test residuals scatter (vs true)
ax2 = axes[1]
test_residuals = [ols_resid_true, ols_learn_resid_true, pc_resid_true]
for i, (resid, color) in enumerate(zip(test_residuals, colors)):
    jitter = np.random.normal(0, 0.05, size=len(resid))
    ax2.scatter(np.full_like(resid, x_pos[i]) + jitter, resid, color=color, alpha=0.7, edgecolors='black', s=40)
ax2.axhline(0, color='black', linestyle='--', linewidth=1)
ax2.set_xticks(x_pos)
ax2.set_xticklabels(models)
ax2.set_ylabel('Residual ($)')
ax2.set_title('Test Residuals vs True Distribution')

plt.tight_layout()
plt.show()
```

\newpage

# Key Findings

```{python}
#| label: analysis-setup

from scipy import stats
from tabulate import tabulate

# Create comparison dataframe for OLS vs PCReg
ols_df = results[results['model_name'] == 'OLS'][['seed', 'test_mape', 'test_sspe', 'ols_reasonable',
                                                    'n_lots', 'cv_error', 'learning_rate', 'rate_effect',
                                                    'actual_correlation', 'T1_est', 'b', 'c',
                                                    'T1_true', 'b_true', 'c_true']].copy()
ols_df = ols_df.rename(columns={'test_mape': 'ols_mape', 'test_sspe': 'ols_sspe',
                                 'T1_est': 'ols_T1', 'b': 'ols_b', 'c': 'ols_c'})

pcreg_df = results[results['model_name'] == 'PCReg_GCV'][['seed', 'test_mape', 'test_sspe',
                                                          'T1_est', 'b', 'c']].copy()
pcreg_df = pcreg_df.rename(columns={'test_mape': 'pcreg_mape', 'test_sspe': 'pcreg_sspe',
                                     'T1_est': 'pcreg_T1', 'b': 'pcreg_b', 'c': 'pcreg_c'})

ols_learn_df = results[results['model_name'] == 'OLS_LearnOnly'][['seed', 'test_mape', 'test_sspe']].copy()
ols_learn_df = ols_learn_df.rename(columns={'test_mape': 'ols_learn_mape', 'test_sspe': 'ols_learn_sspe'})

# Merge all
comparison = ols_df.merge(pcreg_df, on='seed').merge(ols_learn_df, on='seed')

# Calculate Learning Curve (LC) and Rate Curve (RC) estimates (slope scale: 2^b)
comparison['ols_LC'] = 2 ** comparison['ols_b']
comparison['ols_RC'] = 2 ** comparison['ols_c']
comparison['pcreg_LC'] = 2 ** comparison['pcreg_b']
comparison['pcreg_RC'] = 2 ** comparison['pcreg_c']
comparison['LC_true'] = 2 ** comparison['b_true']
comparison['RC_true'] = 2 ** comparison['c_true']

# Calculate absolute errors in slope scale (LC/RC)
comparison['ols_T1_ape'] = np.abs(comparison['ols_T1'] - comparison['T1_true']) / comparison['T1_true']
comparison['pcreg_T1_ape'] = np.abs(comparison['pcreg_T1'] - comparison['T1_true']) / comparison['T1_true']
comparison['ols_LC_ae'] = np.abs(comparison['ols_LC'] - comparison['LC_true'])
comparison['pcreg_LC_ae'] = np.abs(comparison['pcreg_LC'] - comparison['LC_true'])
comparison['ols_RC_ae'] = np.abs(comparison['ols_RC'] - comparison['RC_true'])
comparison['pcreg_RC_ae'] = np.abs(comparison['pcreg_RC'] - comparison['RC_true'])

# Calculate win indicators
comparison['pcreg_wins_mape'] = comparison['pcreg_mape'] < comparison['ols_mape']
comparison['pcreg_wins_sspe'] = comparison['pcreg_sspe'] < comparison['ols_sspe']

# Win rates
overall_win_mape = comparison['pcreg_wins_mape'].mean() * 100
win_reasonable_mape = comparison[comparison['ols_reasonable'] == True]['pcreg_wins_mape'].mean() * 100
win_unreasonable_mape = comparison[comparison['ols_reasonable'] == False]['pcreg_wins_mape'].mean() * 100
n_reasonable = comparison['ols_reasonable'].sum()
n_unreasonable = (~comparison['ols_reasonable']).sum()
pct_unreasonable = n_unreasonable / len(comparison) * 100
```

Across `{python} f'{len(comparison):,}'` simulation scenarios, OLS produced economically unreasonable coefficients (learning curve or rate effect outside 70-100%) in **`{python} f'{pct_unreasonable:.1f}'`%** of cases (`{python} f'{n_unreasonable:,}'` scenarios).

## PCReg Significantly Outperforms OLS When Coefficients Are Unreasonable

When OLS produces unreasonable coefficients, PCReg wins **`{python} f'{win_unreasonable_mape:.1f}'`%** of scenarios on Test MAPE.

```{python}
#| label: unreasonable-analysis
#| output: asis

# Filter to unreasonable scenarios
unreasonable = comparison[comparison['ols_reasonable'] == False].copy()

# Statistical tests for unreasonable scenarios
wilcox_mape_unreas = stats.wilcoxon(unreasonable['ols_mape'], unreasonable['pcreg_mape'], alternative='greater')
ttest_mape_unreas = stats.ttest_rel(unreasonable['ols_mape'], unreasonable['pcreg_mape'], alternative='greater')

# Build comparison table for unreasonable scenarios
metrics_unreas = {
    'Metric': ['Test MAPE', 'T1 APE', 'LC Abs Error', 'RC Abs Error'],
    'OLS Mean': [
        unreasonable['ols_mape'].mean(),
        unreasonable['ols_T1_ape'].mean(),
        unreasonable['ols_LC_ae'].mean(),
        unreasonable['ols_RC_ae'].mean()
    ],
    'PCReg Mean': [
        unreasonable['pcreg_mape'].mean(),
        unreasonable['pcreg_T1_ape'].mean(),
        unreasonable['pcreg_LC_ae'].mean(),
        unreasonable['pcreg_RC_ae'].mean()
    ],
    'OLS Median': [
        unreasonable['ols_mape'].median(),
        unreasonable['ols_T1_ape'].median(),
        unreasonable['ols_LC_ae'].median(),
        unreasonable['ols_RC_ae'].median()
    ],
    'PCReg Median': [
        unreasonable['pcreg_mape'].median(),
        unreasonable['pcreg_T1_ape'].median(),
        unreasonable['pcreg_LC_ae'].median(),
        unreasonable['pcreg_RC_ae'].median()
    ]
}
df_unreas = pd.DataFrame(metrics_unreas)

# Format for display
df_unreas_display = df_unreas.copy()
for col in ['OLS Mean', 'PCReg Mean', 'OLS Median', 'PCReg Median']:
    df_unreas_display[col] = df_unreas_display[col].apply(lambda x: f'{x:.4f}')

print(tabulate(df_unreas_display, headers='keys', tablefmt='pipe', showindex=False))
```

: Performance comparison when OLS produces unreasonable coefficients (n=`{python} f'{n_unreasonable:,}'`). Lower values are better. {#tbl-unreasonable}

**Statistical Significance**: Wilcoxon signed-rank test confirms PCReg significantly outperforms OLS on Test MAPE (p \< `{python} f'{wilcox_mape_unreas.pvalue:.2e}'`).

## PCReg Performs Comparably When OLS Coefficients Are Reasonable

When OLS produces reasonable coefficients, OLS "wins" on Test MAPE in **`{python} f'{100 - win_reasonable_mape:.1f}'`%** of scenarios. However, the performance differences are negligible in practical terms.

```{python}
#| label: reasonable-analysis
#| output: asis

# Filter to reasonable scenarios
reasonable = comparison[comparison['ols_reasonable'] == True].copy()

# Statistical tests for reasonable scenarios
wilcox_mape_reas = stats.wilcoxon(reasonable['ols_mape'], reasonable['pcreg_mape'])
ttest_mape_reas = stats.ttest_rel(reasonable['ols_mape'], reasonable['pcreg_mape'])

# Build comparison table for reasonable scenarios
metrics_reas = {
    'Metric': ['Test MAPE', 'T1 APE', 'LC Abs Error', 'RC Abs Error'],
    'OLS Mean': [
        reasonable['ols_mape'].mean(),
        reasonable['ols_T1_ape'].mean(),
        reasonable['ols_LC_ae'].mean(),
        reasonable['ols_RC_ae'].mean()
    ],
    'PCReg Mean': [
        reasonable['pcreg_mape'].mean(),
        reasonable['pcreg_T1_ape'].mean(),
        reasonable['pcreg_LC_ae'].mean(),
        reasonable['pcreg_RC_ae'].mean()
    ],
    'OLS Median': [
        reasonable['ols_mape'].median(),
        reasonable['ols_T1_ape'].median(),
        reasonable['ols_LC_ae'].median(),
        reasonable['ols_RC_ae'].median()
    ],
    'PCReg Median': [
        reasonable['pcreg_mape'].median(),
        reasonable['pcreg_T1_ape'].median(),
        reasonable['pcreg_LC_ae'].median(),
        reasonable['pcreg_RC_ae'].median()
    ]
}
df_reas = pd.DataFrame(metrics_reas)

# Format for display
df_reas_display = df_reas.copy()
for col in ['OLS Mean', 'PCReg Mean', 'OLS Median', 'PCReg Median']:
    df_reas_display[col] = df_reas_display[col].apply(lambda x: f'{x:.4f}')

print(tabulate(df_reas_display, headers='keys', tablefmt='pipe', showindex=False))
```

: Performance comparison when OLS produces reasonable coefficients (n=`{python} f'{n_reasonable:,}'`). {#tbl-reasonable}

```{python}
#| label: reasonable-diff-calc

# Calculate practical differences
mape_diff_reas = (reasonable['pcreg_mape'].mean() - reasonable['ols_mape'].mean()) * 100
mape_diff_reas_pct = mape_diff_reas / (reasonable['ols_mape'].mean() * 100) * 100
```

**Key Insight**: When OLS coefficients are reasonable, the mean Test MAPE difference is only **`{python} f'{abs(mape_diff_reas):.2f}'` percentage points** (`{python} f'{abs(mape_diff_reas_pct):.1f}'`% relative difference). While statistically detectable (Wilcoxon p=`{python} f'{wilcox_mape_reas.pvalue:.2e}'`), this difference is negligible in practice. OLS's occasional large errors (visible in the heavy right tail) inflate the mean, but the median performance is nearly identical.

## Summary: Win Rates by Coefficient Reasonableness

| OLS Coefficients | N Scenarios | PCReg Win Rate (Test MAPE) |
|:-------------------|------------------:|--------------------------------:|
| Reasonable (70-100%) | `{python} f'{n_reasonable:,}'` | `{python} f'{win_reasonable_mape:.1f}'`% |
| Unreasonable (\<70% or \>100%) | `{python} f'{n_unreasonable:,}'` | `{python} f'{win_unreasonable_mape:.1f}'`% |
| **Overall** | `{python} f'{len(comparison):,}'` | `{python} f'{overall_win_mape:.1f}'`% |

: PCReg win rates against OLS by coefficient reasonableness {#tbl-winrates}

```{python}
#| label: fig-kde-mape
#| fig-cap: "Distribution of Test MAPE for OLS vs PCReg, stratified by whether OLS produced reasonable coefficients. When unreasonable (right), OLS shows a heavy right tail of large errors that PCReg avoids."

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# Reasonable coefficients
ax1 = axes[0]
reasonable_data = comparison[comparison['ols_reasonable'] == True]
sns.kdeplot(data=reasonable_data, x='ols_mape', ax=ax1, label='OLS', color='#e74c3c', fill=True, alpha=0.3)
sns.kdeplot(data=reasonable_data, x='pcreg_mape', ax=ax1, label='PCReg', color='#3498db', fill=True, alpha=0.3)
ax1.set_xlabel('Test MAPE')
ax1.set_ylabel('Density')
ax1.set_title(f'OLS Reasonable (n={len(reasonable_data):,})')
ax1.legend()
ax1.set_xlim(0, reasonable_data[['ols_mape', 'pcreg_mape']].quantile(0.95).max())

# Unreasonable coefficients
ax2 = axes[1]
unreasonable_data = comparison[comparison['ols_reasonable'] == False]
sns.kdeplot(data=unreasonable_data, x='ols_mape', ax=ax2, label='OLS', color='#e74c3c', fill=True, alpha=0.3)
sns.kdeplot(data=unreasonable_data, x='pcreg_mape', ax=ax2, label='PCReg', color='#3498db', fill=True, alpha=0.3)
ax2.set_xlabel('Test MAPE')
ax2.set_ylabel('Density')
ax2.set_title(f'OLS Unreasonable (n={len(unreasonable_data):,})')
ax2.legend()
ax2.set_xlim(0, unreasonable_data[['ols_mape', 'pcreg_mape']].quantile(0.95).max())

plt.tight_layout()
plt.show()
```

## OLS-LearnOnly Performs Poorly Outside Training Range

```{python}
#| label: ols-learn-analysis

# Compare OLS-LearnOnly performance
ols_learn_worse = (comparison['ols_learn_mape'] > comparison['ols_mape']).mean() * 100
ols_learn_mean_mape = comparison['ols_learn_mape'].mean() * 100
ols_mean_mape = comparison['ols_mape'].mean() * 100
pcreg_mean_mape = comparison['pcreg_mape'].mean() * 100

# PCReg vs OLS-LearnOnly comparison
comparison['pcreg_wins_vs_learnonly'] = comparison['pcreg_mape'] < comparison['ols_learn_mape']
pcreg_vs_learnonly_overall = comparison['pcreg_wins_vs_learnonly'].mean() * 100
pcreg_vs_learnonly_reasonable = comparison[comparison['ols_reasonable'] == True]['pcreg_wins_vs_learnonly'].mean() * 100
pcreg_vs_learnonly_unreasonable = comparison[comparison['ols_reasonable'] == False]['pcreg_wins_vs_learnonly'].mean() * 100
```

OLS with only the learning variable (OLS-LearnOnly) ignores the rate effect, which leads to poor extrapolation:

| Model         |                           Mean Test MAPE |
|:--------------|-----------------------------------------:|
| OLS-LearnOnly | `{python} f'{ols_learn_mean_mape:.1f}'`% |
| OLS           |       `{python} f'{ols_mean_mape:.1f}'`% |
| PCReg         |     `{python} f'{pcreg_mean_mape:.1f}'`% |

: Average Test MAPE by model {#tbl-mape-comparison}

OLS-LearnOnly has worse Test MAPE than full OLS in **`{python} f'{ols_learn_worse:.1f}'`%** of scenarios. While simplifying the model may seem appealing, omitting the rate effect leads to systematic prediction errors outside the training range.

PCReg outperforms OLS-LearnOnly on Test MAPE in **`{python} f'{pcreg_vs_learnonly_overall:.1f}'`%** of all scenarios:

-   When OLS coefficients are **reasonable**: PCReg wins **`{python} f'{pcreg_vs_learnonly_reasonable:.1f}'`%**
-   When OLS coefficients are **unreasonable**: PCReg wins **`{python} f'{pcreg_vs_learnonly_unreasonable:.1f}'`%**

Across all scenarios, PCReg outperforms OLS on Test MAPE in **`{python} f'{overall_win_mape:.1f}'`%** of cases, indicating a consistent predictive advantage even when OLS is competitive.

### Coefficient Bias Analysis

Constraints introduce bias in coefficient estimates. We analyze whether this bias is systematic and how it affects prediction:

```{python}
#| label: tbl-bias
#| tbl-cap: "Coefficient bias statistics (Estimated - True). Mean/Median near 0 indicates unbiased estimation; lower Std indicates more stable estimates."

# Calculate statistics for each model and coefficient
bias_stats = results.groupby('model_name').agg({
    'T1_bias': ['mean', 'median', 'std'],
    'b_bias': ['mean', 'median', 'std'],
    'c_bias': ['mean', 'median', 'std']
}).T

# Create clean row structure
rows = []
for coef in ['T1_bias', 'b_bias', 'c_bias']:
    coef_name = coef.replace('_bias', '').replace('T1', '$T_1$').replace('b', '$b$').replace('c', '$c$')
    for stat in ['mean', 'median', 'std']:
        row_data = {'Coefficient': coef_name, 'Statistic': stat.capitalize()}
        for model in ['OLS', 'PCReg_GCV']:
            if model in bias_stats.columns:
                val = bias_stats.loc[(coef, stat), model]
                row_data[model] = f'{val:.2f}' if abs(val) > 0.01 else f'{val:.4f}'
        rows.append(row_data)

bias_table = pd.DataFrame(rows)
bias_table = bias_table.rename(columns={'PCReg_GCV': 'PCReg'})

bias_table
```

```{python}
#| label: fig-bias-dist
#| fig-cap: "Distribution of coefficient errors by model (1st-99th percentile). Vertical line at 0 indicates no bias. PCReg shows tighter distributions despite some bias, leading to lower variance in predictions."

fig, axes = plt.subplots(1, 3, figsize=(14, 4))

# Helper function to get percentile limits
def get_pct_limits(data1, data2, lower_pct=1, upper_pct=99):
    combined = pd.concat([data1, data2])
    return combined.quantile(lower_pct/100), combined.quantile(upper_pct/100)

# T1 bias
ax1 = axes[0]
ols_data = results[results['model_name'] == 'OLS']['T1_bias']
pcreg_data = results[results['model_name'] == 'PCReg_GCV']['T1_bias']
xlim_low, xlim_high = get_pct_limits(ols_data, pcreg_data)
ols_filtered = ols_data[(ols_data >= xlim_low) & (ols_data <= xlim_high)]
pcreg_filtered = pcreg_data[(pcreg_data >= xlim_low) & (pcreg_data <= xlim_high)]
sns.kdeplot(data=ols_filtered, ax=ax1, label='OLS', color='#e74c3c', fill=True, alpha=0.3)
sns.kdeplot(data=pcreg_filtered, ax=ax1, label='PCReg', color='#3498db', fill=True, alpha=0.3)
ax1.axvline(x=0, color='black', linestyle='--', linewidth=2, label='No Bias')
ax1.set_xlabel('T1 Error (Est - True)')
ax1.set_ylabel('Density')
ax1.set_title('T1 Coefficient Bias')
ax1.legend()
ax1.set_xlim(xlim_low, xlim_high)

# b bias
ax2 = axes[1]
ols_data = results[results['model_name'] == 'OLS']['b_bias']
pcreg_data = results[results['model_name'] == 'PCReg_GCV']['b_bias']
xlim_low, xlim_high = get_pct_limits(ols_data, pcreg_data)
ols_filtered = ols_data[(ols_data >= xlim_low) & (ols_data <= xlim_high)]
pcreg_filtered = pcreg_data[(pcreg_data >= xlim_low) & (pcreg_data <= xlim_high)]
sns.kdeplot(data=ols_filtered, ax=ax2, label='OLS', color='#e74c3c', fill=True, alpha=0.3)
sns.kdeplot(data=pcreg_filtered, ax=ax2, label='PCReg', color='#3498db', fill=True, alpha=0.3)
ax2.axvline(x=0, color='black', linestyle='--', linewidth=2, label='No Bias')
ax2.set_xlabel('b Error (Est - True)')
ax2.set_ylabel('Density')
ax2.set_title('Learning Slope (b) Bias')
ax2.legend()
ax2.set_xlim(xlim_low, xlim_high)

# c bias
ax3 = axes[2]
ols_data = results[results['model_name'] == 'OLS']['c_bias']
pcreg_data = results[results['model_name'] == 'PCReg_GCV']['c_bias']
xlim_low, xlim_high = get_pct_limits(ols_data, pcreg_data)
ols_filtered = ols_data[(ols_data >= xlim_low) & (ols_data <= xlim_high)]
pcreg_filtered = pcreg_data[(pcreg_data >= xlim_low) & (pcreg_data <= xlim_high)]
sns.kdeplot(data=ols_filtered, ax=ax3, label='OLS', color='#e74c3c', fill=True, alpha=0.3)
sns.kdeplot(data=pcreg_filtered, ax=ax3, label='PCReg', color='#3498db', fill=True, alpha=0.3)
ax3.axvline(x=0, color='black', linestyle='--', linewidth=2, label='No Bias')
ax3.set_xlabel('c Error (Est - True)')
ax3.set_ylabel('Density')
ax3.set_title('Rate Slope (c) Bias')
ax3.legend()
ax3.set_xlim(xlim_low, xlim_high)

plt.tight_layout()
plt.show()
```

**Key Insight**: While OLS is theoretically unbiased (mean errors near 0), it has high variance---the distribution is wide. PCReg may have slight bias but much lower variance, leading to better overall prediction accuracy (the classic bias-variance tradeoff).

## Parameter Effects on Model Performance

Beyond coefficient reasonableness, several design factors systematically influence when PCReg outperforms OLS. We examine sample size, predictor correlation, and noise level.

```{python}
#| label: parameter-effects-analysis
#| output: asis

from tabulate import tabulate

# Create correlation bins for analysis
comparison['corr_bin'] = pd.cut(comparison['actual_correlation'], 
                                 bins=[0, 0.8, 0.9, 0.95, 1.0],
                                 labels=['<0.80', '0.80-0.90', '0.90-0.95', '>0.95'])

# === Table 1: Win Rate by OLS Reasonableness (recap) ===
reason_table = comparison.groupby('ols_reasonable').agg(
    n_scenarios=('pcreg_wins_mape', 'count'),
    win_rate=('pcreg_wins_mape', 'mean'),
    mean_ols_mape=('ols_mape', 'mean'),
    mean_pcreg_mape=('pcreg_mape', 'mean')
).reset_index()
reason_table['ols_reasonable'] = reason_table['ols_reasonable'].map({True: 'Reasonable (70-100%)', False: 'Unreasonable'})
reason_table['win_rate'] = (reason_table['win_rate'] * 100).round(1).astype(str) + '%'
reason_table['mean_ols_mape'] = (reason_table['mean_ols_mape'] * 100).round(1).astype(str) + '%'
reason_table['mean_pcreg_mape'] = (reason_table['mean_pcreg_mape'] * 100).round(1).astype(str) + '%'
reason_table.columns = ['OLS Coefficients', 'N', 'PCReg Win Rate', 'OLS Mean MAPE', 'PCReg Mean MAPE']

print("**By OLS Coefficient Reasonableness:**\n")
print(tabulate(reason_table, headers='keys', tablefmt='pipe', showindex=False))
print("\n")

# === Table 2: Win Rate by Sample Size ===
n_table = comparison.groupby('n_lots').agg(
    n_scenarios=('pcreg_wins_mape', 'count'),
    win_rate=('pcreg_wins_mape', 'mean'),
    pct_unreasonable=('ols_reasonable', lambda x: (~x).mean()),
    mean_ols_mape=('ols_mape', 'mean'),
    mean_pcreg_mape=('pcreg_mape', 'mean')
).reset_index()
n_table['win_rate'] = (n_table['win_rate'] * 100).round(1).astype(str) + '%'
n_table['pct_unreasonable'] = (n_table['pct_unreasonable'] * 100).round(1).astype(str) + '%'
n_table['mean_ols_mape'] = (n_table['mean_ols_mape'] * 100).round(1).astype(str) + '%'
n_table['mean_pcreg_mape'] = (n_table['mean_pcreg_mape'] * 100).round(1).astype(str) + '%'
n_table.columns = ['Sample Size', 'N', 'PCReg Win Rate', '% OLS Unreasonable', 'OLS Mean MAPE', 'PCReg Mean MAPE']

print("**By Sample Size (n_lots):**\n")
print(tabulate(n_table, headers='keys', tablefmt='pipe', showindex=False))
print("\n")

# === Table 3: Win Rate by Correlation ===
corr_table = comparison.groupby('corr_bin', observed=True).agg(
    n_scenarios=('pcreg_wins_mape', 'count'),
    win_rate=('pcreg_wins_mape', 'mean'),
    pct_unreasonable=('ols_reasonable', lambda x: (~x).mean()),
    mean_ols_mape=('ols_mape', 'mean'),
    mean_pcreg_mape=('pcreg_mape', 'mean')
).reset_index()
corr_table['win_rate'] = (corr_table['win_rate'] * 100).round(1).astype(str) + '%'
corr_table['pct_unreasonable'] = (corr_table['pct_unreasonable'] * 100).round(1).astype(str) + '%'
corr_table['mean_ols_mape'] = (corr_table['mean_ols_mape'] * 100).round(1).astype(str) + '%'
corr_table['mean_pcreg_mape'] = (corr_table['mean_pcreg_mape'] * 100).round(1).astype(str) + '%'
corr_table.columns = ['Correlation', 'N', 'PCReg Win Rate', '% OLS Unreasonable', 'OLS Mean MAPE', 'PCReg Mean MAPE']

print("**By Predictor Correlation:**\n")
print(tabulate(corr_table, headers='keys', tablefmt='pipe', showindex=False))
print("\n")

# === Table 4: Win Rate by CV Error ===
cv_table = comparison.groupby('cv_error').agg(
    n_scenarios=('pcreg_wins_mape', 'count'),
    win_rate=('pcreg_wins_mape', 'mean'),
    pct_unreasonable=('ols_reasonable', lambda x: (~x).mean()),
    mean_ols_mape=('ols_mape', 'mean'),
    mean_pcreg_mape=('pcreg_mape', 'mean')
).reset_index()
cv_table['win_rate'] = (cv_table['win_rate'] * 100).round(1).astype(str) + '%'
cv_table['pct_unreasonable'] = (cv_table['pct_unreasonable'] * 100).round(1).astype(str) + '%'
cv_table['mean_ols_mape'] = (cv_table['mean_ols_mape'] * 100).round(1).astype(str) + '%'
cv_table['mean_pcreg_mape'] = (cv_table['mean_pcreg_mape'] * 100).round(1).astype(str) + '%'
cv_table.columns = ['CV Error', 'N', 'PCReg Win Rate', '% OLS Unreasonable', 'OLS Mean MAPE', 'PCReg Mean MAPE']

print("**By Noise Level (CV Error):**\n")
print(tabulate(cv_table, headers='keys', tablefmt='pipe', showindex=False))
```

```{python}
#| label: parameter-effects-summary

# Calculate key statistics for inline text
win_by_n = comparison.groupby('n_lots')['pcreg_wins_mape'].mean() * 100
win_n5 = win_by_n.get(5, win_by_n.iloc[0])
win_n30 = win_by_n.get(30, win_by_n.iloc[-1])

win_by_corr = comparison.groupby('corr_bin', observed=True)['pcreg_wins_mape'].mean() * 100
win_low_corr = win_by_corr.iloc[0] if len(win_by_corr) > 0 else 50
win_high_corr = win_by_corr.iloc[-1] if len(win_by_corr) > 0 else 50

unreasonable_by_n = comparison.groupby('n_lots')['ols_reasonable'].apply(lambda x: (~x).mean() * 100)
unreasonable_n5 = unreasonable_by_n.get(5, unreasonable_by_n.iloc[0])
unreasonable_n30 = unreasonable_by_n.get(30, unreasonable_by_n.iloc[-1])
```

**Key Observations:**

1.  **Sample Size Effect**: As sample size decreases, OLS becomes less stable. With n=5 lots, PCReg wins **`{python} f'{win_n5:.1f}'`%** of scenarios compared to **`{python} f'{win_n30:.1f}'`%** at n=30. The rate of unreasonable OLS coefficients also increases from **`{python} f'{unreasonable_n30:.1f}'`%** to **`{python} f'{unreasonable_n5:.1f}'`%** as sample size decreases.

2.  **Correlation Effect**: Higher predictor correlation destabilizes OLS. At correlation \>0.95, PCReg wins **`{python} f'{win_high_corr:.1f}'`%** of scenarios versus **`{python} f'{win_low_corr:.1f}'`%** at lower correlations. This is the multicollinearity effect. When predictors are highly correlated, OLS coefficient estimates become unstable and constraints provide crucial stability.

3.  **Noise Level Effect**: Higher CV error increases the advantage of PCReg. With more noise, OLS has greater difficulty separating learning and rate effects, leading to more unreasonable coefficient estimates.

4.  **Compounding Effects**: These factors interact---small samples with high correlation and high noise represent the most challenging scenarios for OLS, where PCReg provides the greatest benefit.

\newpage

# Recommendations: When to Use PCReg

**Practical Decision Rules:**

1.  **If OLS produces unreasonable coefficients** (LC or RC outside 70-100%): **Use PCReg**, it significantly outperforms OLS in these scenarios
2.  **For small samples (n ≤ 5 or 10 lots) with noisy data**: **Prefer PCReg**, it wins 60-67% of scenarios
3.  **For large samples (n ≥ 30 lots)**: **OLS is generally preferred**, it wins \~80% of scenarios
4.  **For intermediate cases**: Either method is acceptable; PCReg provides insurance against unreasonable coefficients with minimal downside

**Bottom line**: The difference between OLS and PCReg is typically small and can be controlled by explicitly defining loose constraints and arbitrarily small penalties.

## Software

The `penalized-constrained` Python package was developed specifically for the cost estimating community. As of this paper's publication, the software is in active development but has achieved stable functionality.

**Installation:** While we anticipate release to PyPI for convenient installation via `pip install penalized-constrained`, the package is currently available via GitHub. To install from the development repository:

``` bash
pip install git+https://github.com/frankij11/Penalized-Constrained-Regression.git
```

For quick-start guides and basic usage examples, see the package documentation on GitHub or @sec-appendix-software in the appendices.

A key advantage of this framework is that **PCReg collapses to OLS** when no constraints are defined, no penalties are applied (α=0), and a linear functional form is used. This allows analysts to use a single, cohesive library for all regression analysis. From standard OLS to fully constrained penalized models with custom prediction functions all without switching tools or workflows. This library is designed to integrate seamlessly Python's scikit-learn data science workflows and allow machine learning techniques to solve for optimal parameters.

## Summary

Our simulation study demonstrates that PCReg provides meaningful advantages in small-sample, high-noise scenarios where OLS is most likely to produce unreasonable coefficients, while large samples favor standard OLS. The constraints introduce beneficial bias that reduces prediction variance—a classic bias-variance tradeoff that works in the analyst's favor when data are limited. With GCV enabling reliable hyperparameter selection using as few as 5 observations and OLS-LearnOnly's poor extrapolation performance reinforcing that omitting the rate effect oversimplifies the problem, analysts have clear guidance: match the method to the sample size and data quality, with PCReg serving as effective insurance when uncertainty is high.

## References {.unnumbered}

::: {#refs}
:::

\newpage
\appendix

# Appendix A: Simulation Details {#sec-appendix-simulation}

## Data Generation Process

For each of the 8,100 scenarios:

1.  **Select quantity profile**: Randomly sample a defense program from the SAR database with sufficient lot history
2.  **Extract lot structure**: Use actual procurement quantities and calculate lot midpoints
3.  **Generate true costs**: Apply learning curve model with scenario parameters
4.  **Add noise**: Multiply true costs by lognormal error: $Y_{obs} = Y_{true} \cdot e^{\epsilon}$ where $\epsilon \sim N(-\sigma^2/2, \sigma^2)$
5.  **Split data**: First $n$ lots for training, **all remaining lots** for test (variable test set size)

## Model Specifications

| Model         | Description                        | Constraints |
|:--------------|:-----------------------------------|:-----------:|
| OLS           | Standard log-log OLS               |     No      |
| OLS_LearnOnly | OLS with learning variable only    |     No      |
| PCReg_GCV     | GCV-selected penalty + constraints |     Yes     |

: Models compared in main paper {#tbl-models-main}

\newpage

# Appendix B: Full Results (All Models) {#sec-appendix-results}

```{python}
#| label: tbl-all-models

summary_all = results_all.groupby('model_name').agg({
    'test_mape': 'mean',
    'test_sspe': 'mean',
    'b_error': 'mean',
    'c_error': 'mean',
    'r2': 'mean'
}).round(4)
summary_all = summary_all.sort_values('test_mape')

# Store values for inline printing
model_results = []
for model in summary_all.index:
    row = summary_all.loc[model]
    model_results.append({
        'model': model,
        'mape': row['test_mape'],
        'sspe': row['test_sspe'],
        'b_err': row['b_error'],
        'c_err': row['c_error'],
        'r2': row['r2']
    })
```

| Model               | Test MAPE | Test SSPE | b Error | c Error |    R2 |
|:--------------------|----------:|----------:|--------:|--------:|------:|
| PCReg_GCV_Tight     |    0.0561 |    0.0861 |  0.0200 |  0.0337 | 0.852 |
| PCReg_ConstrainOnly |    0.0764 |    0.2052 |  0.0354 |  0.0791 | 0.877 |
| PCReg_GCV           |    0.0773 |    0.2074 |  0.0338 |  0.0827 | 0.871 |
| PCRegGCV_LogMSE     |    0.0778 |    0.2832 |  0.0341 |  0.0776 | 0.877 |
| PCReg_CV            |    0.0794 |    0.2538 |  0.0353 |  0.0814 | 0.862 |
| BayesianRidge       |    0.0801 |    0.4595 |  0.0360 |  0.0886 | 0.882 |
| PCReg_AICc          |    0.0808 |    0.2171 |  0.0349 |  0.0913 | 0.863 |
| RidgeCV             |    0.0952 |    0.9362 |  0.0481 |  0.1216 | 0.896 |
| LassoCV             |    0.0957 |    0.9767 |  0.0428 |  0.1082 | 0.858 |
| OLS                 |    0.0994 |    0.9727 |  0.0520 |  0.1319 | 0.897 |
| OLS_LearnOnly       |    0.1592 |    0.9543 |  0.0827 |  0.2361 | 0.789 |

: Overall model performance across all 8,100 scenarios. Lower Test MAPE is better. {#tbl-all-results}

\newpage

# Appendix C: Software Documentation {#sec-appendix-software}

## Installation

``` bash
pip install penalized-constrained
```

## Basic Usage

```{python}
#| echo: true
#| eval: false

import penalized_constrained as pcreg
import numpy as np

model = pcreg.PenalizedConstrainedCV(
    coef_names=['T1', 'b', 'c'],
    bounds={
        'T1': (0, None),      # T1 must be positive
        'b': (-0.5, 0),       # Learning rate 70-100%
        'c': (-0.5, 0)        # Rate effect 70-100%
    },
    prediction_fn=lambda X, p: p[0] * X[:,0]**p[1] * X[:,1]**p[2],
    loss='sspe',
    selection='gcv'
)
model.fit(X_train, y_train)
```

## Citation

``` bibtex
@inproceedings{joy2026pcreg,
  title={Penalized-Constrained Regression: Combining Regularization
         and Domain Constraints for Cost Estimation},
  author={Joy, Kevin and Watstein, Max},
  booktitle={ICEAA Professional Development \& Training Workshop},
  year={2026}
}
```

\newpage

# Appendix D: Cross Validation Methods {#sec-appendix-cv}

## Overview

Cross Validation (CV) is a resampling technique used to evaluate how well a statistical model generalizes to independent data. In the context of penalized-constrained regression, CV helps us select optimal penalty parameters ($\lambda$ and $\alpha$) that balance model fit against overfitting. This appendix provides technical details on the CV methods implemented in our framework.

## K-Fold Cross Validation

### Methodology

K-Fold CV divides the training data into K roughly equal-sized subsets (folds). The model is trained K times, each time using K-1 folds for training and the remaining fold for validation. The CV score is the average prediction error across all K validation folds.

### Algorithm

For each candidate penalty parameter value:

1. Partition training data into K folds: $D = \{F_1, F_2, ..., F_K\}$
2. For each fold $k = 1, ..., K$:
   - Train model on $D \setminus F_k$ (all data except fold k)
   - Predict on validation fold $F_k$
   - Calculate validation error: $E_k = \sum_{i \in F_k} L(y_i, \hat{y}_i)$
3. Compute CV score: $\text{CV}_K = \frac{1}{K} \sum_{k=1}^K E_k$
4. Select penalty parameters that minimize $\text{CV}_K$

### Choice of K

- **K=5 or K=10**: Common choices balancing bias and variance
- **Leave-One-Out (LOO, K=n)**: Minimum bias but high computational cost and high variance
- **Smaller K**: Less computation, higher bias (less training data per fold), lower variance
- **Larger K**: More computation, lower bias (more training data per fold), higher variance

### Advantages

- Directly estimates out-of-sample prediction error
- Straightforward interpretation
- Well-established in machine learning literature
- Provides robust estimates when data is sufficient

### Disadvantages

- Reduces effective training sample size (uses only $(K-1)/K$ of data per fold)
- Problematic for small datasets (n < 20)
- Computationally expensive (requires K model fits per penalty value)
- Can have high variance when K is large or n is small
- May produce unstable penalty selection with small samples

## Generalized Cross Validation (GCV)

### Motivation

GCV [@golub1979generalized; @craven1978smoothing] was developed to approximate Leave-One-Out (LOO) cross validation without the computational burden of fitting n separate models. For small datasets common in cost estimation (n=5-20), GCV provides penalty selection without further reducing the training sample.

### Mathematical Formulation

GCV estimates the prediction error using the full training data and a degrees-of-freedom adjustment:

$$
\text{GCV} = \frac{n \cdot \text{RSS}}{(n - \text{df})^2}
$$

where:

- $n$ = sample size
- $\text{RSS} = \sum_{i=1}^n (y_i - \hat{y}_i)^2$ = residual sum of squares on training data
- $\text{df}$ = effective degrees of freedom (Generalized Degrees of Freedom, GDF)

The penalty parameters that minimize GCV are selected as optimal.

### Degrees of Freedom Adjustment

The denominator $(n - \text{df})^2$ penalizes model complexity. For penalized-constrained regression, we use the Gaines et al. (2018) GDF formula:

$$
\text{df} = |\text{Active predictors}| + |\text{Equality constraints}| + |\text{Binding inequality constraints}|
$$

This accounts for the effective number of parameters after accounting for regularization and active constraints.

### GCV as LOO Approximation

GCV approximates the LOO cross validation score:

$$
\text{LOO} = \frac{1}{n} \sum_{i=1}^n \left(y_i - \hat{y}_i^{(-i)}\right)^2
$$

where $\hat{y}_i^{(-i)}$ is the prediction for observation $i$ from a model trained without observation $i$.

The key approximation is:

$$
y_i - \hat{y}_i^{(-i)} \approx \frac{y_i - \hat{y}_i}{1 - h_i}
$$

where $h_i$ is the leverage (diagonal element of the hat matrix). GCV replaces individual leverages with the average: $h_i \approx \text{df}/n$.

### Advantages

- **No data splitting**: Uses full training sample, crucial for small datasets
- **Efficient computation**: Single model fit per penalty value (vs. K fits for K-Fold)
- **Deterministic**: No randomness from fold assignments
- **Asymptotically equivalent to LOO**: Converges to LOO as n increases
- **Optimal for small samples**: Maintains training power when n < 20

### Disadvantages

- Requires differentiable loss and prediction functions to compute GDF
- Approximation quality depends on leverage uniformity
- Less intuitive than K-Fold CV
- May be unstable if GDF calculation is unreliable
- Not applicable to all model types (works well for penalized regression)

## Assumptions and Limitations

### Common Assumptions (Both K-Fold and GCV)

1. **Independence**: Observations are independent and identically distributed (i.i.d.)
   - Violation: Time series data, clustered data, or spatial correlation
   - Impact: CV may underestimate prediction error

2. **Stationarity**: The data generation process remains stable
   - Violation: Non-stationary processes or regime changes
   - Impact: Past data may not predict future well

3. **Representative samples**: Training data represents the population of interest
   - Violation: Selection bias, small unrepresentative samples
   - Impact: Selected penalties may not generalize

4. **No data leakage**: Validation data must be truly held out
   - Violation: Feature engineering on full dataset before CV
   - Impact: Overly optimistic error estimates

### Specific to K-Fold CV

- **Sufficient data**: Requires enough observations to create meaningful folds (typically n > 50 for K=10)
- **Balanced folds**: Folds should be representative of full dataset

### Specific to GCV

- **Smooth loss function**: GCV derivation assumes differentiable loss
- **Accurate GDF**: Requires reliable degrees of freedom calculation
- **Linear-like behavior**: Works best for models with relatively smooth penalty effects

## Comparison and Recommendations

| Criterion | K-Fold CV | GCV | Recommendation |
|:----------|:---------|:----|:---------------|
| **Sample size n < 10** | Poor | Good | **Use GCV** |
| **Sample size 10 ≤ n < 20** | Fair | Good | **Prefer GCV** |
| **Sample size 20 ≤ n < 50** | Good | Good | Either acceptable |
| **Sample size n ≥ 50** | Good | Good | **Prefer K-Fold** (more robust) |
| **Computational cost** | High (K fits) | Low (1 fit) | GCV faster |
| **Interpretability** | High | Moderate | K-Fold clearer |
| **Deterministic** | No (random folds) | Yes | GCV if reproducibility critical |

: Comparison of K-Fold CV and GCV {#tbl-cv-comparison}

### Practical Guidance for Cost Estimation

1. **Small samples (n=5-15 lots)**: Use GCV (default in our implementation)
   - Preserves full training data
   - More stable penalty selection
   - Computationally efficient

2. **Medium samples (n=15-30 lots)**: Either method acceptable
   - GCV for consistency with small-sample cases
   - K-Fold (K=5) for more conservative penalty selection

3. **Large samples (n>30 lots)**: Use K-Fold CV (K=10)
   - More robust to model assumptions
   - Direct estimate of out-of-sample error
   - Standard practice in machine learning

4. **When GDF is unreliable**: Fall back to K-Fold CV
   - Non-differentiable loss functions
   - Highly constrained solutions with uncertain active set

## Implementation in PCReg

The `PenalizedConstrainedCV` class supports both methods:

```python
# GCV (default for small datasets)
model_gcv = pcreg.PenalizedConstrainedCV(
    selection='gcv',  # Generalized Cross Validation
    ...
)

# K-Fold CV
model_kfold = pcreg.PenalizedConstrainedCV(
    selection='cv',   # K-Fold Cross Validation
    cv_folds=5,      # Number of folds (default=5)
    ...
)
```

The implementation automatically handles:
- GDF calculation for GCV
- Stratified fold assignment for K-Fold CV
- Grid search over penalty parameter space
- Selection of optimal penalty values

## References for Cross Validation

Key papers on cross validation methods:

- @golub1979generalized: Original GCV formulation
- @craven1978smoothing: Theoretical foundations of GCV
- @stone1974cross: Cross-validatory choice of prediction rules
- @hastie2009elements: Comprehensive treatment in Section 7.10

\newpage

# Appendix A: Detailed Diagnostic Report {.appendix}

This appendix provides comprehensive diagnostic information for the PCReg_GCV model fit from the motivating example.

## Model Specification

```{python}
#| label: appendix-model-spec
#| output: asis
#| echo: false

from tabulate import tabulate
import pandas as pd

# Model specification details
spec_data = {
    'Parameter': [
        'Model Type',
        'Loss Function',
        'Alpha ($\\lambda$)',
        'L1 Ratio ($\\alpha$)',
        'Fit Intercept',
        'Optimization Method',
        'Convergence Status',
        'GDF Method'
    ],
    'Value': [
        report.model_spec.model_type,
        report.model_spec.loss_function,
        f'{report.model_spec.alpha:.6f}',
        f'{report.model_spec.l1_ratio:.4f}',
        str(report.model_spec.fit_intercept),
        report.model_spec.method,
        'Yes' if report.model_spec.converged else 'No',
        report.fit_stats.gdf_method
    ]
}

df_spec = pd.DataFrame(spec_data)
print(df_spec.to_markdown(index=False))
```

## Coefficient Estimates

```{python}
#| label: appendix-coefficients
#| output: asis
#| echo: false

# Extract coefficient information
coef_data = []
for coef in report.coefficients:
    coef_data.append({
        'Parameter': coef.name,
        'Estimate': f'{coef.value:.6f}',
        'Bootstrap SE': f'{coef.bootstrap_se:.6f}' if coef.bootstrap_se is not None else 'N/A',
        '95% CI Lower': f'{coef.bootstrap_ci_lower:.6f}' if coef.bootstrap_ci_lower is not None else 'N/A',
        '95% CI Upper': f'{coef.bootstrap_ci_upper:.6f}' if coef.bootstrap_ci_upper is not None else 'N/A',
        'Lower Bound': f'{coef.lower_bound}' if coef.lower_bound != -np.inf else 'None',
        'Upper Bound': f'{coef.upper_bound}' if coef.upper_bound != np.inf else 'None',
        'Status': coef.bound_status
    })

df_coef = pd.DataFrame(coef_data)
print(df_coef.to_markdown(index=False))
```

## Fit Statistics

```{python}
#| label: appendix-fit-stats
#| output: asis
#| echo: false

# Fit statistics
fit_data = {
    'Statistic': [
        'R²',
        'Adjusted R²',
        'SEE',
        'SPE',
        'MAPE',
        'RMSE',
        'CV',
        'GDF'
    ],
    'Value': [
        f'{report.fit_stats.r2:.6f}',
        f'{report.fit_stats.adj_r2:.6f}',
        f'{report.fit_stats.see:.6f}',
        f'{report.fit_stats.spe:.6f}',
        f'{report.fit_stats.mape:.4f}',
        f'{report.fit_stats.rmse:.6f}',
        f'{report.fit_stats.cv:.6f}',
        f'{report.fit_stats.gdf:.2f}'
    ]
}

df_fit = pd.DataFrame(fit_data)
print(df_fit.to_markdown(index=False))
```

## Constraint Summary

```{python}
#| label: appendix-constraints
#| output: asis
#| echo: false

print(f"**Number of specified bounds:** {report.constraints.n_specified}\n")
print(f"**Number of active constraints:** {report.constraints.n_active}\n")

if report.constraints.active_constraints:
    print("\n**Active constraints:**\n")
    for param, bound_type in report.constraints.active_constraints:
        print(f"- {param} at {bound_type} bound")
else:
    print("\nNo constraints are active (all parameters are in the interior).")
```

## Bootstrap Results

```{python}
#| label: appendix-bootstrap
#| output: asis
#| echo: false

if report.bootstrap_results is not None:
    boot = report.bootstrap_results

    print(f"**Number of bootstrap samples:** {boot.n_bootstrap}\n")
    print(f"**Confidence level:** {boot.confidence*100:.0f}%\n")
    print(f"**Successful constrained fits:** {boot.constrained.n_successful}\n")
    if boot.unconstrained is not None:
        print(f"**Successful unconstrained fits:** {boot.unconstrained.n_successful}\n")

    print("\n### Bootstrap Summary Statistics\n")

    # Create comparison table
    boot_summary = boot.summary_dataframe(include_unconstrained=True)
    print(boot_summary.to_markdown(index=False, floatfmt='.6f'))
else:
    print("Bootstrap confidence intervals were not computed for this model.")
```

## Data Summary

```{python}
#| label: appendix-data-summary
#| output: asis
#| echo: false

data_info = {
    'Property': [
        'Number of samples',
        'Number of features',
        'Target mean',
        'Target std',
        'Target min',
        'Target max'
    ],
    'Value': [
        report.data_summary.n_samples,
        report.data_summary.n_features,
        f'{report.data_summary.y_mean:.4f}',
        f'{report.data_summary.y_std:.4f}',
        f'{report.data_summary.y_min:.4f}',
        f'{report.data_summary.y_max:.4f}'
    ]
}

df_data = pd.DataFrame(data_info)
print(df_data.to_markdown(index=False))
```

## Model Equation

```{python}
#| label: appendix-equation
#| output: asis
#| echo: false

if report.equation is not None:
    if report.equation.is_custom:
        print("**Custom Prediction Function:**\n")
        print("```python")
        if report.equation.source:
            print(report.equation.source)
        print("```\n")

    if report.equation.latex:
        print(f"\n**Model Equation:** ${report.equation.latex}$\n")
    elif report.equation.text:
        print(f"\n**Model Equation:** `{report.equation.text}`\n")
```