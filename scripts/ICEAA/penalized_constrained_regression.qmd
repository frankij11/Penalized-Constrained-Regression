---
title: "Small Data, Big Problems"
subtitle: "Penalized-Constrained Regression for Cost Estimation"
author:
  - name: Kevin Joy
    affiliations:
      - id: herren
        name: Herren Associates
  - name: Max Watstein
    affiliations:
      - id: herren
        name: Herren Associates

date: 2026-02-17
format:
  pdf:
    toc: true
    toc-depth: 3
    number-sections: true
    number-offset: 1
    colorlinks: true
    documentclass: article
    geometry:
      - margin=1in
    fontsize: 11pt
    include-in-header:
      text: |
        \usepackage{booktabs}
        \usepackage{float}
        \floatplacement{figure}{H}
        \floatplacement{table}{H}
        \usepackage[title,titletoc]{appendix}
        \renewcommand{\appendixname}{Appendix}
    include-before-body:
      text: |
        \thispagestyle{empty}
        \newpage
        \pagenumbering{roman}
bibliography: references.bib
execute:
  echo: false
  warning: false
  message: false
---

```{python}
#| label: setup

import pandas as pd
import numpy as np
from pathlib import Path
import subprocess
import sys
import matplotlib.pyplot as plt
import seaborn as sns
import tabulate

# Set plotting style
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("colorblind")
plt.rcParams['figure.dpi'] = 150
plt.rcParams['savefig.dpi'] = 150
plt.rcParams['font.size'] = 10

# Set paths
OUTPUT_DIR = Path('output_v2')

# Run simulation if data doesn't exist
if not (OUTPUT_DIR / 'simulation_results.parquet').exists():
    print("Simulation data not found. Running simulation...")
    print("Note: Running simulation can take ~50 minutes on a typical computer")
    print("Run:")
    print("python run_simulation.py")
    print("python simulation_analysis.py")

# Load data
df_example = pd.read_csv(OUTPUT_DIR / 'motivational_example_data.csv')
results_all = pd.read_parquet(OUTPUT_DIR / 'simulation_results.parquet')

# Filter to OLS, PCReg, and OLS_LearnOnly for paper analysis
MODELS_FOR_PAPER = ['OLS', 'PCReg_GCV', 'OLS_LearnOnly']
results = results_all[results_all['model_name'].isin(MODELS_FOR_PAPER)].copy()

# Calculate derived columns for analysis
results['LC_est'] = 2 ** results['b']
results['RC_est'] = 2 ** results['c']
results['LC_true'] = 2 ** results['b_true']
results['RC_true'] = 2 ** results['c_true']

# Define OLS reasonableness (based on OLS results for each scenario)
ols_reasonableness = results[results['model_name'] == 'OLS'][['seed', 'LC_est', 'RC_est']].copy()
ols_reasonableness['ols_reasonable'] = (
    (ols_reasonableness['LC_est'] >= 0.70) & (ols_reasonableness['LC_est'] <= 1.0) &
    (ols_reasonableness['RC_est'] >= 0.70) & (ols_reasonableness['RC_est'] <= 1.0)
)
ols_reasonableness = ols_reasonableness[['seed', 'ols_reasonable']]

# Merge back to all results
results = results.merge(ols_reasonableness, on='seed', how='left')

# Calculate bias for each coefficient
results['T1_bias'] = results['T1_est'] - results['T1_true']
results['b_bias'] = results['b'] - results['b_true']
results['c_bias'] = results['c'] - results['c_true']
```

\newpage
\pagenumbering{arabic}

# Abstract {.unnumbered}

Small datasets with intercorrelation pose serious challenges to the stability of coefficients generated by Ordinary Least Squares (OLS) regression. A motivating example in cost estimating is Learning Curve with Rate Effect analysis, where datasets are typically small, the lot midpoint (Learning) is correlated to lot quantity (Rate) as production ramps up, and slopes are expected to be $\leq 100\%$. Lasso, Ridge, and Elastic Net regularization methods address multicollinearity by penalizing coefficients. Separately, constrained optimization methods can impose explicit restrictions on coefficient values when prior knowledge about their behavior is known, such as bounding slopes within a known range. This paper investigates the combined effects of penalized regularization methods and constrained optimization.  We explore how to assess model stability and goodness-of-fit using likelihood-free diagnostic techniques suited to optimization-based regressions, such as cross-validation for generalization error.

\newpage

# Introduction

While developing Cost Estimating Relationships (CERs) for small datasets (5-30 data points), a recurring pattern emerged. The regression models often showed strong fit statistics with high R² and low standard error, but nonsensical coefficients. Coefficients often had wrong signs, implausible magnitudes, and poor p-values. As Department of Defense (DoD) analysts, this story may feel all too familiar.

Intercorrelated datasets are a frequent presence in cost analysis, causing regression models to misbehave. This is especially true for Learning Curve with Rate Effect analysis, where datasets are typically small and the lot midpoint is usually correlated to lot quantity. Traditional remedies for multicollinearity can be insufficient for learning curve datasets. Increasing sample size is infeasible as it requires waiting years for additional production lots and cost data to become available. Dropping explanatory variables, such as rate effect, leads to model misspecification that ignores a fundamental cost driver. This issue is compounded when predicting outside the relevant range, which is very common. Even when these remedies are employed, OLS can still produce implausible coefficients with the wrong signs, such as learning curve slopes (LCSs) \>100%. Regularization techniques like Ridge, Lasso, and Elastic Net can help stabilize estimates, but they don't guarantee plausible coefficients. Constrained optimization can enforce domain knowledge, but without regularization, it may not solve the underlying multicollinearity problem. This paper explores the combination of penalized regularization and constrained optimization to address both challenges.

For our research, we reviewed existing literature on multicollinearity, regularization, constrained optimization, and learning theory. Drawing on these foundations, we developed the Penalized-Constrained Regression (PCReg) framework, which integrates Elastic Net penalties with domain-knowledge coefficient constraints. We then used Monte Carlo simulation to evaluate PCReg's accuracy estimating the true parameters and compared its performance against OLS across a range of sample sizes and collinearity conditions, measuring both coefficient recovery and out-of-sample prediction accuracy.

This paper develops and validates PCReg for cost estimation, contributing the following:

1.  **Model**: Integration of Elastic Net penalties and constrained optimization
2.  **Diagnostic**: Bootstrap confidence intervals and Generalized Degrees of Freedom (GDF)-adjusted fit statistics for optimization-based regression (see @sec-generalized-degrees-freedom for discussion of GDF)
3.  **Practical Guidance**: When to use PCReg and how to assess model fit

Additionally, we have developed the following tools for the cost estimating community to apply PCReg:

1.  **Python Package**: PCReg application in Python
2.  **Excel Template**: PCReg application in Excel with limited implementation

\newpage

# Motivating Example

## The Learning Curve Problem

The motivating example for this research is the learning curve with rate effect:

$$
\begin{aligned}
\text{Average Unit Cost} &= T_1 \cdot (\text{Lot Midpoint})^b \cdot (\text{Lot Quantity})^c \cdot \varepsilon \\
\\
\text{where: } T_1 &= \text{theoretical first unit cost} \\
b &= \text{learning slope in log space} \\
c &= \text{rate slope in log space} \\
\varepsilon &= \text{multiplicative error}
\end{aligned}
$$ {#eq-learning-curve}

$b$ and $c$ are both slopes in log space. When transformed to unit space, they become the LCS (LCS = $2^b$) and the rate effect ($2^c$), both typically ranging from 70-100%. As DoD programs ramp up production from Engineering and Manufacturing Development (EMD) to Full Rate Production (FRP), lot midpoint (the learning predictor) becomes highly correlated with lot quantity (the rate predictor). By definition, LCS and Rate Effect are $\leq 100\%$, meaning costs strictly decrease with cumulative production or lot quantity.

If estimated LCS or Rate Effect are \>100%, this is often an indicator of multicollinearity, or it may suggest that other explanatory variables,such as supply chain disruptions or diseconomies of scale, are missing and need to be modeled. This does not indicate a violation of the definition itself. Once these factors are properly modeled as separate explanatory variables, estimated LCS and Rate Effect should be $\leq 100\%$, consistent with their definition. It is the analyst's responsibility to ensure the model is properly specified and includes all relevant cost drivers.

## Example Scenario {#sec-example}

We demonstrate the "Learning Curve Problem" using the following dataset which was generated by our Simulation Study (@sec-simulation).

```{python}
#| label: scenario-params

df_train = df_example[df_example['lot_type'] == 'train'].copy()
df_test = df_example[df_example['lot_type'] == 'test'].copy()

true_lr = 2 ** df_example['b_true'].iloc[0]
true_re = 2 ** df_example['c_true'].iloc[0]
true_T1 = df_example['T1_true'].iloc[0]
true_b = df_example['b_true'].iloc[0]
true_c = df_example['c_true'].iloc[0]
n_train = len(df_train)
n_test = len(df_test)

# Calculate correlation between log predictors
log_mp = np.log(df_train['lot_midpoint'])
log_qty = np.log(df_train['lot_quantity'])
corr = np.corrcoef(log_mp, log_qty)[0, 1]

# Get CV from data
cv_error = df_example['cv_error'].iloc[0]

# Calculate true cost curve for plotting
lot_mp_range = np.linspace(df_example['lot_midpoint'].min() * 0.8,
                            df_example['lot_midpoint'].max() * 1.2, 100)
# Use median lot quantity for true curve
median_qty = df_example['lot_quantity'].median()
true_curve = true_T1 * (lot_mp_range ** true_b) * (median_qty ** true_c)
```

```{python}
#| label: tbl-scenario-example
#| tbl-cap: "Example Scenario Dataset"
#| echo: false
#| output: asis

from tabulate import tabulate

# Choose and format the columns you want to show
cols = ["lot_midpoint", "lot_quantity", "observed_cost"]
df_show = df_train[cols].copy()

# Format floats to a consistent precision
md = tabulate(
    df_show,
    headers="keys",
    tablefmt="pipe",   # Pandoc-friendly pipe table
    showindex=False,
    floatfmt=".3f"
)

print(md)
```

Using OLS, we get the following results:

```{python}
#| label: model-fits

from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer
from sklearn.compose import TransformedTargetRegressor
from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_squared_error
import penalized_constrained as pcreg



X_train = df_train[['lot_midpoint', 'lot_quantity']].values
y_train = df_train['observed_cost'].values
X_test = df_test[['lot_midpoint', 'lot_quantity']].values
y_test_true = df_test['true_cost'].values  # True underlying values
y_test_obs = df_test['observed_cost'].values  # Noisy observations

# ==== OLS Stats Model ===
import statsmodels.formula.api as smf
ols_sm = smf.ols(formula="np.log(observed_cost) ~ np.log(lot_midpoint) + np.log(lot_quantity)", data=df_train).fit()
    
# === OLS ===
ols = TransformedTargetRegressor(
    regressor=Pipeline([
        ('log', FunctionTransformer(np.log)),
        ('reg', LinearRegression()),
    ]),
    func=np.log,
    inverse_func=np.exp,
)
ols.fit(X_train, y_train)

ols_b, ols_c = ols.regressor_.named_steps['reg'].coef_
ols_lr = 2 ** ols_b
ols_re = 2 ** ols_c
ols_T1 = np.exp(ols.regressor_.named_steps['reg'].intercept_)
ols_pred_train = ols.predict(X_train)
ols_pred_test = ols.predict(X_test)
ols_r2_train = r2_score(y_train, ols_pred_train)
ols_mape_true = mean_absolute_percentage_error(y_test_true, ols_pred_test)
ols_resid_train = y_train - ols_pred_train
ols_resid_true = y_test_true - ols_pred_test

# === OLS Learn Only ===
X_train_learn = df_train[['lot_midpoint']].values
X_test_learn = df_test[['lot_midpoint']].values

ols_learn = TransformedTargetRegressor(
    regressor=Pipeline([
        ('log', FunctionTransformer(np.log)),
        ('reg', LinearRegression()),
    ]),
    func=np.log,
    inverse_func=np.exp,
)
ols_learn.fit(X_train_learn, y_train)

ols_learn_b = ols_learn.regressor_.named_steps['reg'].coef_[0]
ols_learn_lr = 2 ** ols_learn_b
ols_learn_T1 = np.exp(ols_learn.regressor_.named_steps['reg'].intercept_)
ols_learn_pred_train = ols_learn.predict(X_train_learn)
ols_learn_pred_test = ols_learn.predict(X_test_learn)
ols_learn_r2_train = r2_score(y_train, ols_learn_pred_train)
ols_learn_mape_true = mean_absolute_percentage_error(y_test_true, ols_learn_pred_test)
ols_learn_resid_train = y_train - ols_learn_pred_train
ols_learn_resid_true = y_test_true - ols_learn_pred_test

# === PCReg ===
def prediction_fn(X, params):
    T1, b, c = params
    return T1 * (X[:, 0] ** b) * (X[:, 1] ** c)

pc_gcv = pcreg.PenalizedConstrainedCV(
    coef_names=['T1', 'b', 'c'],
    bounds={'T1': (0, None), 'b': (-0.5, 0), 'c': (-0.5, 0)},
    prediction_fn=prediction_fn,
    fit_intercept=False,
    x0=[100, -0.1, -0.1],
    alphas=np.linspace(0, 5, 10),
    l1_ratios= [0.0, 0.5, 1.0],
    selection='gcv',
    loss='sspe',
    penalty_exclude=['T1'],
    n_jobs=1
)
pc_gcv.fit(X_train, y_train)

pc_T1, pc_b, pc_c = pc_gcv.coef_
pc_lr = 2 ** pc_b
pc_re = 2 ** pc_c
pc_pred_train = pc_gcv.predict(X_train)
pc_pred_test = pc_gcv.predict(X_test)
pc_r2_train = r2_score(y_train, pc_pred_train)
pc_mape_true = mean_absolute_percentage_error(y_test_true, pc_pred_test)
pc_resid_train = y_train - pc_pred_train
pc_resid_true = y_test_true - pc_pred_test

# Compute GDF early for inline references
_report_early = pcreg.generate_summary_report(
    pc_gcv, X_train, y_train,
    full=False,
    gdf_method='gaines',
    bootstrap=False
)
gdf = _report_early.fit_stats.gdf
n_active = _report_early.constraints.n_active
```

| Metric        |                                                        OLS |
|:----------------------|------------------------------------------------:|
| $T_1$         |                                 `{python} f'{ols_T1:.0f}'` |
| LCS           |                            `{python} f'{ols_lr*100:.1f}'`% |
| Rate Effect   |                            `{python} f'{ols_re*100:.1f}'`% |
| R²            |                           `{python} f'{ols_r2_train:.2f}'` |
| F-statistic   | `{python} f'{ols_sm.fvalue:.1f} (p={ols_sm.f_pvalue:.2})'` |

: Example Scenario OLS Results {#tbl-ols-results}

We find that by using OLS, this example dataset yields invalid coefficients, specifically an LCS of `{python} f'{ols_lr*100:.1f}'`% which violates our definition of LCS being $\leq 100\%$. But otherwise this is a good fit with strong R² (`{python} f'{ols_r2_train:.2f}'`) and significant F-statistic (`{python} f'p={ols_sm.f_pvalue:.2}'`). This is a common pattern in cost estimating where OLS produces good fit statistics but implausible coefficients. \newpage

# Theoretical Foundations {#sec-theory}

## Ridge Regularization

C. M. Theobald, in *Generalizations of Mean Square Error Applied to Ridge Regression* [@theobald1974ridge], showed that for any OLS problem there exists a ridge parameter $\lambda^* > 0$ such that the ridge estimator has strictly lower Mean Squared Error (MSE) than OLS. R. W. Farebrother, in *Further Results on the Mean Square Error of Ridge Regression* [@farebrother1976ridge], extended Theobald's result to broader classes of weighted MSE criteria. Together, these results establish that ridge shrinkage can always improve on OLS in terms of population MSE, not merely training error. The optimal $\lambda^*$ depends on unknown population parameters, which cross-validation estimates empirically.

## Lasso Regularization and Constrained Optimization

Our paper is not the first to explore combining regularization (L1 penalties) with constraint optimization. @james2020pac developed the *Penalized and Constrained* optimization method (PAC) *"to compute the solution path for high-dimensional, linearly-constrained criteria"*. The paper demonstrated that PAC methods remain superior to unconstrained alternatives even when constraints are imperfectly specified:

> *"PAC and relaxed PAC are surprisingly robust to random violations in the constraints... they were still both superior to their unconstrained counterparts for all values \[of constraint error\] and all settings."*

\newpage

# PCReg Framework

The PCReg framework addresses the Learning Curve Problem, highlighted by our @sec-example, by applying the @sec-theory research findings to the cost estimation domain. Because PCReg uses optimization rather than closed-form estimation, traditional OLS diagnostics are not directly available, so PCReg also provides specialized model diagnostics based on GDF, bootstrap inference, and cross-validation.

## PCReg Model

The PCReg Model integrates regularization penalties (Lasso [L1] and Ridge [L2]) with domain-knowledge constraints ($\theta_{\text{lower}} \leq \theta \leq \theta_{\text{upper}}$) to stabilize coefficient estimates, while ensuring they remain within plausible ranges.

::: {.callout-note appearance="minimal"}
**Notation:** We use $\theta$ (rather than $\beta$) to emphasize that PCReg handles both linear regression and nonlinear optimization problems. For standard linear models, $\theta$ represents regression coefficients; for nonlinear models (e.g., learning curves), $\theta$ represents the optimized parameters. See @sec-pcreg-notation in the Appendix for complete notation details.
:::

### Objective Function

The objective function minimized by PCReg combines a loss function with regularization penalties subject to coefficient bounds:

$$
\begin{aligned}
\text{Objective:} \quad & \underset{\theta}{\arg\min} \; \underbrace{\sum_{i=1}^{n} L\!\left(y_i, \hat{y}_i\right)}_{\text{Loss}} \;+\; \underbrace{\lambda \Big[\, \alpha \, \|\theta\|_1 \;+\; \frac{1-\alpha}{2} \, \|\theta\|_2^2 \,\Big]}_{\text{Elastic Net Penalty}} \\[10pt]
\text{Subject to:} \quad & \theta_{\text{lower}} \le \theta \le \theta_{\text{upper}} \quad \text{(parameter bounds)} \\[12pt]
\text{where:} \quad & \begin{array}[t]{rl}
\hat{y}_i = f(X_i, \theta) &= \text{predicted value} \\[4pt]
L(y_i, \hat{y}_i) &= \text{loss function (e.g., SSE, SSPE)} \\[4pt]
\lambda \ge 0 &= \text{regularization penalty} \\[4pt]
\alpha \in [0,1] &= \text{L1/L2 ratio} \\[4pt]
\|\theta\|_1 = \sum_j |\theta_j| &= \text{Lasso penalty} \\[4pt]
\|\theta\|_2^2 = \sum_j \theta_j^2 &= \text{Ridge penalty}
\end{array}
\end{aligned}
$$

The lower and upper bounds ($\theta_{\text{lower}}$ and $\theta_{\text{upper}}$) can be set to achieve loose bounds or tight bounds for the coefficient:

-   **Loose bounds** (e.g., 70%-100%): Wide range which allows flexibility while preventing egregious coefficient violations
-   **Tight bounds** (e.g., 85%-95%): Narrower range which incorporates strong prior knowledge but risks over-constraining coefficients

### Model Specification

Users can specify the following components of the PCReg model:

-   **Functional Form**: Defaults to linear (`X @ coef + intercept`), but any function can be entered
-   **Loss function**: Defaults to Sum of Squared Percentage Errors (SSPE), but alternatives include Sum of Squared Errors (SSE), MSE, or any custom loss function
-   **Coefficient bounds**: Componentwise constraints that encode domain knowledge (e.g., LCS between 70% and 100%)
-   **Penalty exclusion**: Option to exclude specific coefficients from regularization while still applying bounds
-   **Optimization Method**: Defaults to Sequential Least Squares Programming (SLSQP) for constrained optimization, but any optimization method can be used

The framework maximizes flexibility and collapses to known regression techniques in special cases:

-   OLS when functional form is linear, $\alpha=0$, and bounds are not specified
-   Ridge when functional form is linear, $\alpha>0$ and $\text{l1\_ratio}=0$
-   Lasso when functional form is linear, $\alpha>0$ and $\text{l1\_ratio}=1$
-   Elastic Net when functional form is linear, $\alpha>0$ and $0 < \text{l1\_ratio} < 1$
-   Constrained regression when bounds are specified but $\alpha=0$

### Cross Validation (CV) for Hyperparameter Selection

Finding the optimal penalty parameters for PCReg requires testing different values rather than using a formula. CV is a method that evaluates how well a model predicts new data by testing it on observations not used during training. The CV process evaluates all combinations of user-specified penalties ($\lambda$) and L1 ratio values ($\alpha$) to find the hyperparameters that produce the best out-of-sample predictions.

Traditional K-Fold CV [@stone1974cross] splits the training data into K subsets, repeatedly training on K-1 subsets and validating on the remaining one. For small datasets, this approach further reduces the already limited training sample size.

Generalized Cross Validation (GCV) approximates leave-one-out cross validation using a mathematical formula rather than actually splitting data [@golub1979generalized]. It estimates prediction error based on training residuals adjusted by model complexity (degrees of freedom), making it efficient for small samples.

PCReg implements both K-Fold CV and GCV, with GCV as the default for small datasets. For each combination of penalty parameters, the GCV score is calculated as:

$$
\begin{aligned}
\text{GCV} &= \frac{n \cdot \text{RSS}}{(n - \text{df})^2} \\[6pt]
\text{where:} \quad & \begin{array}[t]{rl}
\text{RSS} &= \text{Residual Sum of Squares} \\[4pt]
n &= \text{number of observations} \\[4pt]
\text{df} &= \text{effective degrees of freedom}
\end{array}
\end{aligned}
$$

The optimal penalty parameters are those that minimize the GCV score. See @sec-generalized-degrees-freedom for how PCReg calculates effective degrees of freedom for constrained and penalized models.

## Model Diagnostics

The flexibility of PCReg means that classical inference (p-values and F-statistics) cannot be guaranteed, because the framework supports coefficient bounds, regularization penalties, custom loss functions, and custom functional forms. No single inference framework covers all scenarios. When coefficients are at bounds (from constraints or L1 penalties), classical theory is no longer directly applicable. Additionally, introducing custom loss and functional forms requires specifying their corresponding distributional assumptions as in Generalized Linear Models (GLMs). We therefore focus on fit statistics that are robust and informative across model specifications.

### Optimization Diagnostics

We report optimization diagnostics such as convergence status, number of iterations, and whether a parameter is at a bound. When coefficients are at bounds, this indicates that constraints are actively shaping the solution. We have found that applying constraints can change parameters without forcing to a bound. Just because a coefficient is not at a bound does not mean it is unaffected by the constraints; constraints can still significantly impact the optimization process.

### In-Sample Fit Statistics

Standard in-sample fit measures remain valid across all model configurations, including Coefficient of Determination (R²), SSE, Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE), and residual diagnostics.

### Generalized Degrees of Freedom (GDF)-Adjusted Fit Statistics {#sec-generalized-degrees-freedom}

Generalized Cross Validation (GCV), Standard Error of Estimate (SEE), and Adjusted R² all rely on degrees of freedom. However, classical degrees of freedom ($df=n - p$) assumes unconstrained estimation. GDF suggests that optimization-based models should lose degrees of freedom to take into account the constraints. We implement both @gaines2018constrained and @hu2010gdf GDF formulas to properly adjust fit statistics (SEE, Standard Percentage Error (SPE), Adjusted R²) and account for the effective model complexity.

**Gaines et al. (2018) Formula (Default):** @gaines2018constrained derive degrees of freedom for constrained Lasso by counting only constraints that are actually binding at the solution:

$$
\text{GDF}_{\text{Gaines}} = n - |\text{Active predictors}| - |\text{Equality constraints}| - |\text{Binding inequality constraints}|
$$

where $n$ is the sample size and active predictors are non-zero coefficients. This reflects the intuition that non-binding constraints don't reduce effective degrees of freedom.

**Hu (2010) Alternative Formula:** @hu2010gdf proposes a more conservative approach where *all specified constraints* count against degrees of freedom, regardless of whether they bind:

$$
\text{GDF}_{\text{Hu}} = n - p - (\text{\# Constraints}) + (\text{\# Redundancies})
$$

where $p$ is the total number of estimated parameters (including intercept if fitted). Hu's method always reduces GDF when constraints are specified, making it appropriate for Minimum-Percentage Error under the Zero-Percentage Bias (ZMPE)-type CERs where constraints fundamentally shape the solution.

### Bootstrap Confidence Intervals

Standard bootstrap confidence intervals can be misleading for penalized regression [@goeman2025penalized], as they assume interior solutions and no regularization bias. Our framework addresses this by computing **both** constrained and unconstrained bootstrap resampling:

-   **Constrained bootstrap**: Resamples data and refits the model *with* the specified bounds and regularization (penalized-constrained estimation), quantifying coefficient stability under the imposed modeling constraints

-   **Unconstrained bootstrap**: Resamples data and fits OLS *without* bounds or penalization (pure unconstrained estimation), showing coefficient variability in the absence of constraints and regularization. This serves as a baseline for comparison.

## Software Implementation

The DoD cost estimating community can apply PCReg using either a Python Package or Excel Template.

### Python Package

We developed a full feature Python package, [`penalized_constrained`](https://github.com/frankij11/Penalized-Constrained-Regression), following scikit-learn conventions and API design principles. The package provides a comprehensive implementation of the PCReg framework with support for model fitting, hyperparameter selection via cross-validation, diagnostic reporting with GDF-adjusted fit statistics, and bootstrap confidence intervals. See Appendix @sec-appendix-software for additional information.

### Excel Template

To make the methodology accessible to practitioners without programming expertise, we developed an [Excel Template](https://github.com/frankij11/Penalized-Constrained-Regression/raw/main/PCReg_Template.xlsx) that implements a simplified version of the PCReg algorithm. The template provides an intuitive interface for applying PCReg to cost estimation problems. Visit the [GitHub repository](https://github.com/frankij11/Penalized-Constrained-Regression) for more information.

## Example Scenario

Now we will apply PCReg to the example scenario introduced earlier.

### PCReg Model Specification

We apply the following PCReg Model Specification to the example scenario:

```{python}
#| label: pcreg-gcv-spec-revisited
#| echo: true
#| eval: false

# Define the nonlinear functional form
def prediction_fn(X, params):
    """Learning curve with rate effect: T1 * (midpoint^b) * (quantity^c)"""
    T1, b, c = params
    return T1 * (X[:, 0] ** b) * (X[:, 1] ** c)

# Configure PCReg model
pc_gcv = pcreg.PenalizedConstrainedCV(
    coef_names=['T1', 'b', 'c'],
    bounds={'T1': (0, None), 'b': (-0.5, 0), 'c': (-0.5, 0)},
    prediction_fn=prediction_fn,
    fit_intercept=False,
    x0=[100, -0.1, -0.1],
    alphas=np.linspace(0, 5, 10),  # Range of lambda values to test
    l1_ratios= [0.0, 0.5, 1.0],  # Test Ridge, Elastic Net, and Lasso penalties
    selection='gcv',           # Generalized Cross-Validation
    loss='sspe',               # Sum of Squared Percentage Errors
    penalty_exclude=['T1'],    # Don't penalize the intercept
    n_jobs=1
)

# Fit the model
pc_gcv.fit(X_train, y_train)
```

*Note:* Due to scikit-learn API conventions, $\lambda$ is labeled `alpha` and $\alpha$ is labeled `l1_ratio` in the code. These correspond to regularization strength and L1/L2 mixing ratio in the theoretical formulation.

**Key Specification Details:**

-   **Coefficients**: $T_1$ (first unit cost), $b$ (learning slope), $c$ (rate slope)
-   **Bounds**: $T_1 > 0$, $-0.5 \leq b \leq 0$, $-0.5 \leq c \leq 0$
-   **alphas (penalty)**: Range between 0 and 5 using linear spacing to explore Lasso, Ridge, and Elastic Net penalties
-   **l1_ratios**: Range between 0 and 1 to explore different mixes of L1 and L2 regularization
-   **Penalty Selection**: GCV automatically selects optimal $\lambda$ without data splitting
-   **Loss Function**: SSPE minimizes percentage errors (appropriate for cost data)
-   **Penalty Exclusion**: $T_1$ is not penalized (only slopes are regularized)

Below is a comparison of OLS and PCReg specifications used for this example scenario:

| Specification    |        OLS |                                PCReg |
|:-----------------|-----------:|-------------------------------------:|
| loss function    |        SSE |                                 SSPE |
| functional form  | Log-Linear |           Nonlinear                  |
| lambda (penalty) |        N/A |    `{python} f'{pc_gcv.alpha_:.4f}'` |
| alpha (L1 ratio) |        N/A | `{python} f'{pc_gcv.l1_ratio_:.2f}'` |

: Comparison of OLS and PCReg coefficient Specification {#tbl-ols-pcreg-comparison}

### PCReg Model Results

Using OLS and PCReg, we get the following results:

| Metric | OLS | PCReg |
|:-----------------|--------------------------:|--------------------------:|
| $T_1$ | `{python} f'{ols_T1:.0f}'` | `{python} f'{pc_T1:.0f}'` |
| LCS | `{python} f'{ols_lr*100:.1f}'`% | `{python} f'{pc_lr*100:.1f}'`% |
| Rate Effect | `{python} f'{ols_re*100:.1f}'`% | `{python} f'{pc_re*100:.1f}'`% |
| R² (Train) | `{python} f'{ols_r2_train:.2f}'` | `{python} f'{pc_r2_train:.2f}'` |

: Comparison of OLS and PCReg coefficient estimates {#tbl-ols-pcreg-results}

We see that the PCReg model yields a plausible LCS (`{python} f'{pc_lr*100:.1f}'`%), however the R² is slightly lower than the OLS regression. 

### PCReg Model Diagnostics

We were able to find a reasonable solution with PCReg that satisfies our domain knowledge constraints (LCS and Rate Effect $\leq 100\%$) and we still have a reasonable R². Now we must assess the stability of the coefficients. The PCReg model includes bootstrap diagnostics to assess coefficient stability and the impact of constraints. Comparing constrained and unconstrained bootstrap distributions reveals the impact of constraints on coefficient stability. When the constrained and unconstrained distributions differ substantially, constraints are actively shaping the solution. When they are similar, the data naturally satisfies the constraints. The constrained and unconstrained bootstrap results for each coefficient are summarized below.

```{python}
#| label: model-diagnostics

# Generate diagnostic report with bootstrap
report = pcreg.generate_summary_report(
    pc_gcv, X_train, y_train,
    full=True,
    gdf_method='gaines',
    bootstrap=True,
    n_bootstrap=500,
    random_state=42
)

# Extract key diagnostic values
gdf = report.fit_stats.gdf
gdf_method = report.fit_stats.gdf_method
n_active = report.constraints.n_active
active_constraints = report.constraints.active_constraints

# Bootstrap results
boot = report.bootstrap_results
boot_const = boot.constrained if boot else None
boot_unconst = boot.unconstrained if boot else None
```

```{python}
#| label: bootstrap-summary

if boot_const is not None and boot_unconst is not None:
    coef_names = ['T1', 'b', 'c']
    # Constrained bootstrap stats
    const_means = boot_const.coef_mean
    const_stds = boot_const.coef_std
    const_ranges = [(boot_const.coef_ci_lower[i], boot_const.coef_ci_upper[i]) for i in range(3)]
    # Unconstrained bootstrap stats
    unconst_means = boot_unconst.coef_mean
    unconst_stds = boot_unconst.coef_std
    unconst_ranges = [(boot_unconst.coef_ci_lower[i], boot_unconst.coef_ci_upper[i]) for i in range(3)]
```

| Coefficient | Mean | Std | 95% CI |
|:-------------------|-----------------:|-----------------:|:-----------------|
| $T_1$ | `{python} f'{const_means[0]:.2f}'` | `{python} f'{const_stds[0]:.2f}'` | \[`{python} f'{const_ranges[0][0]:.2f}'`, `{python} f'{const_ranges[0][1]:.2f}'`\] |
| $b$ | `{python} f'{const_means[1]:.4f}'` | `{python} f'{const_stds[1]:.4f}'` | \[`{python} f'{const_ranges[1][0]:.4f}'`, `{python} f'{const_ranges[1][1]:.4f}'`\] |
| $c$ | `{python} f'{const_means[2]:.4f}'` | `{python} f'{const_stds[2]:.4f}'` | \[`{python} f'{const_ranges[2][0]:.4f}'`, `{python} f'{const_ranges[2][1]:.4f}'`\] |

: Constrained Bootstrap (with bounds and regularization) {#tbl-boot-constrained}

| Coefficient | Mean | Std | 95% CI |
|:-------------------|-----------------:|-----------------:|:-----------------|
| $T_1$ | `{python} f'{unconst_means[0]:.2f}'` | `{python} f'{unconst_stds[0]:.2f}'` | \[`{python} f'{unconst_ranges[0][0]:.2f}'`, `{python} f'{unconst_ranges[0][1]:.2f}'`\] |
| $b$ | `{python} f'{unconst_means[1]:.4f}'` | `{python} f'{unconst_stds[1]:.4f}'` | \[`{python} f'{unconst_ranges[1][0]:.4f}'`, `{python} f'{unconst_ranges[1][1]:.4f}'`\] |
| $c$ | `{python} f'{unconst_means[2]:.4f}'` | `{python} f'{unconst_stds[2]:.4f}'` | \[`{python} f'{unconst_ranges[2][0]:.4f}'`, `{python} f'{unconst_ranges[2][1]:.4f}'`\] |

: Unconstrained Bootstrap (no bounds, $\alpha$=0) {#tbl-boot-unconstrained}

@fig-bootstrap-kde shows the Bootstrap coefficient distributions comparing constrained (blue) vs unconstrained (red) estimation. Vertical lines show fitted values. Constraints reduce variance and keep estimates within economically plausible ranges.

```{python}
#| label: fig-bootstrap-kde
#| fig-cap: "Bootstrap Coefficient Distributions"

if boot_const is not None and boot_unconst is not None:
    fig, axes = plt.subplots(1, 3, figsize=(14, 4))
    coef_names = ['T1', 'b', 'c']
    fitted_vals = pc_gcv.coef_

    for i, (ax, name) in enumerate(zip(axes, coef_names)):
        const_samples = boot_const.bootstrap_coefs[:, i]
        unconst_samples = boot_unconst.bootstrap_coefs[:, i]

        # Plot KDEs
        sns.kdeplot(const_samples, ax=ax, label='Constrained', color='#3498db', fill=True, alpha=0.4)
        sns.kdeplot(unconst_samples, ax=ax, label='Unconstrained', color='#e74c3c', fill=True, alpha=0.3)

        # Add fitted value line
        ax.axvline(fitted_vals[i], color='black', linestyle='--', linewidth=2, label=f'Fitted={fitted_vals[i]:.4f}')

        ax.set_xlabel(name)
        ax.set_ylabel('Density')
        ax.set_title(f'{name} Bootstrap Distribution')
        ax.legend(fontsize=8)

    plt.tight_layout()
    plt.show()
```

The bootstrap results above reveal three key diagnostic insights:

1.  **Constraints at bounds:** When bootstrap samples frequently hit constraint boundaries, the data is pulling towards implausible values, which is exactly when constraints help most.
2.  **Variance reduction:** The constrained bootstrap shows tighter distributions than the unconstrained, reducing coefficient uncertainty at the cost of some bias.
3.  **Divergence indicates constraint impact:** The differences between constrained and unconstrained means indicate how much the constraints are actively shaping the solution.

```{python}
#| label: save-diagnostic-report
#| output: false

# Save HTML diagnostic report for interactive exploration
_ = report.to_html('pcreg_diagnostic_report.html', X=X_train, y=y_train)
```

A comprehensive diagnostic report with full model specifications and bootstrap distributions is provided in Appendix @sec-appendix-diagnostic.

\newpage

# Simulation Study {#sec-simulation}

```{python}
#| label: simulation-design-params

# Extract unique factor levels from results
n_lots_levels = sorted(results['n_lots'].unique())
cv_error_levels = sorted(results['cv_error'].unique())
learning_rate_levels = sorted(results['learning_rate'].unique())
rate_effect_levels = sorted(results['rate_effect'].unique())

# Calculate simulation size
n_combinations = len(n_lots_levels) * len(cv_error_levels) * len(learning_rate_levels) * len(rate_effect_levels)
n_replications = len(results[results['model_name'] == 'OLS']) // n_combinations
n_scenarios = n_combinations * n_replications

# Format levels for display
n_lots_str = ', '.join(map(str, n_lots_levels))
cv_error_str = ', '.join(map(str, cv_error_levels))
learning_rate_str = ', '.join([f'{lr*100:.0f}%' for lr in learning_rate_levels])
rate_effect_str = ', '.join([f'{re*100:.0f}%' for re in rate_effect_levels])
```

We performed a Monte Carlo simulation study to evaluate PCReg and compare with traditional OLS.

## Simulation Design

Our simulation study specifies four factors and generates `{python} f'{n_scenarios:,}'` scenarios (`{python} n_combinations` factor combinations × `{python} n_replications` replications). The four factors and their corresponding levels are provided below:

| Factor                  | Levels                       |
|:------------------------|:-----------------------------|
| Number of Training Lots | `{python} n_lots_str`        |
| CoV error               | `{python} cv_error_str`      |
| LCS           | `{python} learning_rate_str` |
| Rate effect             | `{python} rate_effect_str`   |

: Simulation study factorial design {#tbl-design}

For each scenario, we:

1.  **Randomly selected a quantity profile** from the Selected Acquisition Report (SAR) database (actual defense program procurement histories)

2.  **Generated simulated average unit costs** using the learning curve model (@eq-learning-curve) with the scenario's true parameters

3.  **Added multiplicative lognormal noise** with the specified Coefficient of Variation (CoV)

4.  **Split data** with first $n$ training lots used for model fitting and remaining lots reserved for testing

5.  **Fit models** to the training data using the following approaches:

    1.  **OLS**: Learning curve with rate effect
    2.  **OLS-LearnOnly**: Learning curve without rate effect, representing the common remedy of dropping a correlated predictor
    3.  **PCReg**: Employed default PCReg configuration


6.  **Evaluated Models** comparing the estimated coefficients against the true parameters, measuring coefficient recovery and out-of-sample prediction accuracy

This approach ensures realistic lot structures (varying quantities, realistic ramp-up patterns) while controlling the true underlying parameters. The number of test lots varies by scenario depending on the program's total lot history. Some programs have many available lots beyond training, others have few or none. This variability is acceptable as it reflects real-world conditions.

## Example Scenario

The following tables describe the example scenario. @tbl-scenario-factors shows the simulation factor levels selected for this scenario from the factorial design (@tbl-design), and @tbl-scenario-derived shows the derived characteristics specific to this scenario.

| Factor                  |                            Value |
|:------------------------|---------------------------------:|
| Number of Training Lots |               `{python} n_train` |
| CoV Error               |         `{python} f'{cv_error}'` |
| True LCS      | `{python} f'{true_lr*100:.1f}'`% |
| True Rate Effect        | `{python} f'{true_re*100:.1f}'`% |

: Example Scenario Simulation Factors {#tbl-scenario-factors}

| Characteristic        |                       Value |
|:----------------------|----------------------------:|
| Total lots (from SAR) | `{python} n_train + n_test` |
| Test lots             |           `{python} n_test` |
| True $T_1$            | `{python} f'{true_T1:.0f}'` |
| Predictor Correlation |    `{python} f'{corr:.2f}'` |

: Example Scenario Derived Characteristics {#tbl-scenario-derived}

Below is the example scenario simulation dataset with training and test lots. The training data matches the dataset shown earlier, but now we also see the test lots with their true underlying costs.

```{python}
#| label: tbl-scenario-example-simulation
#| tbl-cap: "Motivating example dataset with training and test lots"
#| echo: false
#| output: asis

from tabulate import tabulate

# Choose and format the columns you want to show
cols = ["lot_type", "lot_midpoint", "lot_quantity", "observed_cost", "true_cost"]
df_show = df_example[cols].copy()

# Format values
train_mask = df_show['lot_type'] == 'train'

# Convert observed_cost to object type to allow mixed string/numeric values
df_show['observed_cost'] = df_show['observed_cost'].astype(object)

# Hide observed costs for test rows
df_show.loc[~train_mask, 'observed_cost'] = '—'

# Format numeric columns
for col in ['lot_midpoint', 'lot_quantity', 'true_cost']:
    df_show[col] = df_show[col].apply(lambda x: f"{x:.3f}" if isinstance(x, float) else str(x))

# Format observed_cost only for training rows (test rows already have '—')
df_show.loc[train_mask, 'observed_cost'] = df_show.loc[train_mask, 'observed_cost'].apply(
    lambda x: f"{x:.3f}" if isinstance(x, float) else str(x)
)

md = tabulate(
    df_show,
    headers="keys",
    tablefmt="pipe",
    showindex=False,
)

print(md)
```

@fig-data shows the learning curve data with `{python} n_train` training lots (blue) and the true underlying cost distribution (dashed line). Point sizes reflect lot quantities. The goal is to predict the true relationship, not just fit the noisy observations.

```{python}
#| label: fig-data
#| fig-cap: "Example Scenario Learning Curve"

fig, ax = plt.subplots(figsize=(8, 5))

# Plot true underlying curve
ax.plot(lot_mp_range, true_curve, 'k--', linewidth=2, label='True Cost Function', zorder=1)

# Plot training data with size proportional to lot quantity
train_sizes = (df_train['lot_quantity'] / df_train['lot_quantity'].max()) * 200 + 50
ax.scatter(df_train['lot_midpoint'], df_train['observed_cost'],
           s=train_sizes, c='#1f77b4', alpha=0.8, edgecolors='black',
           label=f'Training (n={n_train})', zorder=3)

# Plot test data with size proportional to lot quantity
test_sizes = (df_test['lot_quantity'] / df_test['lot_quantity'].max()) * 200 + 50
ax.scatter(df_test['lot_midpoint'], df_test['true_cost'],
           s=test_sizes, c='#ff7f0e', alpha=0.6, edgecolors='black',
           label=f'True Test Values (n={n_test})', zorder=2)

ax.set_xlabel('Lot Midpoint (Cumulative Units)')
ax.set_ylabel('Cost ($)')
ax.set_title('Learning Curve: Training Data vs True Distribution')
ax.legend()

# Standard units, starting near 0
ax.set_xlim(0, df_example['lot_midpoint'].max() * 1.1)
ax.set_ylim(0, df_example['observed_cost'].max() * 1.2)

plt.tight_layout()
plt.show()
```

```{python}
#| label: model-fits-example

from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer
from sklearn.compose import TransformedTargetRegressor
from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_squared_error
import penalized_constrained as pcreg

X_train = df_train[['lot_midpoint', 'lot_quantity']].values
y_train = df_train['observed_cost'].values
X_test = df_test[['lot_midpoint', 'lot_quantity']].values
y_test_true = df_test['true_cost'].values  # True underlying values
y_test_obs = df_test['observed_cost'].values  # Noisy observations

# === OLS ===
ols = TransformedTargetRegressor(
    regressor=Pipeline([
        ('log', FunctionTransformer(np.log)),
        ('reg', LinearRegression()),
    ]),
    func=np.log,
    inverse_func=np.exp,
)
ols.fit(X_train, y_train)

ols_b, ols_c = ols.regressor_.named_steps['reg'].coef_
ols_lr = 2 ** ols_b
ols_re = 2 ** ols_c
ols_T1 = np.exp(ols.regressor_.named_steps['reg'].intercept_)
ols_pred_train = ols.predict(X_train)
ols_pred_test = ols.predict(X_test)
ols_r2_train = r2_score(y_train, ols_pred_train)
ols_mape_true = mean_absolute_percentage_error(y_test_true, ols_pred_test)
ols_resid_train = y_train - ols_pred_train
ols_resid_true = y_test_true - ols_pred_test

# === OLS Learn Only ===
X_train_learn = df_train[['lot_midpoint']].values
X_test_learn = df_test[['lot_midpoint']].values

ols_learn = TransformedTargetRegressor(
    regressor=Pipeline([
        ('log', FunctionTransformer(np.log)),
        ('reg', LinearRegression()),
    ]),
    func=np.log,
    inverse_func=np.exp,
)
ols_learn.fit(X_train_learn, y_train)

ols_learn_b = ols_learn.regressor_.named_steps['reg'].coef_[0]
ols_learn_lr = 2 ** ols_learn_b
ols_learn_T1 = np.exp(ols_learn.regressor_.named_steps['reg'].intercept_)
ols_learn_pred_train = ols_learn.predict(X_train_learn)
ols_learn_pred_test = ols_learn.predict(X_test_learn)
ols_learn_r2_train = r2_score(y_train, ols_learn_pred_train)
ols_learn_mape_true = mean_absolute_percentage_error(y_test_true, ols_learn_pred_test)
ols_learn_resid_train = y_train - ols_learn_pred_train
ols_learn_resid_true = y_test_true - ols_learn_pred_test

# === PCReg ===
def prediction_fn(X, params):
    T1, b, c = params
    return T1 * (X[:, 0] ** b) * (X[:, 1] ** c)

pc_gcv = pcreg.PenalizedConstrainedCV(
    coef_names=['T1', 'b', 'c'],
    bounds={'T1': (0, None), 'b': (-0.5, 0), 'c': (-0.5, 0)},
    prediction_fn=prediction_fn,
    fit_intercept=False,
    x0=[100, -0.1, -0.1],
    selection='gcv',
    loss='sspe',
    penalty_exclude=['T1'],
    n_jobs=1
)
pc_gcv.fit(X_train, y_train)

pc_T1, pc_b, pc_c = pc_gcv.coef_
pc_lr = 2 ** pc_b
pc_re = 2 ** pc_c
pc_pred_train = pc_gcv.predict(X_train)
pc_pred_test = pc_gcv.predict(X_test)
pc_r2_train = r2_score(y_train, pc_pred_train)
pc_mape_true = mean_absolute_percentage_error(y_test_true, pc_pred_test)
pc_resid_train = y_train - pc_pred_train
pc_resid_true = y_test_true - pc_pred_test
```

OLS results in a *higher* Train R² (`{python} f'{ols_r2_train:.3f}'`) than PCReg (`{python} f'{pc_r2_train:.3f}'`). This is expected because penalties and constraints add bias to the training fit. However, PCReg results in a lower (better) MAPE on on out-of-sample Test data, meaning its predictions are closer to the true underlying costs. For the remainder of the paper we will refer to MAPE on Test data as Test MAPE.

| Metric | True | OLS | OLS-LearnOnly | PCReg |
|:--------------|--------------:|--------------:|--------------:|--------------:|
| $T_1$ | `{python} f'{true_T1:.0f}'` | `{python} f'{ols_T1:.0f}'` | `{python} f'{ols_learn_T1:.0f}'` | `{python} f'{pc_T1:.0f}'` |
| LCS | `{python} f'{true_lr*100:.1f}'`% | `{python} f'{ols_lr*100:.1f}'`% | `{python} f'{ols_learn_lr*100:.1f}'`% | `{python} f'{pc_lr*100:.1f}'`% |
| Rate Effect | `{python} f'{true_re*100:.1f}'`% | `{python} f'{ols_re*100:.1f}'`% | -- | `{python} f'{pc_re*100:.1f}'`% |
| Valid Coefficients | Yes | `{python} 'NO' if ols_lr > 1 or ols_re < 0.7 or ols_re > 1 else 'Yes'` | `{python} 'NO' if ols_learn_lr > 1 else 'Yes'` | Yes |
| Train R² | -- | `{python} f'{ols_r2_train:.3f}'` | `{python} f'{ols_learn_r2_train:.3f}'` | `{python} f'{pc_r2_train:.3f}'` |
| Test MAPE (vs True) | -- | `{python} f'{ols_mape_true*100:.1f}'`% | `{python} f'{ols_learn_mape_true*100:.1f}'`% | `{python} f'{pc_mape_true*100:.1f}'`% |

: Comparison of estimation methods. {#tbl-comparison}

@fig-residuals shows the Residuals for each model on training data (left) and against true test values (right). PCReg shows larger training residuals but smaller errors when predicting the true underlying relationship.

```{python}
#| label: fig-residuals
#| fig-cap: "Example Scenario Residuals"

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

models = ['OLS', 'OLS-LearnOnly', 'PCReg']
x_pos = np.arange(len(models))
colors = ['#e74c3c', '#9b59b6', '#3498db']

# Training residuals scatter
ax1 = axes[0]
train_residuals = [ols_resid_train, ols_learn_resid_train, pc_resid_train]
for i, (resid, color) in enumerate(zip(train_residuals, colors)):
    jitter = np.random.normal(0, 0.05, size=len(resid))
    ax1.scatter(np.full_like(resid, x_pos[i]) + jitter, resid, color=color, alpha=0.7, edgecolors='black', s=40)
ax1.axhline(0, color='black', linestyle='--', linewidth=1)
ax1.set_xticks(x_pos)
ax1.set_xticklabels(models)
ax1.set_ylabel('Residual ($)')
ax1.set_title('Training Residuals')

# Test residuals scatter (vs true)
ax2 = axes[1]
test_residuals = [ols_resid_true, ols_learn_resid_true, pc_resid_true]
for i, (resid, color) in enumerate(zip(test_residuals, colors)):
    jitter = np.random.normal(0, 0.05, size=len(resid))
    ax2.scatter(np.full_like(resid, x_pos[i]) + jitter, resid, color=color, alpha=0.7, edgecolors='black', s=40)
ax2.axhline(0, color='black', linestyle='--', linewidth=1)
ax2.set_xticks(x_pos)
ax2.set_xticklabels(models)
ax2.set_ylabel('Residual ($)')
ax2.set_title('Test Residuals vs True Distribution')

plt.tight_layout()
plt.show()
```

\newpage

# Key Findings

```{python}
#| label: analysis-setup

from scipy import stats
from tabulate import tabulate

# Create comparison dataframe for OLS vs PCReg
ols_df = results[results['model_name'] == 'OLS'][['seed', 'test_mape', 'test_sspe', 'ols_reasonable',
                                                    'n_lots', 'cv_error', 'learning_rate', 'rate_effect',
                                                    'actual_correlation', 'T1_est', 'b', 'c',
                                                    'T1_true', 'b_true', 'c_true']].copy()
ols_df = ols_df.rename(columns={'test_mape': 'ols_mape', 'test_sspe': 'ols_sspe',
                                 'T1_est': 'ols_T1', 'b': 'ols_b', 'c': 'ols_c'})

pcreg_df = results[results['model_name'] == 'PCReg_GCV'][['seed', 'test_mape', 'test_sspe',
                                                          'T1_est', 'b', 'c']].copy()
pcreg_df = pcreg_df.rename(columns={'test_mape': 'pcreg_mape', 'test_sspe': 'pcreg_sspe',
                                     'T1_est': 'pcreg_T1', 'b': 'pcreg_b', 'c': 'pcreg_c'})

ols_learn_df = results[results['model_name'] == 'OLS_LearnOnly'][['seed', 'test_mape', 'test_sspe']].copy()
ols_learn_df = ols_learn_df.rename(columns={'test_mape': 'ols_learn_mape', 'test_sspe': 'ols_learn_sspe'})

# Merge all
comparison = ols_df.merge(pcreg_df, on='seed').merge(ols_learn_df, on='seed')

# Calculate Learning Curve (LC) and Rate Curve (RC) estimates (slope scale: 2^b)
comparison['ols_LC'] = 2 ** comparison['ols_b']
comparison['ols_RC'] = 2 ** comparison['ols_c']
comparison['pcreg_LC'] = 2 ** comparison['pcreg_b']
comparison['pcreg_RC'] = 2 ** comparison['pcreg_c']
comparison['LC_true'] = 2 ** comparison['b_true']
comparison['RC_true'] = 2 ** comparison['c_true']

# Calculate absolute errors in slope scale (LC/RC)
comparison['ols_T1_ape'] = np.abs(comparison['ols_T1'] - comparison['T1_true']) / comparison['T1_true']
comparison['pcreg_T1_ape'] = np.abs(comparison['pcreg_T1'] - comparison['T1_true']) / comparison['T1_true']
comparison['ols_LC_ae'] = np.abs(comparison['ols_LC'] - comparison['LC_true'])
comparison['pcreg_LC_ae'] = np.abs(comparison['pcreg_LC'] - comparison['LC_true'])
comparison['ols_RC_ae'] = np.abs(comparison['ols_RC'] - comparison['RC_true'])
comparison['pcreg_RC_ae'] = np.abs(comparison['pcreg_RC'] - comparison['RC_true'])

# Calculate win indicators
comparison['pcreg_wins_mape'] = comparison['pcreg_mape'] < comparison['ols_mape']
comparison['pcreg_wins_sspe'] = comparison['pcreg_sspe'] < comparison['ols_sspe']

# Win rates
overall_win_mape = comparison['pcreg_wins_mape'].mean() * 100
win_reasonable_mape = comparison[comparison['ols_reasonable'] == True]['pcreg_wins_mape'].mean() * 100
win_unreasonable_mape = comparison[comparison['ols_reasonable'] == False]['pcreg_wins_mape'].mean() * 100
n_reasonable = comparison['ols_reasonable'].sum()
n_unreasonable = (~comparison['ols_reasonable']).sum()
pct_unreasonable = n_unreasonable / len(comparison) * 100
```

## PCReg vs OLS

Across `{python} f'{len(comparison):,}'` simulation scenarios, OLS produced economically unreasonable coefficients, as measured by LCS or rate effect outside 70-100%, in `{python} f'{n_unreasonable:,}'` scenarios which is **`{python} f'{pct_unreasonable:.1f}'`%** of cases. Win rates are determined by Test MAPE score, where the model with the lower Test MAPE wins.

| OLS Coefficients | Scenarios | % of Scenarios | OLS Win Rate | PCReg Win Rate  |
|:-------------------|------------------:|------------------:|------------------:|--------------------------------:|
| Reasonable (70-100%) | `{python} f'{n_reasonable:,}'` | `{python} f'{n_reasonable / len(comparison) * 100:.1f}'`% | `{python} f'{100 - win_reasonable_mape:.1f}'`% | `{python} f'{win_reasonable_mape:.1f}'`% |
| Unreasonable (\<70% or \>100%) | `{python} f'{n_unreasonable:,}'` | `{python} f'{pct_unreasonable:.1f}'`% | `{python} f'{100 - win_unreasonable_mape:.1f}'`% | `{python} f'{win_unreasonable_mape:.1f}'`% |
| **Overall** | `{python} f'{len(comparison):,}'` | 100.0% | `{python} f'{100 - overall_win_mape:.1f}'`% | `{python} f'{overall_win_mape:.1f}'`% |

: PCReg vs OLS {#tbl-winrates}

@fig-kde-mape shows the distribution of Test MAPE for OLS vs PCReg, stratified by whether OLS produced reasonable coefficients. When unreasonable (right), OLS shows a heavy right tail of large errors that PCReg avoids.

```{python}
#| label: fig-kde-mape
#| fig-cap: "Distribution of Test MAPE"

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# Reasonable coefficients
ax1 = axes[0]
reasonable_data = comparison[comparison['ols_reasonable'] == True]
sns.kdeplot(data=reasonable_data, x='ols_mape', ax=ax1, label='OLS', color='#e74c3c', fill=True, alpha=0.3)
sns.kdeplot(data=reasonable_data, x='pcreg_mape', ax=ax1, label='PCReg', color='#3498db', fill=True, alpha=0.3)
ax1.set_xlabel('Test MAPE')
ax1.set_ylabel('Density')
ax1.set_title(f'OLS Reasonable (n={len(reasonable_data):,})')
ax1.legend()
ax1.set_xlim(0, reasonable_data[['ols_mape', 'pcreg_mape']].quantile(0.95).max())

# Unreasonable coefficients
ax2 = axes[1]
unreasonable_data = comparison[comparison['ols_reasonable'] == False]
sns.kdeplot(data=unreasonable_data, x='ols_mape', ax=ax2, label='OLS', color='#e74c3c', fill=True, alpha=0.3)
sns.kdeplot(data=unreasonable_data, x='pcreg_mape', ax=ax2, label='PCReg', color='#3498db', fill=True, alpha=0.3)
ax2.set_xlabel('Test MAPE')
ax2.set_ylabel('Density')
ax2.set_title(f'OLS Unreasonable (n={len(unreasonable_data):,})')
ax2.legend()
ax2.set_xlim(0, unreasonable_data[['ols_mape', 'pcreg_mape']].quantile(0.95).max())

plt.tight_layout()
plt.show()
```


### PCReg Significantly Outperforms OLS When Coefficients Are Unreasonable

When OLS produces unreasonable coefficients, PCReg wins **`{python} f'{win_unreasonable_mape:.1f}'`%** of scenarios on Test MAPE.

```{python}
#| label: tbl-unreasonable
#| tbl-cap: Performance comparison when OLS produces unreasonable coefficients
#| echo: false
#| output: asis


# Filter to unreasonable scenarios
unreasonable = comparison[comparison['ols_reasonable'] == False].copy()

# Statistical tests for unreasonable scenarios
wilcox_mape_unreas = stats.wilcoxon(unreasonable['ols_mape'], unreasonable['pcreg_mape'], alternative='greater')
ttest_mape_unreas = stats.ttest_rel(unreasonable['ols_mape'], unreasonable['pcreg_mape'], alternative='greater')

# Build comparison table for unreasonable scenarios
metrics_unreas = {
    'Metric': ['Test MAPE', 'T1 APE', 'LC Abs Error', 'RC Abs Error'],
    'OLS Mean': [
        unreasonable['ols_mape'].mean(),
        unreasonable['ols_T1_ape'].mean(),
        unreasonable['ols_LC_ae'].mean(),
        unreasonable['ols_RC_ae'].mean()
    ],
    'PCReg Mean': [
        unreasonable['pcreg_mape'].mean(),
        unreasonable['pcreg_T1_ape'].mean(),
        unreasonable['pcreg_LC_ae'].mean(),
        unreasonable['pcreg_RC_ae'].mean()
    ],
    'OLS Median': [
        unreasonable['ols_mape'].median(),
        unreasonable['ols_T1_ape'].median(),
        unreasonable['ols_LC_ae'].median(),
        unreasonable['ols_RC_ae'].median()
    ],
    'PCReg Median': [
        unreasonable['pcreg_mape'].median(),
        unreasonable['pcreg_T1_ape'].median(),
        unreasonable['pcreg_LC_ae'].median(),
        unreasonable['pcreg_RC_ae'].median()
    ]
}
df_unreas = pd.DataFrame(metrics_unreas)

# Format for display
df_unreas_display = df_unreas.copy()
for col in ['OLS Mean', 'PCReg Mean', 'OLS Median', 'PCReg Median']:
    df_unreas_display[col] = df_unreas_display[col].apply(lambda x: f'{x:.4f}')

print(tabulate(df_unreas_display, headers='keys', tablefmt='pipe', showindex=False))
```

### PCReg Performs Comparably When OLS Coefficients Are Reasonable

When OLS produces reasonable coefficients (n=`{python} f'{n_reasonable:,}'`), OLS "wins" on Test MAPE in **`{python} f'{100 - win_reasonable_mape:.1f}'`%** of scenarios. However, the performance differences are negligible in practical terms.

```{python}
#| label: tbl-reasonable
#| tbl-cap: Performance comparison when OLS produces reasonable coefficients
#| echo: false
#| output: asis


# Filter to reasonable scenarios
reasonable = comparison[comparison['ols_reasonable'] == True].copy()

# Statistical tests for reasonable scenarios
wilcox_mape_reas = stats.wilcoxon(reasonable['ols_mape'], reasonable['pcreg_mape'])
ttest_mape_reas = stats.ttest_rel(reasonable['ols_mape'], reasonable['pcreg_mape'])

# Build comparison table for reasonable scenarios
metrics_reas = {
    'Metric': ['Test MAPE', 'T1 APE', 'LC Abs Error', 'RC Abs Error'],
    'OLS Mean': [
        reasonable['ols_mape'].mean(),
        reasonable['ols_T1_ape'].mean(),
        reasonable['ols_LC_ae'].mean(),
        reasonable['ols_RC_ae'].mean()
    ],
    'PCReg Mean': [
        reasonable['pcreg_mape'].mean(),
        reasonable['pcreg_T1_ape'].mean(),
        reasonable['pcreg_LC_ae'].mean(),
        reasonable['pcreg_RC_ae'].mean()
    ],
    'OLS Median': [
        reasonable['ols_mape'].median(),
        reasonable['ols_T1_ape'].median(),
        reasonable['ols_LC_ae'].median(),
        reasonable['ols_RC_ae'].median()
    ],
    'PCReg Median': [
        reasonable['pcreg_mape'].median(),
        reasonable['pcreg_T1_ape'].median(),
        reasonable['pcreg_LC_ae'].median(),
        reasonable['pcreg_RC_ae'].median()
    ]
}
df_reas = pd.DataFrame(metrics_reas)

# Format for display
df_reas_display = df_reas.copy()
for col in ['OLS Mean', 'PCReg Mean', 'OLS Median', 'PCReg Median']:
    df_reas_display[col] = df_reas_display[col].apply(lambda x: f'{x:.4f}')

print(tabulate(df_reas_display, headers='keys', tablefmt='pipe', showindex=False))
```

```{python}
#| label: reasonable-diff-calc

# Calculate practical differences
mape_diff_reas = (reasonable['pcreg_mape'].mean() - reasonable['ols_mape'].mean()) * 100
mape_diff_reas_pct = mape_diff_reas / (reasonable['ols_mape'].mean() * 100) * 100
```

## OLS-LearnOnly Performs Poorly Outside Training Range

```{python}
#| label: ols-learn-analysis

# Compare OLS-LearnOnly performance
ols_learn_worse = (comparison['ols_learn_mape'] > comparison['ols_mape']).mean() * 100
ols_learn_mean_mape = comparison['ols_learn_mape'].mean() * 100
ols_mean_mape = comparison['ols_mape'].mean() * 100
pcreg_mean_mape = comparison['pcreg_mape'].mean() * 100

# PCReg vs OLS-LearnOnly comparison
comparison['pcreg_wins_vs_learnonly'] = comparison['pcreg_mape'] < comparison['ols_learn_mape']
pcreg_vs_learnonly_overall = comparison['pcreg_wins_vs_learnonly'].mean() * 100
pcreg_vs_learnonly_reasonable = comparison[comparison['ols_reasonable'] == True]['pcreg_wins_vs_learnonly'].mean() * 100
pcreg_vs_learnonly_unreasonable = comparison[comparison['ols_reasonable'] == False]['pcreg_wins_vs_learnonly'].mean() * 100
```

OLS-LearnOnly, which ignores the rate effect, exhibits poor extrapolation:

| Model         |                           Mean Test MAPE |
|:--------------|-----------------------------------------:|
| OLS-LearnOnly | `{python} f'{ols_learn_mean_mape:.1f}'`% |
| OLS           |       `{python} f'{ols_mean_mape:.1f}'`% |
| PCReg         |     `{python} f'{pcreg_mean_mape:.1f}'`% |

: Mean Test MAPE by model {#tbl-mape-comparison}

OLS-LearnOnly has worse Test MAPE than full OLS in **`{python} f'{ols_learn_worse:.1f}'`%** of scenarios. While simplifying the model may seem appealing, omitting the rate effect leads to systematic prediction errors outside the training range.

PCReg outperforms OLS-LearnOnly on Test MAPE in **`{python} f'{pcreg_vs_learnonly_overall:.1f}'`%** of all scenarios:

-   When OLS coefficients are **reasonable**: PCReg wins **`{python} f'{pcreg_vs_learnonly_reasonable:.1f}'`%**
-   When OLS coefficients are **unreasonable**: PCReg wins **`{python} f'{pcreg_vs_learnonly_unreasonable:.1f}'`%**

## Coefficient Bias Analysis

Constraints introduce bias in coefficient estimates. We analyze whether this bias is systematic and how it affects prediction below. A Mean/Median near 0 indicates unbiased estimation; lower Std indicates more stable estimates.

```{python}
#| label: tbl-bias
#| tbl-cap: "Coefficient bias statistics"
#| echo: false
#| output: asis

# Calculate statistics for each model and coefficient
bias_stats = results.groupby('model_name').agg({
    'T1_bias': ['mean', 'median', 'std'],
    'b_bias': ['mean', 'median', 'std'],
    'c_bias': ['mean', 'median', 'std']
}).T

# Create pivoted structure with one row per coefficient
rows = []
for coef in ['T1_bias', 'b_bias', 'c_bias']:
    coef_name = coef.replace('_bias', '').replace('T1', '$T_1$').replace('b', '$b$').replace('c', '$c$')
    row_data = {'Coefficient': coef_name}

    # Add columns for each model and statistic combination
    for model in ['OLS', 'PCReg_GCV']:
        model_name = 'PCReg' if model == 'PCReg_GCV' else model
        if model in bias_stats.columns:
            for stat in ['mean', 'median', 'std']:
                val = bias_stats.loc[(coef, stat), model]
                col_name = f'{model_name} {stat.capitalize()}'
                row_data[col_name] = f'{val:.4f}'
    rows.append(row_data)

bias_table = pd.DataFrame(rows)

print(tabulate(bias_table, headers='keys', tablefmt='pipe', showindex=False))
```

@fig-bias-dist shows the distribution of coefficient errors by model (1st-99th percentile). The vertical line at 0 indicates no bias. PCReg shows tighter distributions despite some bias, leading to lower variance in predictions.

```{python}
#| label: fig-bias-dist
#| fig-cap: "Distribution of coefficient errors by model"

fig, axes = plt.subplots(1, 3, figsize=(14, 4))

# Helper function to get percentile limits
def get_pct_limits(data1, data2, lower_pct=1, upper_pct=99):
    combined = pd.concat([data1, data2])
    return combined.quantile(lower_pct/100), combined.quantile(upper_pct/100)

# T1 bias
ax1 = axes[0]
ols_data = results[results['model_name'] == 'OLS']['T1_bias']
pcreg_data = results[results['model_name'] == 'PCReg_GCV']['T1_bias']
xlim_low, xlim_high = get_pct_limits(ols_data, pcreg_data)
ols_filtered = ols_data[(ols_data >= xlim_low) & (ols_data <= xlim_high)]
pcreg_filtered = pcreg_data[(pcreg_data >= xlim_low) & (pcreg_data <= xlim_high)]
sns.kdeplot(data=ols_filtered, ax=ax1, label='OLS', color='#e74c3c', fill=True, alpha=0.3)
sns.kdeplot(data=pcreg_filtered, ax=ax1, label='PCReg', color='#3498db', fill=True, alpha=0.3)
ax1.axvline(x=0, color='black', linestyle='--', linewidth=2, label='No Bias')
ax1.set_xlabel('T1 Error (Est - True)')
ax1.set_ylabel('Density')
ax1.set_title('T1 Coefficient Bias')
ax1.legend()
ax1.set_xlim(xlim_low, xlim_high)

# b bias
ax2 = axes[1]
ols_data = results[results['model_name'] == 'OLS']['b_bias']
pcreg_data = results[results['model_name'] == 'PCReg_GCV']['b_bias']
xlim_low, xlim_high = get_pct_limits(ols_data, pcreg_data)
ols_filtered = ols_data[(ols_data >= xlim_low) & (ols_data <= xlim_high)]
pcreg_filtered = pcreg_data[(pcreg_data >= xlim_low) & (pcreg_data <= xlim_high)]
sns.kdeplot(data=ols_filtered, ax=ax2, label='OLS', color='#e74c3c', fill=True, alpha=0.3)
sns.kdeplot(data=pcreg_filtered, ax=ax2, label='PCReg', color='#3498db', fill=True, alpha=0.3)
ax2.axvline(x=0, color='black', linestyle='--', linewidth=2, label='No Bias')
ax2.set_xlabel('b Error (Est - True)')
ax2.set_ylabel('Density')
ax2.set_title('Learning Slope (b) Bias')
ax2.legend()
ax2.set_xlim(xlim_low, xlim_high)

# c bias
ax3 = axes[2]
ols_data = results[results['model_name'] == 'OLS']['c_bias']
pcreg_data = results[results['model_name'] == 'PCReg_GCV']['c_bias']
xlim_low, xlim_high = get_pct_limits(ols_data, pcreg_data)
ols_filtered = ols_data[(ols_data >= xlim_low) & (ols_data <= xlim_high)]
pcreg_filtered = pcreg_data[(pcreg_data >= xlim_low) & (pcreg_data <= xlim_high)]
sns.kdeplot(data=ols_filtered, ax=ax3, label='OLS', color='#e74c3c', fill=True, alpha=0.3)
sns.kdeplot(data=pcreg_filtered, ax=ax3, label='PCReg', color='#3498db', fill=True, alpha=0.3)
ax3.axvline(x=0, color='black', linestyle='--', linewidth=2, label='No Bias')
ax3.set_xlabel('c Error (Est - True)')
ax3.set_ylabel('Density')
ax3.set_title('Rate Slope (c) Bias')
ax3.legend()
ax3.set_xlim(xlim_low, xlim_high)

plt.tight_layout()
plt.show()
```

While OLS is theoretically unbiased with mean errors near 0, it has high variance and wide distribution. PCReg may have slight bias but much lower variance, leading to better overall prediction accuracy as explained by the classic bias-variance tradeoff.

## Parameter Effects on Model Performance

Beyond coefficient reasonableness, several design factors systematically influence when PCReg outperforms OLS. We examine sample size, predictor correlation, and noise level.

```{python}
#| label: parameter-effects-analysis
#| output: asis

from tabulate import tabulate

# Create correlation bins for analysis
comparison['corr_bin'] = pd.cut(comparison['actual_correlation'], 
                                 bins=[0, 0.8, 0.9, 0.95, 1.0],
                                 labels=['<0.80', '0.80-0.90', '0.90-0.95', '>0.95'])
```

```{python}
#| label: parameter-effects-summary

# Calculate key statistics for inline text
win_by_n = comparison.groupby('n_lots')['pcreg_wins_mape'].mean() * 100
win_n5 = win_by_n.get(5, win_by_n.iloc[0])
win_n30 = win_by_n.get(30, win_by_n.iloc[-1])

win_by_corr = comparison.groupby('corr_bin', observed=True)['pcreg_wins_mape'].mean() * 100
win_low_corr = win_by_corr.iloc[0] if len(win_by_corr) > 0 else 50
win_high_corr = win_by_corr.iloc[-1] if len(win_by_corr) > 0 else 50

unreasonable_by_n = comparison.groupby('n_lots')['ols_reasonable'].apply(lambda x: (~x).mean() * 100)
unreasonable_n5 = unreasonable_by_n.get(5, unreasonable_by_n.iloc[0])
unreasonable_n30 = unreasonable_by_n.get(30, unreasonable_by_n.iloc[-1])
```


```{python}
#| label: pe-wr-Training
#| tbl-cap:  Win Rate by Training Size
#| echo: false
#| output: asis

n_table = comparison.groupby('n_lots').agg(
    n_scenarios=('pcreg_wins_mape', 'count'),
    win_rate=('pcreg_wins_mape', 'mean'),
    pct_unreasonable=('ols_reasonable', lambda x: (~x).mean()),
    mean_ols_mape=('ols_mape', 'mean'),
    mean_pcreg_mape=('pcreg_mape', 'mean')
).reset_index()
n_table['win_rate'] = (n_table['win_rate'] * 100).round(1).astype(str) + '%'
n_table['pct_unreasonable'] = (n_table['pct_unreasonable'] * 100).round(1).astype(str) + '%'
n_table['mean_ols_mape'] = (n_table['mean_ols_mape'] * 100).round(1).astype(str) + '%'
n_table['mean_pcreg_mape'] = (n_table['mean_pcreg_mape'] * 100).round(1).astype(str) + '%'
n_table.columns = ['Sample Size', 'N', 'PCReg Win Rate', '% OLS Unreasonable', 'OLS Mean MAPE', 'PCReg Mean MAPE']

print(tabulate(n_table, headers='keys', tablefmt='pipe', showindex=False))
print("\n")
```

**Sample Size Effect**: As sample size, or training size, decreases, OLS becomes less stable. With n=5 lots, PCReg wins **`{python} f'{win_n5:.1f}'`%** of scenarios compared to **`{python} f'{win_n30:.1f}'`%** at n=30. The rate of unreasonable OLS coefficients also increases from **`{python} f'{unreasonable_n30:.1f}'`%** to **`{python} f'{unreasonable_n5:.1f}'`%** as sample size decreases.

```{python}
#| label: pe-wr-corr
#| tbl-cap:  Win Rate by Correlation
#| echo: false
#| output: asis

corr_table = comparison.groupby('corr_bin', observed=True).agg(
    n_scenarios=('pcreg_wins_mape', 'count'),
    win_rate=('pcreg_wins_mape', 'mean'),
    pct_unreasonable=('ols_reasonable', lambda x: (~x).mean()),
    mean_ols_mape=('ols_mape', 'mean'),
    mean_pcreg_mape=('pcreg_mape', 'mean')
).reset_index()
corr_table['win_rate'] = (corr_table['win_rate'] * 100).round(1).astype(str) + '%'
corr_table['pct_unreasonable'] = (corr_table['pct_unreasonable'] * 100).round(1).astype(str) + '%'
corr_table['mean_ols_mape'] = (corr_table['mean_ols_mape'] * 100).round(1).astype(str) + '%'
corr_table['mean_pcreg_mape'] = (corr_table['mean_pcreg_mape'] * 100).round(1).astype(str) + '%'
corr_table.columns = ['Correlation', 'N', 'PCReg Win Rate', '% OLS Unreasonable', 'OLS Mean MAPE', 'PCReg Mean MAPE']

print(tabulate(corr_table, headers='keys', tablefmt='pipe', showindex=False))
print("\n")
```

**Correlation Effect**: Higher predictor correlation destabilizes OLS. At correlation \>0.95, PCReg wins **`{python} f'{win_high_corr:.1f}'`%** of scenarios versus **`{python} f'{win_low_corr:.1f}'`%** at lower correlations. This is the multicollinearity effect. When predictors are highly correlated, OLS coefficient estimates become unstable and constraints provide crucial stability.

```{python}
#| label: pe-wr-noise
#| tbl-cap:  Win Rate by CV (Noise)
#| echo: false
#| output: asis

cv_table = comparison.groupby('cv_error').agg(
    n_scenarios=('pcreg_wins_mape', 'count'),
    win_rate=('pcreg_wins_mape', 'mean'),
    pct_unreasonable=('ols_reasonable', lambda x: (~x).mean()),
    mean_ols_mape=('ols_mape', 'mean'),
    mean_pcreg_mape=('pcreg_mape', 'mean')
).reset_index()
cv_table['win_rate'] = (cv_table['win_rate'] * 100).round(1).astype(str) + '%'
cv_table['pct_unreasonable'] = (cv_table['pct_unreasonable'] * 100).round(1).astype(str) + '%'
cv_table['mean_ols_mape'] = (cv_table['mean_ols_mape'] * 100).round(1).astype(str) + '%'
cv_table['mean_pcreg_mape'] = (cv_table['mean_pcreg_mape'] * 100).round(1).astype(str) + '%'
cv_table.columns = ['CoV Error', 'N', 'PCReg Win Rate', '% OLS Unreasonable', 'OLS Mean MAPE', 'PCReg Mean MAPE']

print(tabulate(cv_table, headers='keys', tablefmt='pipe', showindex=False))
```

**Noise Level Effect**: Higher CoV error increases the advantage of PCReg. With more noise, OLS has greater difficulty separating learning and rate effects, leading to more unreasonable coefficient estimates.

**Compounding Effects**: These factors interact---small samples with high correlation and high noise represent the most challenging scenarios for OLS, where PCReg provides the greatest benefit.

\newpage

# Practical Guidance

Based on our simulation results, we recommend the following decision rules for practitioners:

1.  **If OLS produces unreasonable coefficients** (LCS or Rate Effect outside 70-100%): **Use PCReg**, it significantly outperforms OLS in these scenarios winning **`{python} f"{comparison.query('ols_reasonable==False').pcreg_wins_mape.mean():.0%}"`%** of the time
2.  **For small samples (n = 5 or 10 lots) with noisy data (CoV = .20)**: **Prefer PCReg**, it wins **`{python} f"{comparison.query('cv_error==.2 and n_lots<=10').pcreg_wins_mape.mean():.0%}"`**-**`{python} f"{comparison.query('cv_error==.2 and n_lots<=5').pcreg_wins_mape.mean():.0%}"`** of scenarios
3.  **For large samples (n = 30 lots)**: **OLS is generally preferred**, it wins \~**`{python} f"{1-comparison.query('cv_error<.2 and n_lots==30').pcreg_wins_mape.mean():.0%}"`**-**`{python} f"{1-comparison.query('cv_error==.2 and n_lots==30').pcreg_wins_mape.mean():.0%}"`** of scenarios
4.  **For intermediate cases**: Either method is acceptable; PCReg provides insurance against unreasonable coefficients with minimal downside

**Bottom line**: The difference between OLS and PCReg is typically small and can be controlled by explicitly defining loose constraints and arbitrarily small penalties.

\newpage
# Limitations

Several limitations of this study should be acknowledged:

**Loss Function Differences.** OLS and PCReg optimize different objective functions: OLS minimizes SSE in log-space, while PCReg minimizes SSPE in unit-space. Both are evaluated on unit-space metrics (MAPE). Ultimately we chose to measure against unit-space metrics, which directly determine prediction accuracy on future programs. OLS's reliance on log-transforms means it estimates the conditional *median* in unit space, not the mean. This is a known limitation of log-linear regression that PCReg addresses by optimizing directly in unit space.

**Penalty Selection.** Our simulation tested a limited grid of penalty values ($\lambda$ and $\alpha$). Optimal penalty selection remains an active area of research, and the current grid may not capture the best-performing penalty configurations in all scenarios. Additionally, there is not a universally accepted method for selecting penalties in constrained regression, and GCV is one of several possible approaches. Future work could explore alternative penalty selection methods and a wider range of penalty values. A common heuristic is to choose the smallest penalty term that stabilizes coefficients without significantly increasing training error, which can be implemented with a lasso and ridge trace plot.

**Simulation-Based Evaluation.** All results are based on Monte Carlo simulation with known data-generating parameters. While the simulation uses realistic lot structures drawn from SAR data, no real-world cost datasets were used for validation. Our simulation evaluates models on their ability to recover the true underlying cost function, using noiseless test data at quantities beyond the training range. This design reflects the real-world cost estimation problem of predicting future production lots, where the analyst's goal is to estimate the true population relationship. However, in the real world, population parameters are unknown and the size of data is limited, which affects the ability to create test and training splits. Future work will apply PCReg to historical SAR data to evaluate the method's performance on real cost estimation problems.

\newpage
# Summary

Our simulation study demonstrates that PCReg provides meaningful advantages in small-sample, high-noise scenarios where OLS is most likely to produce unreasonable coefficients, while large samples favor standard OLS. The constraints introduce beneficial bias that reduces prediction variance. This is a classic bias-variance tradeoff that works in the analyst's favor when data are limited. GCV enables reliable hyperparameter selection with as few as 5 observations. Furthermore, OLS-LearnOnly's poor extrapolation performance reinforces that omitting the rate effect oversimplifies the problem. The guidance is clear: match the method to the sample size and data quality, with PCReg serving as effective insurance when uncertainty is high.

\newpage
# References {.unnumbered}

::: {#refs}
:::

\newpage
# Acronyms {.unnumbered}

| Acronym | Definition |
|:--------|:-----------|
| CER | Cost Estimating Relationship |
| CoV | Coefficient of Variation |
| CV | Cross Validation |
| DoD | Department of Defense |
| EMD | Engineering and Manufacturing Development |
| FRP | Full Rate Production |
| GCV | Generalized Cross Validation |
| GDF | Generalized Degrees of Freedom |
| GLM | Generalized Linear Model |
| LCS | Learning Curve Slope |
| MAE | Mean Absolute Error |
| MAPE | Mean Absolute Percentage Error |
| MSE | Mean Squared Error |
| OLS | Ordinary Least Squares |
| PAC | Penalized and Constrained |
| PCReg | Penalized-Constrained Regression |
| RMSE | Root Mean Squared Error |
| RSS | Residual Sum of Squares |
| SAR | Selected Acquisition Report |
| SEE | Standard Error of Estimate |
| SLSQP | Sequential Least Squares Programming |
| SPE | Standard Percentage Error |
| SSE | Sum of Squared Errors |
| SSPE | Sum of Squared Percentage Errors |
| ZMPE | Minimum-Percentage Error under the Zero-Percentage Bias |

\newpage

\appendix

# PCReg Mathematical Notation {#sec-pcreg-notation}

## Complete Notation Table

| Symbol | Definition | Example |
|----|----|----|
| $X$ | Feature matrix (n × p) | Lot midpoint, quantity, complexity |
| $X_i$ | i-th row of X | Predictor values for observation i |
| $y_i$ | Observed response | Actual cost for observation i |
| $\theta$ | Parameter vector (p × 1) | Coefficients being estimated |
| $\hat{y}_i = f(X_i, \theta)$ | Predicted value | Model prediction |
| $\lambda \geq 0$ | Regularization strength | Overall penalty weight |
| $\alpha \in [0,1]$ | L1/L2 mixing parameter | 0=Ridge, 1=Lasso |
| $L(y_i, \hat{y}_i)$ | Loss function | SSE, SSPE, MSE, etc. |

: Complete mathematical notation for PCReg framework {#tbl-notation}

## Prediction Function Forms

**Linear Model (Default):** $$\hat{y}_i = \theta_0 + \theta_1 x_{i1} + \theta_2 x_{i2} + \cdots + \theta_p x_{ip} = X_i\theta + \theta_0$$

For linear models, $\theta = [\beta_1, \ldots, \beta_p]$ represents regression coefficients (slopes), and $\theta_0$ is the intercept.

**Nonlinear Model (Learning Curve Example):** $$\hat{y}_i = T_1 \cdot (\text{LotMidpoint}_i)^b \cdot (\text{LotQuantity}_i)^c$$

Here $\theta = [T_1, b, c]$ with domain-specific meanings:

-   $T_1$ = Base cost parameter
-   $b$ = LCS exponent
-   $c$ = Rate effect exponent

## Cost Estimating Terminology Mapping

For the cost estimating community, PCReg notation maps to familiar CER concepts:

| Math Notation      | CER Terminology    | Example                       |
|--------------------|--------------------|-------------------------------|
| $X$                | Cost drivers       | Lot size, quantity, weight    |
| $\theta$           | CER parameters     | Slopes, intercepts, exponents |
| $y$                | Historical costs   | Actual program costs          |
| $\hat{y}$          | Cost estimates     | CER predictions               |
| Bounds on $\theta$ | Domain constraints | LCS: 70-100%, b: -0.5 to 0    |
| $\lambda$          | Regularization     | Shrinkage for small datasets  |

: Mapping between mathematical and cost estimating terminology {#tbl-cer-mapping}

PCReg addresses the typical CER challenge: small samples (5-30 points), correlated predictors, and enforcing domain knowledge bounds on parameters.

\newpage

# Other Models to Consider for Future Methods {#sec-appendix-other-models}

During our simulation study, we evaluated several alternative modeling approaches beyond the primary comparison between PCReg, OLS, and OLS-LearnOnly presented in the main paper. These preliminary results suggest promising directions for future methodological development. However, we require additional time to complete a thorough analysis before making definitive recommendations.

## Model Descriptions

**PCReg Variants:**

- **PCReg**: Our primary model using GCV penalty selection with SSPE loss and domain constraints (default configuration)
- **PCReg_Tight**: PCReg with tighter coefficient bounds (narrower feasible region for domain constraints)
- **PCReg_ConstrainOnly**: Constrained optimization without regularization penalty (λ=0, constraints only)
- **PCRegGCV_LogMSE**: PCReg using log-transformed MSE loss instead of SSPE
- **PCReg_CV**: PCReg using K-Fold cross validation for penalty selection instead of GCV
- **PCReg_AICc**: PCReg using corrected Akaike Information Criterion (AICc) for penalty selection

**Baseline Methods:**

- **OLS**: Standard ordinary least squares regression (log-log linear model)
- **OLS_LearnOnly**: OLS using only the learning curve variable (excludes quantity/rate effect)
- **RidgeCV**: Ridge regression (L2 penalty only) with cross validation, no domain constraints
- **LassoCV**: Lasso regression (L1 penalty only) with cross validation, no domain constraints
- **BayesianRidge**: Bayesian Ridge regression with automatic relevance determination

## Preliminary Results

```{python}
#| label: tbl-all-models

summary_all = results_all.groupby('model_name').agg({
    'test_mape': 'mean',
    'test_sspe': 'mean',
    'b_error': 'mean',
    'c_error': 'mean',
    'r2': 'mean'
}).round(4)
summary_all = summary_all.sort_values('test_mape')

# Store values for inline printing
model_results = []
for model in summary_all.index:
    row = summary_all.loc[model]
    model_results.append({
        'model': model,
        'mape': row['test_mape'],
        'sspe': row['test_sspe'],
        'b_err': row['b_error'],
        'c_err': row['c_error'],
        'r2': row['r2']
    })
```

| Model               | Test MAPE | Test SSPE | b Error | c Error |    R2 |
|:--------------------|----------:|----------:|--------:|--------:|------:|
| PCReg_Tight         |    0.0561 |    0.0861 |  0.0200 |  0.0337 | 0.852 |
| PCReg_ConstrainOnly |    0.0764 |    0.2052 |  0.0354 |  0.0791 | 0.877 |
| PCReg               |    0.0773 |    0.2074 |  0.0338 |  0.0827 | 0.871 |
| PCRegGCV_LogMSE     |    0.0778 |    0.2832 |  0.0341 |  0.0776 | 0.877 |
| PCReg_CV            |    0.0794 |    0.2538 |  0.0353 |  0.0814 | 0.862 |
| BayesianRidge       |    0.0801 |    0.4595 |  0.0360 |  0.0886 | 0.882 |
| PCReg_AICc          |    0.0808 |    0.2171 |  0.0349 |  0.0913 | 0.863 |
| RidgeCV             |    0.0952 |    0.9362 |  0.0481 |  0.1216 | 0.896 |
| LassoCV             |    0.0957 |    0.9767 |  0.0428 |  0.1082 | 0.858 |
| OLS                 |    0.0994 |    0.9727 |  0.0520 |  0.1319 | 0.897 |
| OLS_LearnOnly       |    0.1592 |    0.9543 |  0.0827 |  0.2361 | 0.789 |

: Preliminary model performance across all 8,100 scenarios. Lower Test MAPE is better. {#tbl-all-results}

## Future Work

These results are preliminary and require further investigation to understand:

- The performance advantages of tighter constraint bounds (PCReg_Tight)
- Trade-offs between different penalty selection criteria (GCV vs. CV vs. AICc)
- The role of loss function choice (SSPE vs. log-transformed MSE)
- Whether constraints alone (without regularization) provide sufficient regularization
- How standard penalized regression methods (Ridge, Lasso, Bayesian Ridge) compare when adapted to nonlinear cost models

Additionally, we plan to pursue the following:

- **Real-world validation**: Fit PCReg on actual SAR data to establish empirical benchmarks and validate the simulation findings against real cost programs
- **Penalty selection research**: Conduct a more comprehensive study of penalty parameter selection, exploring finer grids and adaptive methods for identifying optimal regularization strength
- **Trace plots**: Develop coefficient trace plots that visualize how estimates change across penalty values, enabling practitioners to select the optimal penalty for their individual case and better understand the bias-variance tradeoff in their specific context

We plan to conduct additional analysis to provide comprehensive guidance on selecting among these alternatives for cost estimation applications.

\newpage

# Software Documentation {#sec-appendix-software}

## Installation

**Note:** The package is currently available on [GitHub](https://github.com/frankij11/Penalized-Constrained-Regression) and will be released to PyPI in the future. Please check back for pip installation instructions.

### Installing from GitHub

You can install the package directly from GitHub using pip:

``` bash
pip install git+https://github.com/frankij11/Penalized-Constrained-Regression.git
```

### Cloning the Repository

Alternatively, you can clone the repository and install locally:

``` bash
git clone https://github.com/frankij11/Penalized-Constrained-Regression.git
cd Penalized-Constrained-Regression
pip install -e .
```

### Downloading the Excel Template

For users preferring Excel, download the [PCReg Template](https://github.com/frankij11/Penalized-Constrained-Regression/raw/main/PCReg_Template.xlsx) directly, or visit the repository to access all files.

## Basic Usage

```{python}
#| echo: true
#| eval: false

import numpy as np
import penalized_constrained as pcreg

# Generate sample data
np.random.seed(42)
X = np.random.randn(100, 2)
y = X @ np.array([-0.15, -0.07]) + 4.5 + 0.1 * np.random.randn(100)

# Fit with cross-validated hyperparameter selection
model = pcreg.PenalizedConstrainedCV(
    bounds=[(-1, 0), (-1, 0)],  # Both coefficients must be ≤ 0
    loss='sspe',                 # Sum of Squared Percentage Error
    cv=5
)
model.fit(X, y)

# Results
print(f"Best alpha: {model.alpha_:.4f}")
print(f"Best l1_ratio: {model.l1_ratio_:.2f}")
print(f"Coefficients: {model.coef_}")
print(f"Active constraints: {model.active_constraints_}")
```

## Citation

``` bibtex
@inproceedings{joy2026pcreg,
  title={Penalized-Constrained Regression: Combining Regularization
         and Domain Constraints for Cost Estimation},
  author={Joy, Kevin and Watstein, Max},
  booktitle={ICEAA Professional Development \& Training Workshop},
  year={2026}
}
```

\newpage

# Detailed Diagnostic Report {#sec-appendix-diagnostic}

This appendix provides comprehensive diagnostic information for the PCReg model fit from the motivating example.

## Model Specification

```{python}
#| label: appendix-model-spec
#| output: asis
#| echo: false

from tabulate import tabulate
import pandas as pd

# Model specification details
spec_data = {
    'Parameter': [
        'Model Type',
        'Loss Function',
        'Alpha ($\\lambda$)',
        'L1 Ratio ($\\alpha$)',
        'Fit Intercept',
        'Optimization Method',
        'Convergence Status',
        'GDF Method'
    ],
    'Value': [
        report.model_spec.model_type,
        report.model_spec.loss_function,
        f'{report.model_spec.alpha:.6f}',
        f'{report.model_spec.l1_ratio:.4f}',
        str(report.model_spec.fit_intercept),
        report.model_spec.method,
        'Yes' if report.model_spec.converged else 'No',
        report.fit_stats.gdf_method
    ]
}

df_spec = pd.DataFrame(spec_data)
print(df_spec.to_markdown(index=False))
```

## Coefficient Estimates

```{python}
#| label: appendix-coefficients
#| output: asis
#| echo: false

# Extract coefficient information
coef_data = []
for coef in report.coefficients:
    coef_data.append({
        'Parameter': coef.name,
        'Estimate': f'{coef.value:.6f}',
        'Bootstrap SE': f'{coef.bootstrap_se:.6f}' if coef.bootstrap_se is not None else 'N/A',
        '95% CI Lower': f'{coef.bootstrap_ci_lower:.6f}' if coef.bootstrap_ci_lower is not None else 'N/A',
        '95% CI Upper': f'{coef.bootstrap_ci_upper:.6f}' if coef.bootstrap_ci_upper is not None else 'N/A',
        'Lower Bound': f'{coef.lower_bound}' if coef.lower_bound != -np.inf else 'None',
        'Upper Bound': f'{coef.upper_bound}' if coef.upper_bound != np.inf else 'None',
        'Status': coef.bound_status
    })

df_coef = pd.DataFrame(coef_data)
print(df_coef.to_markdown(index=False))
```

## Fit Statistics

```{python}
#| label: appendix-fit-stats
#| output: asis
#| echo: false

# Fit statistics
fit_data = {
    'Statistic': [
        'R²',
        'Adjusted R²',
        'SEE',
        'SPE',
        'MAPE',
        'RMSE',
        'CV',
        'GDF'
    ],
    'Value': [
        f'{report.fit_stats.r2:.6f}',
        f'{report.fit_stats.adj_r2:.6f}',
        f'{report.fit_stats.see:.6f}',
        f'{report.fit_stats.spe:.6f}',
        f'{report.fit_stats.mape:.4f}',
        f'{report.fit_stats.rmse:.6f}',
        f'{report.fit_stats.cv:.6f}',
        f'{report.fit_stats.gdf:.2f}'
    ]
}

df_fit = pd.DataFrame(fit_data)
print(df_fit.to_markdown(index=False))
```

## Constraint Summary

```{python}
#| label: appendix-constraints
#| output: asis
#| echo: false

print(f"**Number of specified bounds:** {report.constraints.n_specified}\n")
print(f"**Number of active constraints:** {report.constraints.n_active}\n")

if report.constraints.active_constraints:
    print("\n**Active constraints:**\n")
    for param, bound_type in report.constraints.active_constraints:
        print(f"- {param} at {bound_type} bound")
else:
    print("\nNo constraints are active (all parameters are in the interior).")
```

## Bootstrap Results

```{python}
#| label: appendix-bootstrap
#| output: asis
#| echo: false

if report.bootstrap_results is not None:
    boot = report.bootstrap_results

    print(f"**Number of bootstrap samples:** {boot.n_bootstrap}\n")
    print(f"**Confidence level:** {boot.confidence*100:.0f}%\n")
    print(f"**Successful constrained fits:** {boot.constrained.n_successful}\n")
    if boot.unconstrained is not None:
        print(f"**Successful unconstrained fits:** {boot.unconstrained.n_successful}\n")

    print("\n### Bootstrap Summary Statistics\n")

    # Create comparison table
    boot_summary = boot.summary_dataframe(include_unconstrained=True)
    print(boot_summary.to_markdown(index=False, floatfmt='.6f'))
else:
    print("Bootstrap confidence intervals were not computed for this model.")
```

## Data Summary

```{python}
#| label: appendix-data-summary
#| output: asis
#| echo: false

data_info = {
    'Property': [
        'Number of samples',
        'Number of features',
        'Target mean',
        'Target std',
        'Target min',
        'Target max'
    ],
    'Value': [
        report.data_summary.n_samples,
        report.data_summary.n_features,
        f'{report.data_summary.y_mean:.4f}',
        f'{report.data_summary.y_std:.4f}',
        f'{report.data_summary.y_min:.4f}',
        f'{report.data_summary.y_max:.4f}'
    ]
}

df_data = pd.DataFrame(data_info)
print(df_data.to_markdown(index=False))
```

## Model Equation

```{python}
#| label: appendix-equation
#| output: asis
#| echo: false

if report.equation is not None:
    if report.equation.is_custom:
        print("**Custom functional form:**\n")
        print("```python")
        if report.equation.source:
            print(report.equation.source)
        print("```\n")

    if report.equation.latex:
        print(f"\n**Model Equation:** ${report.equation.latex}$\n")
    elif report.equation.text:
        print(f"\n**Model Equation:** `{report.equation.text}`\n")
```